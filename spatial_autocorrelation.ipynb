{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d732bfb",
   "metadata": {},
   "source": [
    "#### Removing spatially autocorrelated training and validation points\n",
    "\n",
    "This scripts removes points that are less than 30m from each other and share status as val/training and are same location-date\n",
    "\n",
    "Initially used the Near tool with val/training and location-date as match fields to prep the point data for running here, but that only matches points with the closest point (not all points under 30m away)\n",
    "\n",
    "Pivoted to spatial join output, with val/training split and location-date as match fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3cb4eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the file of points that are less than 30m apart from another point on the same date/location\n",
    "# does also include clusters of more than two points\n",
    "# didn't work well to join back to full dataset, so I recommend only using the full point file below\n",
    "df = pd.read_csv(\"data/AllTraining_080425prj_closepts.csv\")\n",
    "\n",
    "# load the file with all training points with near distance and near feature id\n",
    "all = pd.read_csv(\"data/AllTraining_080425prj.csv\")\n",
    "\n",
    "# file with spatial join output, including points matched to themselves\n",
    "sj = pd.read_csv(\"data/AllTraining_080425prj_SpatialJoin.csv\")\n",
    "\n",
    "# Set the threshold distance (m)\n",
    "threshold = 30\n",
    "\n",
    "# all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922e3fc0",
   "metadata": {},
   "source": [
    "#### Pull in spatial join dataset, remove points matched to themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b7fa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 4997 self-matched points, keeping 3554 point pairs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annie.taylor\\AppData\\Local\\Temp\\ipykernel_18488\\2840446142.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sj_filter.loc[:, 'Target_Count'] = sj_filter['TARGET_FID'].map(target_counts)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Join_Count</th>\n",
       "      <th>TARGET_FID</th>\n",
       "      <th>JOIN_FID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Date</th>\n",
       "      <th>Type</th>\n",
       "      <th>Region</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Split</th>\n",
       "      <th>Date_Loc</th>\n",
       "      <th>NEAR_FID</th>\n",
       "      <th>NEAR_DIST</th>\n",
       "      <th>Orig_FID</th>\n",
       "      <th>Location_1</th>\n",
       "      <th>Date_1</th>\n",
       "      <th>Type_1</th>\n",
       "      <th>Region_1</th>\n",
       "      <th>Latitude_1</th>\n",
       "      <th>Longitude_1</th>\n",
       "      <th>Split_1</th>\n",
       "      <th>Date_Loc_1</th>\n",
       "      <th>NEAR_FID_1</th>\n",
       "      <th>NEAR_DIST_1</th>\n",
       "      <th>Orig_FID_1</th>\n",
       "      <th>Target_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>1</td>\n",
       "      <td>226</td>\n",
       "      <td>254</td>\n",
       "      <td>Oxnard</td>\n",
       "      <td>11/9/2023</td>\n",
       "      <td>hoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.241627</td>\n",
       "      <td>-119.132327</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-09 Oxnard</td>\n",
       "      <td>226.0</td>\n",
       "      <td>26.160596</td>\n",
       "      <td>254</td>\n",
       "      <td>Oxnard</td>\n",
       "      <td>11/9/2023</td>\n",
       "      <td>hoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.241627</td>\n",
       "      <td>-119.132327</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-09 Oxnard</td>\n",
       "      <td>226.0</td>\n",
       "      <td>26.160596</td>\n",
       "      <td>254</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>1</td>\n",
       "      <td>235</td>\n",
       "      <td>320</td>\n",
       "      <td>Oxnard</td>\n",
       "      <td>5/6/2023</td>\n",
       "      <td>hoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.151062</td>\n",
       "      <td>-119.104499</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-06 Oxnard</td>\n",
       "      <td>235.0</td>\n",
       "      <td>28.005998</td>\n",
       "      <td>320</td>\n",
       "      <td>Oxnard</td>\n",
       "      <td>5/6/2023</td>\n",
       "      <td>hoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.151062</td>\n",
       "      <td>-119.104499</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-06 Oxnard</td>\n",
       "      <td>235.0</td>\n",
       "      <td>28.005998</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>1</td>\n",
       "      <td>243</td>\n",
       "      <td>245</td>\n",
       "      <td>Oxnard</td>\n",
       "      <td>10/14/2019</td>\n",
       "      <td>hoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.189403</td>\n",
       "      <td>-119.123544</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-10-14 Oxnard</td>\n",
       "      <td>243.0</td>\n",
       "      <td>29.336011</td>\n",
       "      <td>245</td>\n",
       "      <td>Oxnard</td>\n",
       "      <td>10/14/2019</td>\n",
       "      <td>hoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.189403</td>\n",
       "      <td>-119.123544</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-10-14 Oxnard</td>\n",
       "      <td>243.0</td>\n",
       "      <td>29.336011</td>\n",
       "      <td>245</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>1</td>\n",
       "      <td>243</td>\n",
       "      <td>305</td>\n",
       "      <td>Oxnard</td>\n",
       "      <td>10/14/2019</td>\n",
       "      <td>hoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.189648</td>\n",
       "      <td>-119.123690</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-10-14 Oxnard</td>\n",
       "      <td>243.0</td>\n",
       "      <td>28.300004</td>\n",
       "      <td>305</td>\n",
       "      <td>Oxnard</td>\n",
       "      <td>10/14/2019</td>\n",
       "      <td>hoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.189648</td>\n",
       "      <td>-119.123690</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-10-14 Oxnard</td>\n",
       "      <td>243.0</td>\n",
       "      <td>28.300004</td>\n",
       "      <td>305</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2</td>\n",
       "      <td>245</td>\n",
       "      <td>243</td>\n",
       "      <td>Oxnard</td>\n",
       "      <td>10/14/2019</td>\n",
       "      <td>hoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.189632</td>\n",
       "      <td>-119.123383</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-10-14 Oxnard</td>\n",
       "      <td>305.0</td>\n",
       "      <td>28.300004</td>\n",
       "      <td>243</td>\n",
       "      <td>Oxnard</td>\n",
       "      <td>10/14/2019</td>\n",
       "      <td>hoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.189632</td>\n",
       "      <td>-119.123383</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-10-14 Oxnard</td>\n",
       "      <td>305.0</td>\n",
       "      <td>28.300004</td>\n",
       "      <td>243</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>2</td>\n",
       "      <td>249</td>\n",
       "      <td>281</td>\n",
       "      <td>Oxnard</td>\n",
       "      <td>11/9/2023</td>\n",
       "      <td>hoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.232007</td>\n",
       "      <td>-119.136027</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-09 Oxnard</td>\n",
       "      <td>264.0</td>\n",
       "      <td>26.837272</td>\n",
       "      <td>281</td>\n",
       "      <td>Oxnard</td>\n",
       "      <td>11/9/2023</td>\n",
       "      <td>hoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.232007</td>\n",
       "      <td>-119.136027</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-09 Oxnard</td>\n",
       "      <td>264.0</td>\n",
       "      <td>26.837272</td>\n",
       "      <td>281</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>1</td>\n",
       "      <td>251</td>\n",
       "      <td>282</td>\n",
       "      <td>Oxnard</td>\n",
       "      <td>11/9/2023</td>\n",
       "      <td>hoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.231686</td>\n",
       "      <td>-119.135760</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-09 Oxnard</td>\n",
       "      <td>251.0</td>\n",
       "      <td>27.541831</td>\n",
       "      <td>282</td>\n",
       "      <td>Oxnard</td>\n",
       "      <td>11/9/2023</td>\n",
       "      <td>hoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.231686</td>\n",
       "      <td>-119.135760</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-09 Oxnard</td>\n",
       "      <td>251.0</td>\n",
       "      <td>27.541831</td>\n",
       "      <td>282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>1</td>\n",
       "      <td>254</td>\n",
       "      <td>226</td>\n",
       "      <td>Oxnard</td>\n",
       "      <td>11/9/2023</td>\n",
       "      <td>hoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.241722</td>\n",
       "      <td>-119.132067</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-09 Oxnard</td>\n",
       "      <td>254.0</td>\n",
       "      <td>26.160596</td>\n",
       "      <td>226</td>\n",
       "      <td>Oxnard</td>\n",
       "      <td>11/9/2023</td>\n",
       "      <td>hoop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.241722</td>\n",
       "      <td>-119.132067</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-09 Oxnard</td>\n",
       "      <td>254.0</td>\n",
       "      <td>26.160596</td>\n",
       "      <td>226</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Join_Count  TARGET_FID  JOIN_FID Location        Date  Type Region  \\\n",
       "226           1         226       254   Oxnard   11/9/2023  hoop    NaN   \n",
       "236           1         235       320   Oxnard    5/6/2023  hoop    NaN   \n",
       "245           1         243       245   Oxnard  10/14/2019  hoop    NaN   \n",
       "246           1         243       305   Oxnard  10/14/2019  hoop    NaN   \n",
       "248           2         245       243   Oxnard  10/14/2019  hoop    NaN   \n",
       "254           2         249       281   Oxnard   11/9/2023  hoop    NaN   \n",
       "257           1         251       282   Oxnard   11/9/2023  hoop    NaN   \n",
       "260           1         254       226   Oxnard   11/9/2023  hoop    NaN   \n",
       "\n",
       "      Latitude   Longitude  Split           Date_Loc  NEAR_FID  NEAR_DIST  \\\n",
       "226  34.241627 -119.132327      1  2023-11-09 Oxnard     226.0  26.160596   \n",
       "236  34.151062 -119.104499      1  2023-05-06 Oxnard     235.0  28.005998   \n",
       "245  34.189403 -119.123544      2  2019-10-14 Oxnard     243.0  29.336011   \n",
       "246  34.189648 -119.123690      2  2019-10-14 Oxnard     243.0  28.300004   \n",
       "248  34.189632 -119.123383      2  2019-10-14 Oxnard     305.0  28.300004   \n",
       "254  34.232007 -119.136027      1  2023-11-09 Oxnard     264.0  26.837272   \n",
       "257  34.231686 -119.135760      1  2023-11-09 Oxnard     251.0  27.541831   \n",
       "260  34.241722 -119.132067      1  2023-11-09 Oxnard     254.0  26.160596   \n",
       "\n",
       "     Orig_FID Location_1      Date_1 Type_1 Region_1  Latitude_1  Longitude_1  \\\n",
       "226       254     Oxnard   11/9/2023   hoop      NaN   34.241627  -119.132327   \n",
       "236       320     Oxnard    5/6/2023   hoop      NaN   34.151062  -119.104499   \n",
       "245       245     Oxnard  10/14/2019   hoop      NaN   34.189403  -119.123544   \n",
       "246       305     Oxnard  10/14/2019   hoop      NaN   34.189648  -119.123690   \n",
       "248       243     Oxnard  10/14/2019   hoop      NaN   34.189632  -119.123383   \n",
       "254       281     Oxnard   11/9/2023   hoop      NaN   34.232007  -119.136027   \n",
       "257       282     Oxnard   11/9/2023   hoop      NaN   34.231686  -119.135760   \n",
       "260       226     Oxnard   11/9/2023   hoop      NaN   34.241722  -119.132067   \n",
       "\n",
       "     Split_1         Date_Loc_1  NEAR_FID_1  NEAR_DIST_1  Orig_FID_1  \\\n",
       "226        1  2023-11-09 Oxnard       226.0    26.160596         254   \n",
       "236        1  2023-05-06 Oxnard       235.0    28.005998         320   \n",
       "245        2  2019-10-14 Oxnard       243.0    29.336011         245   \n",
       "246        2  2019-10-14 Oxnard       243.0    28.300004         305   \n",
       "248        2  2019-10-14 Oxnard       305.0    28.300004         243   \n",
       "254        1  2023-11-09 Oxnard       264.0    26.837272         281   \n",
       "257        1  2023-11-09 Oxnard       251.0    27.541831         282   \n",
       "260        1  2023-11-09 Oxnard       254.0    26.160596         226   \n",
       "\n",
       "     Target_Count  \n",
       "226             1  \n",
       "236             1  \n",
       "245             2  \n",
       "246             2  \n",
       "248             1  \n",
       "254             1  \n",
       "257             1  \n",
       "260             1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marked 875 points for deletion based on spatial matches.\n",
      "Filtered dataset now has 4122 points after removing 875 spatially autocorrelated points.\n"
     ]
    }
   ],
   "source": [
    "# Remove points in sj where target ID matches join ID - matched to itself\n",
    "sj_filter = sj[sj['TARGET_FID'] != sj['JOIN_FID']]\n",
    "\n",
    "print(f\"Removed {len(sj) - len(sj_filter)} self-matched points, keeping {len(sj_filter)} point pairs.\")\n",
    "\n",
    "# Track IDs to delete and processed pairs\n",
    "delete_ids = set()\n",
    "processed_pairs = set()\n",
    "\n",
    "# Count how many times each ID appears as TARGET_FID and add that back as a field to each row with that Target_FID\n",
    "target_counts = sj_filter['TARGET_FID'].value_counts().to_dict()\n",
    "sj_filter.loc[:, 'Target_Count'] = sj_filter['TARGET_FID'].map(target_counts)\n",
    "# count how many times each ID appears as JOIN_FID and add that back as a field to each row with that Join_FID\n",
    "join_counts = sj_filter['JOIN_FID'].value_counts().to_dict()\n",
    "sj_filter.loc[:, 'Join_Count'] = sj_filter['JOIN_FID'].map(join_counts)\n",
    "\n",
    "# Display the first couple rows with all columns showing\n",
    "pd.set_option('display.max_columns', None)\n",
    "display(sj_filter.head(8))\n",
    "\n",
    "# FYI that join_ID is the one that matches up with orig_FID\n",
    "# all fields in this table (including duplicate fields) match the joined feature, not the target/original feature\n",
    "\n",
    "# Iterate through pairs\n",
    "for _, row in sj_filter.iterrows():\n",
    "    t_id = int(row['TARGET_FID'])\n",
    "    j_id = int(row['JOIN_FID'])\n",
    "    pair = tuple(sorted([t_id, j_id]))\n",
    "    target_count = int(row['Target_Count'])\n",
    "    join_count = int(row['Join_Count'])\n",
    "\n",
    "    # Skip this pair if either ID is already marked for deletion or pair is processed\n",
    "    if t_id in delete_ids or j_id in delete_ids or pair in processed_pairs:\n",
    "        continue\n",
    "\n",
    "    # Preferentially delete the ID matched with more other points\n",
    "    if target_count > join_count:\n",
    "        delete_ids.add(t_id)\n",
    "    elif join_count > target_count:\n",
    "        delete_ids.add(j_id)\n",
    "    else:\n",
    "        # If counts are equal, delete the higher ID\n",
    "        delete_ids.add(max(pair))\n",
    "    processed_pairs.add(pair)\n",
    "\n",
    "print(f\"Marked {len(delete_ids)} points for deletion based on spatial matches.\")\n",
    "\n",
    "# Optionally, filter out deleted points from the original dataset\n",
    "filtered_sj_df = all[~all['Orig_FID'].isin(delete_ids)]\n",
    "filtered_sj_df.head()\n",
    "\n",
    "print(f\"Filtered dataset now has {len(filtered_sj_df)} points after removing {len(delete_ids)} spatially autocorrelated points.\")\n",
    "\n",
    "# save filtered dataset as csv\n",
    "# filtered_sj_df.to_csv(\"data/AllTraining_080425prj_SJfiltered.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af2c01",
   "metadata": {},
   "source": [
    "#### Pull in full dataset, remove points that are too close together\n",
    "Preferentially remove points that are paired with more than one other point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f77e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 744 points within 30 meters of another point.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Date</th>\n",
       "      <th>Type</th>\n",
       "      <th>Region</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Split</th>\n",
       "      <th>Date_Loc</th>\n",
       "      <th>NEAR_FID</th>\n",
       "      <th>NEAR_DIST</th>\n",
       "      <th>Orig_FID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CA</td>\n",
       "      <td>5/26/2023</td>\n",
       "      <td>other</td>\n",
       "      <td>Morgan Hill</td>\n",
       "      <td>37.188061</td>\n",
       "      <td>-121.725901</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-26 CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CA</td>\n",
       "      <td>5/26/2023</td>\n",
       "      <td>other</td>\n",
       "      <td>Morgan Hill</td>\n",
       "      <td>37.189068</td>\n",
       "      <td>-121.724510</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-26 CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CA</td>\n",
       "      <td>5/26/2023</td>\n",
       "      <td>other</td>\n",
       "      <td>Morgan Hill</td>\n",
       "      <td>37.190223</td>\n",
       "      <td>-121.722561</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-26 CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CA</td>\n",
       "      <td>5/26/2023</td>\n",
       "      <td>other</td>\n",
       "      <td>Morgan Hill</td>\n",
       "      <td>37.187451</td>\n",
       "      <td>-121.719603</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-26 CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>5/26/2023</td>\n",
       "      <td>other</td>\n",
       "      <td>Morgan Hill</td>\n",
       "      <td>37.189402</td>\n",
       "      <td>-121.726455</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-05-26 CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>Santa Maria</td>\n",
       "      <td>2/6/2021</td>\n",
       "      <td>mulch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.990410</td>\n",
       "      <td>-120.497203</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-02-06 Santa Maria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>Santa Maria</td>\n",
       "      <td>2/6/2021</td>\n",
       "      <td>mulch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.990676</td>\n",
       "      <td>-120.501473</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-02-06 Santa Maria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>Santa Maria</td>\n",
       "      <td>2/6/2021</td>\n",
       "      <td>mulch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.989570</td>\n",
       "      <td>-120.501519</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-02-06 Santa Maria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>Santa Maria</td>\n",
       "      <td>2/6/2021</td>\n",
       "      <td>mulch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.990623</td>\n",
       "      <td>-120.502547</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-02-06 Santa Maria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Santa Maria</td>\n",
       "      <td>2/6/2021</td>\n",
       "      <td>mulch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.989798</td>\n",
       "      <td>-120.502557</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-02-06 Santa Maria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4253 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Location       Date   Type       Region   Latitude   Longitude  \\\n",
       "0              CA  5/26/2023  other  Morgan Hill  37.188061 -121.725901   \n",
       "1              CA  5/26/2023  other  Morgan Hill  37.189068 -121.724510   \n",
       "2              CA  5/26/2023  other  Morgan Hill  37.190223 -121.722561   \n",
       "3              CA  5/26/2023  other  Morgan Hill  37.187451 -121.719603   \n",
       "4              CA  5/26/2023  other  Morgan Hill  37.189402 -121.726455   \n",
       "...           ...        ...    ...          ...        ...         ...   \n",
       "4992  Santa Maria   2/6/2021  mulch          NaN  34.990410 -120.497203   \n",
       "4993  Santa Maria   2/6/2021  mulch          NaN  34.990676 -120.501473   \n",
       "4994  Santa Maria   2/6/2021  mulch          NaN  34.989570 -120.501519   \n",
       "4995  Santa Maria   2/6/2021  mulch          NaN  34.990623 -120.502547   \n",
       "4996  Santa Maria   2/6/2021  mulch          NaN  34.989798 -120.502557   \n",
       "\n",
       "      Split                Date_Loc  NEAR_FID  NEAR_DIST  Orig_FID  \n",
       "0         1           2023-05-26 CA       NaN        NaN         1  \n",
       "1         1           2023-05-26 CA       NaN        NaN         2  \n",
       "2         1           2023-05-26 CA       NaN        NaN         3  \n",
       "3         1           2023-05-26 CA       NaN        NaN         4  \n",
       "4         1           2023-05-26 CA       NaN        NaN         5  \n",
       "...     ...                     ...       ...        ...       ...  \n",
       "4992      2  2021-02-06 Santa Maria       NaN        NaN      5899  \n",
       "4993      2  2021-02-06 Santa Maria       NaN        NaN      5900  \n",
       "4994      2  2021-02-06 Santa Maria       NaN        NaN      5901  \n",
       "4995      2  2021-02-06 Santa Maria       NaN        NaN      5902  \n",
       "4996      2  2021-02-06 Santa Maria       NaN        NaN      5903  \n",
       "\n",
       "[4253 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Track pairs and points to delete\n",
    "delete_ids = set()\n",
    "processed_pairs = set()\n",
    "\n",
    "# Iterate through rows\n",
    "for _, row in all.iterrows():\n",
    "    oid = row['Orig_FID']\n",
    "    near_fid = row['NEAR_FID']\n",
    "    near_dist = row['NEAR_DIST']\n",
    "\n",
    "    if pd.notnull(near_fid) and near_dist < threshold:\n",
    "        pair = tuple(sorted([int(oid), int(near_fid)]))\n",
    "        # If either oid or near_fid is already marked for deletion, skip this pair\n",
    "        #  as it has already been remedied \n",
    "        if int(oid) in delete_ids or int(near_fid) in delete_ids:\n",
    "            pass\n",
    "        else:\n",
    "            # Check if oid or near_fid appears elsewhere in NEAR_FID column (excluding this row)\n",
    "            oid_in_near_fid = ((all['NEAR_FID'] == int(oid)) & (all.index != row.name)).any()\n",
    "            near_fid_in_near_fid = ((all['NEAR_FID'] == int(near_fid)) & (all.index != row.name)).any()\n",
    "            # if oid appears in NEAR_FID but near_fid does not, delete oid\n",
    "            if oid_in_near_fid and not near_fid_in_near_fid:\n",
    "                delete_ids.add(int(oid))\n",
    "            # if near_fid appears in NEAR_FID but oid does not, delete near_fid\n",
    "            elif near_fid_in_near_fid and not oid_in_near_fid:\n",
    "                delete_ids.add(int(near_fid))\n",
    "            else:\n",
    "                # If both or neither appears elsewhere, delete the point with higher OID\n",
    "                delete_ids.add(max(pair))\n",
    "        processed_pairs.add(pair) \n",
    "\n",
    "\n",
    "# Filter out the points to delete\n",
    "filtered_df = all[~all['Orig_FID'].isin(delete_ids)]\n",
    "\n",
    "# Save the cleaned data\n",
    "filtered_df.to_csv(\"data/AllTraining_080425prj_FilteredPts.csv\")\n",
    "\n",
    "print(f\"Deleted {len(delete_ids)} points within {threshold} meters of another point.\")\n",
    "filtered_df\n",
    "\n",
    "\n",
    "# save the points that were deleted as a csv\n",
    "# deleted_df = df[df['Orig_FID'].isin(delete_ids)]\n",
    "# deleted_df.to_csv(\"data/AllTraining_080425prj_DeletedPts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6af7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 941 points within 30 meters of another point.\n"
     ]
    }
   ],
   "source": [
    "# Original method that doesn't preferentially delete points with more than one pair\n",
    "# or account for points already marked for deletion\n",
    "\n",
    "# Track pairs and points to delete\n",
    "delete_ids = set()\n",
    "processed_pairs = set()\n",
    "\n",
    "# Iterate through rows\n",
    "for _, row in df.iterrows():\n",
    "    oid = row['Orig_FID']\n",
    "    near_fid = row['NEAR_FID']\n",
    "    near_dist = row['NEAR_DIST']\n",
    "\n",
    "    if pd.notnull(near_fid) and near_dist < threshold:\n",
    "        pair = tuple(sorted([int(oid), int(near_fid)]))\n",
    "        if pair not in processed_pairs:\n",
    "            delete_ids.add(max(pair))  # delete the one with higher OID\n",
    "            processed_pairs.add(pair)\n",
    "\n",
    "# Filter out the points to delete\n",
    "filtered_df = df[~df['Orig_FID'].isin(delete_ids)]\n",
    "\n",
    "print(f\"Deleted {len(delete_ids)} points within {threshold} meters of another point.\")\n",
    "\n",
    "# export csv of deleted points\n",
    "deleted_df = df[df['Orig_FID'].isin(delete_ids)]\n",
    "deleted_df.to_csv(\"data/AllTraining_080425prj_DeletedPts2.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plastic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
