{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of ag plastics across California\n",
    "\n",
    "Sentinel-2 classification of agricultural plastic use in California based on Harvard students' capstone project and Yuanyuan's internship project\n",
    "\n",
    "Notebook authored by Annie Taylor, July 2024\n",
    "\n",
    "Some key changes from previous analyses:\n",
    "* Using validated training datasets (edited, added precise dates)\n",
    "* Using different cloud-masking algorithm\n",
    "* Using updated index equations\n",
    "* Random stratified sample within polygons instead of sampling every point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up ee api and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "iNWKs6W-DOt_",
    "outputId": "7116f3fd-e1d0-4811-ad9f-0185189b1565"
   },
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticker\n",
    "from IPython.display import display, HTML\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import geopandas as gpd\n",
    "import altair as alt\n",
    "import rasterio #used for pt sampling in section 8, zonal stats testing\n",
    "from rasterio.plot import show\n",
    "import seaborn as sns\n",
    "import os\n",
    "from rasterio.mask import mask # for area calc, can probably delete\n",
    "import rioxarray\n",
    "import xarray\n",
    "from rasterstats import zonal_stats\n",
    "# testing diff in git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ee.Authenticate()\n",
    "# ee.Initialize(project='ee-tnc-annietaylor')\n",
    "cloud_project = 'ee-tnc-annietaylor'\n",
    "try:\n",
    "    ee.Initialize(project=cloud_project)\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(project=cloud_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fD1KhF5DEiqe"
   },
   "source": [
    "### Old training dataset import/stats -- archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYEeh6bd7leV"
   },
   "outputs": [],
   "source": [
    "# # Training data - import from repo \n",
    "# # Import the csvs with lat/longs as dfs (could all be one csv/df, keeping separate for testing)\n",
    "# # train_path = \"N:/OCEANS_Program/Plastics/Agricultural_Plastics/Validated Training Data/\" # can use this when connected to VPN \n",
    "# train_path = 'data/Validated_Training_Data/'\n",
    "# santamaria = pd.read_csv(train_path + 'SantaMaria_val_o.csv')\n",
    "# mendocino = pd.read_csv(train_path + 'MendocinoCounty_val_o.csv')\n",
    "# # watsonville = pd.read_csv(train_path + 'Watsonville_val.csv')\n",
    "# watsonville = pd.read_csv(train_path + 'Watsonville_val_BM.csv')\n",
    "# # replace 'black mulch' with 'blackmulch' to avoid issues with space in label\n",
    "# watsonville['Type'] = watsonville['Type'].replace('black mulch', 'blackmulch')\n",
    "# oxnard_original = pd.read_csv(train_path + 'YYT_RElabeled_points_date_fixed.csv')\n",
    "# # Drop the 'TARGET_FID' column from the 'oxnard' DataFrame - shouldn't need this anymore\n",
    "# # oxnard_original = oxnard_original.drop('TARGET_FID', axis=1)\n",
    "# oxnard_add = pd.read_csv(train_path + 'Oxnard_additional_val.csv')\n",
    "# oxnard = pd.concat([oxnard_original, oxnard_add], axis=0, ignore_index=True)\n",
    "# # importing new other trees dataset\n",
    "# othertrees = pd.read_csv(train_path + 'Brandee_Other_Trees_2025.csv')\n",
    "\n",
    "# # Print out unique 'Type' values for each df to check for typos\n",
    "# print('Checking for typos in the Type column:')\n",
    "# # all_dfs = [santamaria, mendocino, watsonville, oxnard]\n",
    "# all_dfs = [santamaria, watsonville, oxnard, othertrees]\n",
    "# for df in all_dfs:\n",
    "#     print(df['Location'].iloc[0])\n",
    "#     print('\\tunique types', df['Type'].unique())\n",
    "#     print('\\tnumber of rows:', df.shape[0])\n",
    "\n",
    "# all_data = pd.concat(all_dfs, axis=0, ignore_index=True)\n",
    "# print('Total number of training points:', all_data.shape[0])\n",
    "# print('\\tunique types', all_data['Type'].unique())\n",
    "# print('Number of rows in each class of Type:')\n",
    "# print(all_data['Type'].value_counts())\n",
    "# # print('unique dates:', all_data['Date'].unique())\n",
    "\n",
    "# # removing unused dates from the training data to get metadata/stats\n",
    "\n",
    "# all_data_test = all_data\n",
    "# # remove dates before 2018-05-09\n",
    "# all_data_test['Date'] = pd.to_datetime(all_data_test['Date']).dt.strftime('%Y-%m-%d')\n",
    "# all_data_test = all_data_test[all_data_test['Date'] > '2018-05-09']\n",
    "# all_data_test = all_data_test[all_data_test['Type'] != 'greenhouse']\n",
    "# print('unique dates:', all_data_test['Date'].unique())\n",
    "# print('Total number of training points:', all_data_test.shape[0])\n",
    "# print('\\tunique types', all_data_test['Type'].unique())\n",
    "# print('Number of rows in each class of Type:')\n",
    "# print(all_data_test['Type'].value_counts())\n",
    "# print(all_data_test['Location'].value_counts())\n",
    "# # display('Watsonville dates: ', all_data_test[all_data_test['Location'] == 'Watsonville']['Date'].unique())\n",
    "# # display('Oxnard dates: ', all_data_test[all_data_test['Location'] == 'Oxnard']['Date'].unique())\n",
    "# # display('Santa Maria dates: ', all_data_test[all_data_test['Location'] == 'Santa Maria']['Date'].unique())\n",
    "\n",
    "# # # print a chart showing the number of points per date, stratified by Location\n",
    "# # # Group the data by 'Date' and 'Location' and count the number of points\n",
    "# # grouped_data = all_data_test.groupby(['Date', 'Location']).size().reset_index(name='Count')\n",
    "# # pivot_data = grouped_data.pivot(index='Date', columns='Location', values='Count').fillna(0)\n",
    "# # pivot_data.plot(kind='bar', stacked=True, figsize=(15, 7))\n",
    "# # plt.title('Number of Points per Date Stratified by Location')\n",
    "# # plt.xlabel('Date')\n",
    "# # plt.ylabel('Number of Points')\n",
    "# # plt.xticks(rotation=45, ha='right')\n",
    "# # plt.legend(title='Location')\n",
    "# # plt.tight_layout()\n",
    "# # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import new field/date split and spatially not-autocorrelated training/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "training = pd.read_csv('data/AllTraining_080425prj_SJfiltered.csv')\n",
    "# combine all black mulch types into one\n",
    "training['Type'] = training['Type'].replace('black mulch', 'blackmulch')\n",
    "# set date column to EE format\n",
    "training['Date'] = pd.to_datetime(training['Date']).dt.strftime('%Y-%m-%d')\n",
    "# remove 2018-05-09 data -- no s2 image for it\n",
    "training = training[training['Date'] != '2018-05-09']\n",
    "\n",
    "# Stats, optional:\n",
    "print('Total number of training points:', training.shape[0])\n",
    "# print('\\tunique types', training['Type'].unique())\n",
    "# print('Number of rows in each class of Type:\\n', training['Type'].value_counts())\n",
    "# print('Unique dates:', training['Date'].unique())\n",
    "# print('Number of rows in each location:\\n', training['Location'].value_counts())\n",
    "# convert date to datetime format for consistency\n",
    "# training['Date'] = pd.to_datetime(training['Date']).dt.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate temporal breakdown of training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print unique month values grouped by Split value\n",
    "# for split_val in sorted(training['Split'].unique()):\n",
    "#     months = sorted(set(pd.to_datetime(training[training['Split'] == split_val]['Date']).dt.strftime('%m')))\n",
    "#     print(f\"Months included in Split {split_val}: {months}\")\n",
    "\n",
    "# # Extract month from 'Date' column\n",
    "training['Month'] = pd.to_datetime(training['Date']).dt.strftime('%m')\n",
    "\n",
    "# # Group by 'Month' and 'Split', then count rows\n",
    "# month_split_counts = training.groupby(['Month', 'Split']).size().unstack(fill_value=0)\n",
    "\n",
    "# # bar chart\n",
    "# month_split_counts.plot(kind='bar', stacked=False)\n",
    "# plt.xlabel('Month')\n",
    "# plt.ylabel('Number of Rows')\n",
    "# plt.title('Number of Rows per Month by Split Value')\n",
    "# plt.legend(title='Split')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# create a new column to test a different test/train split\n",
    "\n",
    "# Calculate the number of rows for each Date_Loc\n",
    "date_loc_counts = training['Date_Loc'].value_counts()\n",
    "\n",
    "# Shuffle the unique Date_Loc values for randomness\n",
    "unique_date_locs = training['Date_Loc'].unique()\n",
    "np.random.seed(87)\n",
    "shuffled_date_locs = np.random.permutation(unique_date_locs)\n",
    "\n",
    "# Calculate the cumulative sum of rows as we add each Date_Loc\n",
    "cumulative_counts = date_loc_counts.loc[shuffled_date_locs].cumsum()\n",
    "# print(f\"Cumulative counts: {cumulative_counts}\")\n",
    "total_rows = date_loc_counts.sum()\n",
    "split_idx = np.argmax(cumulative_counts >= total_rows * 0.7) + 1  # +1 to include the index where threshold is crossed\n",
    "\n",
    "train_date_locs = set(shuffled_date_locs[:split_idx])\n",
    "test_date_locs = set(shuffled_date_locs[split_idx:])\n",
    "\n",
    "# Assign Split_grouped: 1 for train, 2 for test\n",
    "training['Split_grouped'] = training['Date_Loc'].apply(lambda x: 1 if x in train_date_locs else 2)\n",
    "\n",
    "# Check distribution across months\n",
    "month_split_grouped_counts = training.groupby(['Month', 'Split_grouped']).size().unstack(fill_value=0)\n",
    "month_split_grouped_counts.plot(kind='bar', stacked=False)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Rows')\n",
    "plt.title('Number of Rows per Month by Split_grouped Value')\n",
    "plt.legend(title='Split_grouped')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the percentage of points in each split\n",
    "train_pct = (training['Split_grouped'] == 1).mean() * 100\n",
    "test_pct = (training['Split_grouped'] == 2).mean() * 100\n",
    "print(f\"Training: {train_pct:.1f}%  |  Testing: {test_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load EE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# California boundary\n",
    "ca = ee.FeatureCollection(\"TIGER/2018/States\").filter(ee.Filter.equals('NAME', 'California'))\n",
    "# Agricultural areas from CA DWR's crop maps, 2019 to 2022 inclusive (10m raster)\n",
    "# crops = ee.Image('projects/ee-annalisertaylor/assets/TNC/agplastics/dwr_allyears')\n",
    "# Agricultural areas from CA DWR's crop maps, 2019 to 2023 inclusive (10m raster)\n",
    "crops = ee.Image('projects/ee-tnc-annietaylor/assets/agplastics/dwr_allyears_23')\n",
    "# socal region for image clipping\n",
    "socal = ee.FeatureCollection('projects/ee-tnc-annietaylor/assets/agplastics/socal').first()\n",
    "\n",
    "# County boundaries for spatial filtering \n",
    "county_bnd = ee.FeatureCollection(\"TIGER/2016/Counties\").filterBounds(ca)\n",
    "# make features of the counties you want for filtering/analysis\n",
    "ventura = county_bnd.filter(ee.Filter.eq('NAME', 'Ventura'))\n",
    "mendo = county_bnd.filter(ee.Filter.eq('NAME', 'Mendocino'))\n",
    "cruz = county_bnd.filter(ee.Filter.eq('NAME', 'Santa Cruz'))\n",
    "santabarbara = county_bnd.filter(ee.Filter.eq('NAME', 'Santa Barbara'))\n",
    "kern = county_bnd.filter(ee.Filter.eq('NAME', 'Kern'))\n",
    "humboldt = county_bnd.filter(ee.Filter.eq('NAME', 'Humboldt'))\n",
    "sanbenito = county_bnd.filter(ee.Filter.eq('NAME', 'San Benito'))\n",
    "sf = county_bnd.filter(ee.Filter.eq('NAME', 'San Francisco'))\n",
    "# to read all the county options\n",
    "# county_df = geemap.ee_to_df(county_bnd)\n",
    "# pd.set_option('display.max_rows', 75)\n",
    "# county_df['NAME']\n",
    "\n",
    "# Encode labels - ee classifier requires numeric class labels\n",
    "class_mapping = {'hoop': 0, 'mulch': 1, 'other': 2, 'blackmulch': 3}\n",
    "# only watsonville has greenhouse points right now, santa maria has a separate (unimported) dataset\n",
    "# but there are no greenhouse points in the cleaned up training dataset\n",
    "# crops.projection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Process Sentinel-2 Imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud filter the image collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startDate = ee.Date.fromYMD(2010, 1, 1) # setting these to be inclusive for now, can change\n",
    "endDate = ee.Date.fromYMD(2025, 10, 1)\n",
    "\n",
    "s2 = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \\\n",
    "    .filter(ee.Filter.date(startDate, endDate)) \\\n",
    "    .filter(ee.Filter.bounds(ca)) \\\n",
    "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 92))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud filtering with the s2_cloudless collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cloud mask using the cloudless image collection, pulling from EE documentation example\n",
    "s2cloud = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "s2cloud = s2cloud.filterBounds(ca)\n",
    "MAX_CLOUD_PROBABILITY = 15\n",
    "\n",
    "def maskClouds(img):\n",
    "    clouds = ee.Image(img.get('cloud_mask')).select('probability')\n",
    "    isNotCloud = clouds.lt(MAX_CLOUD_PROBABILITY)\n",
    "    return img.updateMask(isNotCloud)\n",
    "\n",
    "# Masks for the 10m bands can include bad data at scene edge; apply masks from the 20m and 60m bands too\n",
    "def maskEdges(s2_img):\n",
    "    return s2_img.updateMask(s2_img.select('B8A').mask().updateMask(s2_img.select('B9').mask()))\n",
    "\n",
    "# So I can investigate how this is performing above plastic cover\n",
    "def addCloudless(img):\n",
    "    cloud_prob = ee.Image(img.get('cloud_mask')).select('probability')\n",
    "    return img.addBands(cloud_prob.rename('cloud_probability'))\n",
    "\n",
    "# Filter s2cloud to same region, fix 10m pixels at edges\n",
    "s2_mask = s2.map(maskEdges)\n",
    "\n",
    "# Join S2 with cloud probability dataset to add cloud mask\n",
    "# cloudless image is saved as a property called 'cloud_mask' in the original image\n",
    "s2sr_cloudmask = ee.Join.saveFirst('cloud_mask').apply(primary=s2_mask, secondary=s2cloud, \\\n",
    "                            condition=ee.Filter.equals(leftField='system:index', rightField='system:index'))\n",
    "\n",
    "# create the cloud masked image collection\n",
    "s2_cldmsk = ee.ImageCollection(s2sr_cloudmask).map(maskClouds)\n",
    "\n",
    "# create an image collection with the cloud probability band, not yet masked\n",
    "# this is for testing to see what values certain pixels have\n",
    "s2_cld_test = ee.ImageCollection(s2sr_cloudmask).map(addCloudless)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud filtering with the QA band \n",
    "Checked this out after seeing problems with the s2_cloudless filter, but apparently this band doesn't contain any info from feb 2022 to feb 2024 so it's not usable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_s2_clouds(image):\n",
    "   qa = image.select('QA60')\n",
    "   # Bits 10 and 11 are clouds and cirrus, respectively.\n",
    "   cloud_bit_mask = 1 << 10\n",
    "   cirrus_bit_mask = 1 << 11\n",
    "   # Both flags should be set to zero, indicating clear conditions.\n",
    "   mask = (qa.bitwiseAnd(cloud_bit_mask).eq(0).And(qa.bitwiseAnd(cirrus_bit_mask).eq(0)))\n",
    "   return image.updateMask(mask)\n",
    "\n",
    "s2_qa_cldmsk = s2.map(mask_s2_clouds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud filtering with MSK_CLDPRB band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_threshold = 10\n",
    "\n",
    "def mask_s2_clouds_cm(image):\n",
    "   cm = image.select('MSK_CLDPRB')\n",
    "   isNotCloud = cm.lt(MAX_CLOUD_PROBABILITY)\n",
    "   return image.updateMask(isNotCloud)\n",
    "\n",
    "s2_cm_cldmsk = s2.map(mask_s2_clouds_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud filtering with the SCL band -- this is what we're using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 = Clouds Medium Probability\n",
    "# 9 = Clouds High Probability\n",
    "# 10 = Cirrus\n",
    "\n",
    "def mask_s2_clouds_scl(image):\n",
    "    scl = image.select('SCL')\n",
    "    isNotCloud = scl.neq(8).And(scl.neq(9).And(scl.neq(10)))\n",
    "    return image.updateMask(isNotCloud)\n",
    "\n",
    "s2_scl_cldmsk = s2.map(mask_s2_clouds_scl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cloud filtering with google cloud score function\n",
    "https://courses.spatialthoughts.com/end-to-end-gee.html#batch-exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Cloud Score+ collection, now available 2015 onwards\n",
    "csPlus = ee.ImageCollection('GOOGLE/CLOUD_SCORE_PLUS/V1/S2_HARMONIZED')\n",
    "csPlusBands = csPlus.first().bandNames() # ['cs', 'cs_cdf']\n",
    "\n",
    "# join cloud score+ with s2 collection\n",
    "filteredS2WithCs = s2.linkCollection(csPlus, csPlusBands)\n",
    "\n",
    "# display('Filtered S2 with CS+ bands:', filteredS2WithCs.first())\n",
    "\n",
    "# Function to mask pixels with low CS+ QA scores.\n",
    "def mask_s2_clouds_gcs(image):\n",
    "  qaBand = 'cs' # probability of clear sky\n",
    "  clearThreshold = 0.5\n",
    "  mask = image.select(qaBand).gte(clearThreshold)\n",
    "  return image.updateMask(mask)\n",
    "\n",
    "s2_gcs_cldmsk = filteredS2WithCs.map(mask_s2_clouds_gcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined SCL + Cloud Score filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to mask pixels with low CS+ QA scores and classified as clouds by SCL band\n",
    "def mask_s2_clouds_combo(image):\n",
    "  # GCS\n",
    "  qaBand = 'cs' # probability of clear sky\n",
    "  clearThreshold = 0.5\n",
    "  mask = image.select(qaBand).gte(clearThreshold)\n",
    "  image = image.updateMask(mask)\n",
    "  # SCL\n",
    "  scl = image.select('SCL')\n",
    "  isNotCloud = scl.neq(8).And(scl.neq(9).And(scl.neq(10)))\n",
    "  return image.updateMask(isNotCloud)\n",
    "\n",
    "s2_combo_cldmsk = filteredS2WithCs.map(mask_s2_clouds_combo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vis params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbVis = {'min': 400, 'max': 1500, 'bands': ['B4', 'B3', 'B2']}\n",
    "RPGI_viz = {'min': -0.9, 'max': -0.4, 'palette': ['#4d004b', '#f7fcfd']}\n",
    "NDTI_viz = {'min': 0.0, 'max': 0.2, 'palette': ['white', 'yellow', 'orange']}\n",
    "# visualize the classified plastic\n",
    "plastic_viz = {'palette': ['FF5733', 'F4F31D', '2596be', '#333333'], 'min': 0, 'max': 3}\n",
    "et_viz = {\n",
    "  'min': 0,\n",
    "  'max': 100,\n",
    "  'palette': [\n",
    "    '9e6212', 'ac7d1d', 'ba9829', 'c8b434', 'd6cf40', 'bed44b', '9fcb51',\n",
    "    '80c256', '61b95c', '42b062', '45b677', '49bc8d', '4dc2a2', '51c8b8',\n",
    "    '55cece', '4db4ba', '459aa7', '3d8094', '356681', '2d4c6e',\n",
    "  ]\n",
    "}\n",
    "band_viz = {'min':603, 'max': 2725}\n",
    "crop_viz = {'palette': ['#FFA500']}\n",
    "crop_viz2 = {'palette': [\"#FFEE00\",\"#FF7300\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add VIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addVIs(image):\n",
    "    image = image.addBands(image.normalizedDifference(['B8', 'B4']).rename('NDVI'))\n",
    "    # image = image.addBands(image.normalizedDifference(['B8', 'B12']).rename('NBR')) # one band at 20m\n",
    "    image = image.addBands(image.normalizedDifference(['B8', 'B11']).rename('NDMI')) # one band at 20m\n",
    "    image = image.addBands(image.normalizedDifference(['B3', 'B8']).rename('NDWI'))\n",
    "    # image = image.addBands(image.expression(\"2.5 * ((b('B8') - b('B4')) / (b('B8') + 6 * b('B4') - 7.5 * b('B2') + 1))\").rename('EVI'))\n",
    "    image = image.addBands(image.normalizedDifference(['B11', 'B12']).rename('NDTI')) # swir1 and swir2, both 20m, tillage index\n",
    "    image = image.addBands(image.expression(\"100 * (b('B2') * (b('B8') - b('B4')))/(1 - ((b('B2') + b('B3') + b('B8')) / 3))\").rename('PGI')) # all 10m\n",
    "    image = image.addBands(image.expression(\"b('B2')/(1 - ((b('B2') + b('B3') + b('B8')) / 3))\").rename('RPGI'))\n",
    "    image = image.addBands(image.normalizedDifference(['B11', 'B4']).rename('PMLI')) # one band at 20m\n",
    "    image = image.addBands(image.normalizedDifference(['B11', 'B8']).rename('NDBI')) # one band at 20m   \n",
    "    return image\n",
    "\n",
    "def adjPGI(image):\n",
    "    # TODO check if this threshold for NDBI is too high\n",
    "    # set all bands in the image to 0 where NDVI is higher than 0.73 OR when NDBI is higher than 0.005\n",
    "    PGI_adj = image.where(image.select('NDVI').gt(0.73), 0)\n",
    "    PGI_adj = PGI_adj.where(PGI_adj.select('NDBI').gte(0.005), 0)\n",
    "    # extract the 'PGI' band from that modified image, and add it to the original image with a new name\n",
    "    image = image.addBands(PGI_adj.select('PGI').rename('PGI_adj'))   \n",
    "    return image\n",
    "\n",
    "# not using this right now, keeping it as a useful example for downsampling the NIR band\n",
    "# will be more precise if we decide to publish any of these data\n",
    "def addNBR_RS(image):\n",
    "    # reduces the resolution of the NIR band (10m) to match the SWIR band (20m)\n",
    "    #   using the mean of all pixels within the larger pixel\n",
    "    swir = image.select('B12')\n",
    "    swir_projection = swir.projection()\n",
    "    nir = image.select('B8').reduceResolution(reducer=ee.Reducer.mean(), maxPixels=16).reproject(crs=swir_projection)\n",
    "    js_math_exp = 'c = (a-b) / (a+b)'\n",
    "    js_math_img = ee.Image().expression(\n",
    "        expression=js_math_exp, opt_map={'a': nir, 'b': swir}\n",
    "    ).rename('NBR_RS')\n",
    "    image = image.addBands(js_math_img)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one for each method of cloud filtering, for comparison\n",
    "# s2_cloudless\n",
    "processedCollection_cld = s2_cldmsk.map(addVIs)\n",
    "processedCollection_cld = processedCollection_cld.map(adjPGI)\n",
    "# scl map\n",
    "processedCollection_scl = s2_scl_cldmsk.map(addVIs)\n",
    "processedCollection_scl = processedCollection_scl.map(adjPGI)\n",
    "# cloud score+ map\n",
    "processedCollection_gcs = s2_gcs_cldmsk.map(addVIs)\n",
    "processedCollection_gcs = processedCollection_gcs.map(adjPGI)\n",
    "# combo cloud score + scl\n",
    "processedCollection_combo = s2_combo_cldmsk.map(addVIs)\n",
    "processedCollection_combo = processedCollection_combo.map(adjPGI)\n",
    "# other cloud masks for testing\n",
    "processedCollection_qa = s2_qa_cldmsk.map(addVIs)\n",
    "processedCollection_cm = s2_cm_cldmsk.map(addVIs)\n",
    "\n",
    "# Choose which cloud filtered collection to use for all analysis\n",
    "processedCollection = processedCollection_combo\n",
    "\n",
    "# display(processedCollection)\n",
    "# print(processedCollection.size().getInfo())\n",
    "# print(processedCollection.first().bandNames().getInfo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare cloud filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# co = ventura # ventura, mendo, cruz, santabarbara\n",
    "\n",
    "# images_original = s2.filterBounds(co) #original images\n",
    "\n",
    "# images_cld = processedCollection_cld.filterBounds(co) # s2 cloudless filter\n",
    "# images_cloudless = s2_cld_test.filterBounds(co) # get the original image with s2_cloudless probability added - not masked yet\n",
    "# images_scl = processedCollection_scl.filterBounds(co)\n",
    "# images_gcs = processedCollection_gcs.filterBounds(co) # gcs cloud filtered\n",
    "# images_combo = processedCollection_combo.filterBounds(co) # combo cloud filtered\n",
    "# images_qa = processedCollection_qa.filterBounds(co) # no adjpgi\n",
    "# images_cm = processedCollection_cm.filterBounds(co) # no adjpgi\n",
    "\n",
    "# start = '2022-01-16' # sf cloudy image = 2022-01-01?, clouds in santa maria = 2020-09-26 \n",
    "# end = '2022-01-17'\n",
    "\n",
    "# original = images_original.filterDate(start, end)#.first()\n",
    "# original_cloudless = images_cloudless.filterDate(start, end).first()\n",
    "# cloudless = images_cld.filterDate(start, end).first()\n",
    "# scl = images_scl.filterDate(start, end)#.first()\n",
    "# gcs = images_gcs.filterDate(start, end)#.first()\n",
    "# combo = images_combo.filterDate(start, end)#.first()\n",
    "# qa = images_qa.filterDate(start, end).first()\n",
    "# cm = images_cm.filterDate(start, end).first()\n",
    "\n",
    "# m = geemap.Map()\n",
    "# m.addLayer(original_cloudless, rgbVis, 'original image + cloudless', False)\n",
    "# m.addLayer(cloudless, rgbVis, 's2 cloudless', False)\n",
    "# m.addLayer(scl, rgbVis, 'cloud filtered SCL', False)\n",
    "# m.addLayer(gcs, rgbVis, 'cloud filtered GCS', False)\n",
    "# m.addLayer(combo, rgbVis, 'cloud filtered combo', False)\n",
    "# m.addLayer(qa, rgbVis, 'cloud filtered QA60', False)\n",
    "# m.addLayer(cm, rgbVis, 'cloud filtered CLDPRB', False)\n",
    "# m.addLayer(original, rgbVis, 'original image')\n",
    "# m.centerObject(co, 10)\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Trt-ehzwV5kt",
    "outputId": "e0fb3665-aa93-4fb0-997e-89d2b74db348"
   },
   "outputs": [],
   "source": [
    "# Select bands to sample from the sentinel-2 images\n",
    "bands = [\n",
    "    'B4', 'B3', 'B2', 'B6', 'B8', 'B11', 'B12',\n",
    "    'NDVI', 'NDTI', 'PGI', 'RPGI', 'PMLI', 'NDBI', 'PGI_adj', 'NDWI', 'NDMI'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Match training data dates to clear sentinel-2 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matching training data to sentinel images, first clear image on or after the point date\n",
    "date_matching_wats = {\n",
    "    'points_date': ['2020-09-26', '2021-09-27', '2022-09-11', '2023-04-12', '2019-11-17', '2023-07-09'],\n",
    "    'image_date': ['2020-09-26', '2021-10-01', '2022-09-16', '2023-04-14', '2019-12-16', '2023-07-28'],\n",
    "    'location': ['Watsonville']*6\n",
    "}\n",
    "date_matching_mendo = {\n",
    "    'points_date': ['2021-06-02'],\n",
    "    'image_date': ['2021-06-06'],\n",
    "    'location': ['Mendocino']*1\n",
    "}\n",
    "date_matching_maria = {\n",
    "    'points_date': ['2022-08-26', '2023-05-05', '2022-03-10', '2021-02-06'],\n",
    "    'image_date': ['2022-08-29', '2023-05-11', '2022-03-12', '2021-02-10'],\n",
    "    'location': ['Santa Maria']*4\n",
    "}\n",
    "date_matching_oxnard = {\n",
    "    # as of 11.13.24, all oxnard dates are added\n",
    "    # For 5-6-23, can use 5/11 (has clouds but not on those points specifically) or 4/21,\n",
    "    # cloud free but 15 days prior\n",
    "    # for 8-1-2023, clouds far from those specific points, take care if adding more points\n",
    "    'points_date': ['2023-10-17', '2023-05-06', '2023-11-09', '2019-05-17', '2022-06-13',\n",
    "                    '2021-07-13', '2022-10-01', '2022-11-01', '2019-10-14', '2020-02-15',\n",
    "                    '2023-08-01', '2021-02-28', '2023-05-07', '2019-08-19', '2023-05-11',\n",
    "                    '2019-04-01', '2022-09-21', '2018-11-19'],\n",
    "    'image_date': ['2023-10-28', '2023-05-11', '2023-11-09', '2019-05-17', '2022-06-15',\n",
    "                   '2021-07-15', '2022-10-03', '2022-11-02', '2019-10-24', '2020-02-16',\n",
    "                   '2023-08-01', '2021-03-02', '2023-05-06', '2019-08-20', '2023-05-11',\n",
    "                   '2019-04-02', '2022-09-23', '2018-12-15'],\n",
    "    'location': ['Oxnard']*18\n",
    "}\n",
    "\n",
    "date_matching_other = {\n",
    "    'points_date': ['2023-05-26', '2023-06-29', #morgan hill\n",
    "                    '2023-04-27', '2022-03-10', #santa maria\n",
    "                    '2023-11-09', #oxnard\n",
    "                    '2023-07-13', '2023-07-08', #santa barbara\n",
    "                    '2023-08-01', '2022-10-08', '2022-12-23'#maricopa\n",
    "                    ],\n",
    "    'image_date': ['2023-06-03', '2023-07-03',\n",
    "                   '2023-05-11', '2022-03-12',\n",
    "                   '2023-11-09',\n",
    "                   '2023-07-15', '2023-07-10',\n",
    "                   '2023-08-01', '2022-10-08', '2023-01-06'\n",
    "                   ],\n",
    "\n",
    "    'location': ['CA']*10\n",
    "}\n",
    "\n",
    "# Convert combined dictionary to dataframe\n",
    "dates_wats = pd.DataFrame.from_dict(date_matching_wats)\n",
    "dates_mendo = pd.DataFrame.from_dict(date_matching_mendo)\n",
    "dates_maria = pd.DataFrame.from_dict(date_matching_maria)\n",
    "dates_oxnard = pd.DataFrame.from_dict(date_matching_oxnard)\n",
    "dates_other = pd.DataFrame.from_dict(date_matching_other)\n",
    "# print(dates_wats, '\\n', dates_mendo, '\\n', dates_maria, '\\n', dates_oxnard)\n",
    "\n",
    "# combine dfs vertically\n",
    "date_matching = pd.concat([dates_wats, dates_mendo, dates_maria, dates_oxnard, dates_other], axis=0, ignore_index=True)\n",
    "# print(date_matching.sort_values(by='points_date'))\n",
    "\n",
    "# This is to check points on the map to get good image matching\n",
    "# co = cruz # ventura, mendo, cruz, santabarbara\n",
    "\n",
    "# # option to import points from csv to show them on the map\n",
    "# watsonville_points = \"N:/OCEANS_Program/Plastics/Agricultural_Plastics/Validated Training Data/Watsonville_val_BM.csv\"\n",
    "# oxnard_points = \"N:/OCEANS_Program/Plastics/Agricultural_Plastics/Validated Training Data/YYT_RElabeled_points_date_fixed.csv\"\n",
    "# oxnard_points = \"N:/OCEANS_Program/Plastics/Agricultural_Plastics/Validated Training Data/oxnard_onedate.csv\"\n",
    "\n",
    "# date = '2018-05-30'\n",
    "# nextday = ee.Date(date).advance(1, 'day').format('YYYY-MM-dd')\n",
    "# oneday = processedCollection.filterDate(date, nextday).median()\n",
    "# oneday_original = s2.filterDate(date, nextday).median()\n",
    "\n",
    "# first_cruz = ee.Image(processedCollection.filterBounds(co).first())\n",
    "# display(first_cruz)\n",
    "\n",
    "# m = geemap.Map()\n",
    "# # m.add_basemap('HYBRID')\n",
    "# m.addLayer(oneday_original, rgbVis, 'original image')\n",
    "# m.addLayer(oneday, rgbVis, 'cloud filtered')\n",
    "# # m.add_points_from_xy(oxnard_points, x=\"Longitude\", y=\"Latitude\")\n",
    "# m.centerObject(co, 10)\n",
    "# # m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### calculate date gaps for manuscript -- archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert columns to datetime format\n",
    "# date_matching['points_date'] = pd.to_datetime(date_matching['points_date'])\n",
    "# date_matching['image_date'] = pd.to_datetime(date_matching['image_date'])\n",
    "\n",
    "# # Calculate the difference in days\n",
    "# date_matching['days_difference'] = (date_matching['image_date'] - date_matching['points_date']).dt.days\n",
    "\n",
    "# print(date_matching)\n",
    "# print(date_matching['days_difference'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evapotranspiration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ee.ImageCollection('OpenET/ENSEMBLE/CONUS/GRIDMET/MONTHLY/v2_0').filterBounds(ca).select('et_ensemble_mad')\n",
    "\n",
    "# https://developers.google.com/earth-engine/datasets/catalog/OpenET_ENSEMBLE_CONUS_GRIDMET_MONTHLY_v2_0#bands\n",
    "# Using the ensemble value of ET = 'et_ensemble_mad'\n",
    "\n",
    "# The ET data only goes through the end of 2024, which is okay with our current training data\n",
    "# the function will error if we try to sample in 2025 (probably an 'image has no bands' error)\n",
    "\n",
    "date = ee.Date('2022-07-17')\n",
    "date = ee.Date(date.format('yyyy-MM-01'))\n",
    "\n",
    "data = et.filterDate(date, date.advance(1, 'month'))\n",
    "# display(data)\n",
    "\n",
    "m = geemap.Map()\n",
    "m.setCenter(-119.17, 34.19, 6)\n",
    "m.addLayer(data, et_viz, 'Ensemble ET')\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satellite embeddings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = ee.ImageCollection('GOOGLE/SATELLITE_EMBEDDING/V1/ANNUAL').filterBounds(ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the projections of sentinel-2 and ET\n",
    "These are used later as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ET data doesn't change projection since we don't composite it\n",
    "# data.first().bandNames()\n",
    "ET_projection = ee.Image(et.first()).projection() # only one band\n",
    "# display(ET_projection)\n",
    "\n",
    "# projection of S2 data is the same before and after processing\n",
    "# display(processedCollection_combo.first().select('B4').projection().getInfo())\n",
    "# display(s2.first().select('B4').projection().getInfo())\n",
    "\n",
    "S2_projection = processedCollection.first().select('B4').projection()\n",
    "# display(S2_projection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get S2 and ET data for the same date\n",
    "This was code to test the workflow, needs to be incorporated below\n",
    "This is copied from later in the workflow when I'm classifying every S2 image across the water year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wy = 2024\n",
    "# create dates\n",
    "stdate = f'{wy}-10-10'\n",
    "endate = f'{wy}-10-30' # end date is exclusive\n",
    "\n",
    "# Example coordinates (longitude, latitude)\n",
    "lon, lat = -119.24912, 34.22826\n",
    "point = ee.Geometry.Point([lon, lat])\n",
    "\n",
    "# I want a image for ventura for testing purposes\n",
    "s2_oneday = processedCollection.filterDate(stdate, endate).filterBounds(point).first().setDefaultProjection(S2_projection)\n",
    "# display(s2_oneday.select('B12').projection().getInfo())\n",
    "\n",
    "# then add corresponding ET data for each month\n",
    "def add_ET(image):\n",
    "    image_sts = image.get('system:time_start')\n",
    "    date = image.date()\n",
    "    et_date = ee.Date(ee.Date(date).format('yyyy-MM-01')) #convert to first of month for ET data\n",
    "    et_img = et.filterDate(et_date, et_date.advance(1, 'month')).median().rename('ET').setDefaultProjection(ET_projection) # new\n",
    "    return image.addBands(et_img).set('system:time_start', image_sts)\n",
    "\n",
    "stacked = add_ET(s2_oneday)\n",
    "\n",
    "# redundant code to get the et_img to add to map below\n",
    "image_sts = s2_oneday.get('system:time_start')\n",
    "date = s2_oneday.date()\n",
    "et_date = ee.Date(ee.Date(date).format('yyyy-MM-01')) #convert to first of month for ET data\n",
    "et_img = et.filterDate(et_date, et_date.advance(1, 'month')).median().rename('ET').setDefaultProjection(ET_projection) # new\n",
    "# et_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample to a grid, including crop mask\n",
    "Using 0.75% coverage of old crop mask to new grid crop mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the grid size and projection\n",
    "gridScale = 30\n",
    "# CA teale albers = applicable to all CA\n",
    "# gridProjection = ee.Projection('EPSG:3310').atScale(gridScale)\n",
    "# UTM zone 11 = socal\n",
    "gridProjection = ee.Projection('EPSG:32611').atScale(gridScale)\n",
    "\n",
    "# Set the resampling mode\n",
    "# stacked = stacked.resample('bilinear')\n",
    "\n",
    "# Aggregate pixels with 'mean' statistics\n",
    "stackedResampled = stacked.reduceResolution(\n",
    "  reducer = ee.Reducer.mean(),\n",
    "  maxPixels = 1024\n",
    ").reproject(\n",
    "    crs = gridProjection\n",
    ")\n",
    "\n",
    "# unmask(0) replaces all masked (None) pixels with 0 so i can use min reducer\n",
    "filled = crops.unmask(0)\n",
    "\n",
    "# Convert all non-zero values to 1\n",
    "binary_crop = filled.gt(0)  # gt(0) returns 1 if >0, else 0\n",
    "\n",
    "cropsResampled = binary_crop.reduceResolution(ee.Reducer.mean()).reproject(crs = gridProjection)\n",
    "\n",
    "# if you reduce by mean, values are the percentage of crop cover in the larger pixel\n",
    "# testing a good threshold to filter out roads etc. \n",
    "cropsResampled = cropsResampled.gt(0.75)\n",
    "\n",
    "m = geemap.Map()\n",
    "m.add_basemap('SATELLITE')\n",
    "m.centerObject(stackedResampled, 10)\n",
    "\n",
    "m.addLayer(stackedResampled.select(['B4', 'B3', 'B2']), rgbVis, 'resamp S2', False)\n",
    "m.addLayer(s2_oneday, rgbVis, 'old S2', False)\n",
    "m.addLayer(stackedResampled.select(['B4', 'B3', 'B2']).mask(crops), rgbVis, 'old mask s2', False)\n",
    "m.addLayer(stackedResampled.select(['B4', 'B3', 'B2']).mask(cropsResampled), rgbVis, 'new mask s2')\n",
    "m.addLayer(cropsResampled.mask(cropsResampled), crop_viz, 'resamp crops', True)\n",
    "m.addLayer(crops, crop_viz2, 'old crops', False)\n",
    "# m.addLayer(stackedResampled.select('ET'), et_viz, 'resamp ET')\n",
    "# m.addLayer(et_img, et_viz, 'old ET')\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare resolutions of the three bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2022-03-12'\n",
    "nextday = ee.Date(date).advance(1, 'day').format('YYYY-MM-dd')\n",
    "oneday = processedCollection.filterDate(date, nextday).median()\n",
    "# oneday_original = s2.filterDate(date, nextday).median()\n",
    "\n",
    "m = geemap.Map()\n",
    "m.setCenter(-119.17, 34.19, 6)\n",
    "m.addLayer(data, et_viz, 'Ensemble ET')\n",
    "m.addLayer(oneday.select('B12'), band_viz, 'B12') # SWIR2, 20m\n",
    "m.addLayer(oneday.select('B8'), band_viz, 'B8') # NIR, 10m\n",
    "# m\n",
    "\n",
    "# print(processedCollection_scl.first().select('B8').projection().getInfo())\n",
    "# print(processedCollection_scl.first().select('B12').projection().nominalScale().getInfo())\n",
    "# data.first().projection()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### archive code that didn't work to resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = ee.Image(processedCollection_scl.first())\n",
    "\n",
    "# et_image = et.first()\n",
    "# et_proj = et_image.projection()\n",
    "# et_crs = et_proj.crs().getInfo()\n",
    "# et_transform = et_image.projection().getInfo()['transform']\n",
    "# et_transform = et_proj.transform().getInfo()  # This must be a list of 6 floats\n",
    "\n",
    "# Confirm it's a list of 6 floats\n",
    "# print(\"ET transform:\", et_transform)\n",
    "# print(\"ET CRS:\", et_crs)\n",
    "\n",
    "# s2_projection_10m = test.select('B8').projection()\n",
    "# s2_projection_20m = test.select('B12').projection()\n",
    "\n",
    "# s2_resampled_10m = (\n",
    "#     test.setDefaultProjection(s2_projection_10m)\n",
    "#     .reduceResolution(reducer=ee.Reducer.mean(), maxPixels=1024)\n",
    "#     .reproject(crs=et_crs, crsTransform=et_transform)\n",
    "# )\n",
    "\n",
    "# s2_resampled_20m = (\n",
    "#     test.setDefaultProjection(s2_projection_20m)\n",
    "#     .reduceResolution(reducer=ee.Reducer.mean(), maxPixels=1024)\n",
    "#     .reproject(crs=et_crs, crsTransform=et_transform)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert the sentinel image to the same projection as the ET data\n",
    "# test = ee.Image(processedCollection_scl.first())\n",
    "\n",
    "# et_projection = et.first().projection()\n",
    "# et_transform = et_image.projection().getInfo()['transform']\n",
    "# et_crs = et_projection.crs().getInfo()\n",
    "# s2_projection_10m = test.select('B8').projection() #this is the projection of the 10m band\n",
    "# s2_projection_20m = test.select('B12').projection() # this is the projection of the 20m band\n",
    "\n",
    "# # print('ET projection:', et_projection.getInfo())\n",
    "# # print('S2 projection 10m:', s2_projection_10m.getInfo())  \n",
    "# # print('S2 projection 20m:', s2_projection_20m.getInfo())  \n",
    "\n",
    "# s2_resampled_10m = (\n",
    "#     test.setDefaultProjection(s2_projection_10m)\n",
    "#     # Force the next reprojection to aggregate instead of resampling\n",
    "#     .reduceResolution(reducer=ee.Reducer.mean())\n",
    "#     # Request the data at the scale and projection of the ET image\n",
    "#     .reproject(crs=et_crs, crsTransform=et_transform)\n",
    "# )\n",
    "\n",
    "# s2_resampled_20m = (\n",
    "#     test.setDefaultProjection(s2_projection_20m)\n",
    "#     # Force the next reprojection to aggregate instead of resampling\n",
    "#     .reduceResolution(reducer=ee.Reducer.mean())\n",
    "#     # Request the data at the scale and projection of the ET image\n",
    "#     .reproject(crs=et_crs, crsTransform=et_transform)\n",
    "# )\n",
    "\n",
    "# # print('Resampled 10m band projection:', s2_resampled_10m.select('B8').projection().getInfo())\n",
    "# # print('Resampled 20m band projection:', s2_resampled_20m.select('B12').projection().getInfo())\n",
    "\n",
    "# m = geemap.Map()\n",
    "# m.setCenter(-119.17, 34.19, 6)\n",
    "# m.centerObject(test, 10)\n",
    "# m.addLayer(data, et_viz, 'Ensemble ET')\n",
    "# # m.addLayer(oneday.select('B12'), band_viz, 'B12', False) # SWIR2, 20m\n",
    "# # m.addLayer(oneday.select('B8'), band_viz, 'B8', False) # NIR, 10m\n",
    "# m.addLayer(test.select('B12'), band_viz, 'B12 original') # SWIR, 20m\n",
    "# m.addLayer(test.select('B8'), band_viz, 'B8 original') # NIR, 10m\n",
    "# m.addLayer(s2_resampled_10m.select('B8'), band_viz, 'resampled 10m') #NIR, 30m\n",
    "# m.addLayer(s2_resampled_20m.select('B12'), band_viz, 'resampled 20m') #SWIR, 30m\n",
    "# m.addLayer(s2_resampled_20m_2.select('B12'), band_viz, 'resampled 20m method 2') #SWIR, 30m\n",
    "# # m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample to new training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add matching sentinel image date to training dataset points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training is my df of points with lat/longs, type label, date of high res image, and location \n",
    "# date_matching_loc matches the training points by date and location to the sentinel images\n",
    "\n",
    "# display(training.shape[0])\n",
    "\n",
    "# Ensure 'Date' in training is in YYYY-MM-DD format for matching\n",
    "training['Date'] = pd.to_datetime(training['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Copy date_matching and convert 'points_date' to YMD format for matching\n",
    "date_matching_loc = date_matching.copy()\n",
    "date_matching_loc['points_date'] = pd.to_datetime(date_matching_loc['points_date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Merge on both 'Date' (training) <-> 'points_date' and 'Location' <-> 'location'\n",
    "training_merged = training.merge(\n",
    "    date_matching_loc[['points_date', 'location', 'image_date']],\n",
    "    left_on=['Date', 'Location'],\n",
    "    right_on=['points_date', 'location'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# format image_date to YYYY-MM-DD\n",
    "training_merged['image_date'] = pd.to_datetime(training_merged['image_date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Now training['image_date'] contains the matched image date for each point\n",
    "display(training_merged.head())\n",
    "\n",
    "# print unique date_loc values\n",
    "# print('Unique date/location pairs in training data:', training_merged['Date_Loc'].unique())\n",
    "\n",
    "# cut out region column\n",
    "training_merged = training_merged.drop(columns=['Region'])\n",
    "\n",
    "# check if training_merged has any NaN values\n",
    "# print('NaN values:', training_merged.isnull().sum())\n",
    "\n",
    "# # which date_loc pairs have nan values for image_date?\n",
    "# nan_image_dates = training_merged[training_merged['image_date'].isnull()][['Date', 'Location']]\n",
    "# print('Date/Location pairs with no matching image date:')\n",
    "# print(nan_image_dates)\n",
    "\n",
    "# filter training_merged to only include rows with non-null image_date\n",
    "# training_merged = training_merged[training_merged['image_date'].notnull()]\n",
    "# print the number of rows in training_merged\n",
    "print('Number of rows in training_merged after filtering:', training_merged.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dates(df):\n",
    "    # add a 'class' column to the df by mapping the 'Type' column to numeric values\n",
    "    df['class'] = df['Type'].map(class_mapping)\n",
    "\n",
    "    date_locations = df['Date_Loc'].unique()\n",
    "\n",
    "    all_area_data = []\n",
    "\n",
    "    # Iterate through all of the dates in the df\n",
    "    for dl in date_locations:\n",
    "        # Get df rows for that date/location pair\n",
    "        df_dateloc = df[df['Date_Loc'] == dl]\n",
    "        \n",
    "        # print(f\"DF: {len(df_dateloc)} rows for date/location: {dl}\") # test\n",
    "\n",
    "        fc_date = geemap.df_to_ee(df_dateloc, latitude='Latitude', longitude='Longitude')\n",
    "\n",
    "        # print('FC:', fc_date.size().getInfo()) # test\n",
    "\n",
    "        # Get properties for this date/location pair\n",
    "        date = df_dateloc['Date'].iloc[0]\n",
    "        image_date = df_dateloc['image_date'].iloc[0]\n",
    "        location = df_dateloc['Location'].iloc[0]\n",
    "        # Filter images to only that one corresponding sampling date\n",
    "        image_nextdate = ee.Date(image_date).advance(1, 'day').format('YYYY-MM-dd')\n",
    "        sample_image = processedCollection.filterDate(image_date, image_nextdate).median()\n",
    "        # Sample that image to the fc, keeping class property\n",
    "        data = sample_image.select(bands).sampleRegions(\n",
    "            collection=fc_date,\n",
    "            properties=['class', 'Latitude', 'Longitude', 'Split', 'Split_grouped'],\n",
    "            scale=10, #Reduces points within 10m of each other, should not be any\n",
    "            geometries=True\n",
    "        )\n",
    "\n",
    "        # print('S2 sampled FC:', data.size().getInfo()) # test\n",
    "\n",
    "        # Also sample the monthly ET (30m scale, from landsat and other weather data)\n",
    "        et_date = ee.Date(ee.Date(image_date).format('yyyy-MM-01')) #convert to first of month for ET data\n",
    "        et_img = et.filterDate(et_date, et_date.advance(1, 'month')).median().rename('ET')\n",
    "        data = et_img.sampleRegions(\n",
    "            collection=data,\n",
    "            scale=30, #Reduces points within 30m of each other, should not be any\n",
    "            geometries=True\n",
    "            # keep all properties from sampling points\n",
    "        )\n",
    "\n",
    "        # print('ET sampled FC:', data.size().getInfo()) # test\n",
    "\n",
    "        # sample from the satellite embedding dataset from alphaearth\n",
    "        # Get just the year of the image_date\n",
    "        embed_year = ee.Date(ee.Date(image_date).format('yyyy-01-01'))\n",
    "        embed_img = embed.filterDate(embed_year, embed_year.advance(1, 'year')).mosaic()\n",
    "        data = embed_img.sampleRegions(\n",
    "            collection=data,\n",
    "            scale=10,\n",
    "            geometries=True\n",
    "        )\n",
    "\n",
    "        # Add point date, sampled image date, and location as a property to each feature in the feature collection\n",
    "        data = data.map(lambda feature: feature.set({\n",
    "            'date': date,\n",
    "            'image_date': image_date,\n",
    "            'location': location\n",
    "            }))\n",
    "        \n",
    "        # Append to list of fcs for each date/location pair\n",
    "        all_area_data.append(data)\n",
    "\n",
    "    # Flatten into one fc for the dataset\n",
    "    area_fc = ee.FeatureCollection(all_area_data).flatten()\n",
    "\n",
    "    # print('Total resulting features:', area_fc.size().getInfo()) # test\n",
    "\n",
    "    return area_fc\n",
    "\n",
    "##### Testing\n",
    "# # Filter to two unique Date_Loc pairs for testing\n",
    "# two_pairs = training_merged['Date_Loc'].unique()[10:17]\n",
    "# training_merged_subset = training_merged[training_merged['Date_Loc'].isin(two_pairs)]\n",
    "\n",
    "# training_merged_subset_sampled = sample_dates(training_merged_subset)\n",
    "# training_merged_subset_sampled\n",
    "\n",
    "# # convert that to dataframe\n",
    "# training_merged_subset_sampled_df = geemap.ee_to_df(training_merged_subset_sampled)\n",
    "# training_merged_subset_sampled_df.head()\n",
    "\n",
    "#### Full sampling\n",
    "# Sample the training data for all dates/locations\n",
    "training_merged_sampled = sample_dates(training_merged)\n",
    "# print(training_merged_sampled.size().getInfo())\n",
    "\n",
    "# sample = training_merged_sampled.limit(5)\n",
    "# sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate day gaps for manuscript\n",
    "not archive, but not used right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert columns to datetime format\n",
    "# training_merged['points_date'] = pd.to_datetime(training_merged['points_date'])\n",
    "# training_merged['image_date'] = pd.to_datetime(training_merged['image_date'])\n",
    "\n",
    "# # Calculate the difference in days\n",
    "# training_merged['days_difference'] = (training_merged['image_date'] - training_merged['points_date']).dt.days\n",
    "# # print(training_merged)\n",
    "# display(training_merged['days_difference'].describe())\n",
    "\n",
    "# unique_date_pairs = training_merged[['points_date', 'image_date']].drop_duplicates().sort_values(by=['points_date'])\n",
    "# # display(unique_date_pairs.reset_index(drop=True))\n",
    "\n",
    "# # Calculate the difference in days\n",
    "# unique_date_pairs['days_difference'] = (unique_date_pairs['image_date'] - unique_date_pairs['points_date']).dt.days\n",
    "# display(unique_date_pairs.sort_values(by='days_difference'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing satellite embeddings sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = training_merged\n",
    "# dl = training_merged['Date_Loc'].unique()[10]\n",
    "# print('Sampling for date/location:', dl)\n",
    "\n",
    "# df_dateloc = df[df['Date_Loc'] == dl]\n",
    "# # df_dateloc.head()\n",
    "# # Get properties for this date/location pair\n",
    "# # date = df_dateloc['Date'].iloc[0]\n",
    "# image_date = df_dateloc['image_date'].iloc[0]\n",
    "\n",
    "# print(f\"DF: {len(df_dateloc)} rows for date/location: {dl}\") # test\n",
    "\n",
    "# fc_date = geemap.df_to_ee(df_dateloc, latitude='Latitude', longitude='Longitude')\n",
    "\n",
    "# print('FC:', fc_date.size().getInfo()) # test\n",
    "\n",
    "# embed_year = ee.Date(ee.Date(image_date).format('yyyy-01-01'))\n",
    "# embed_year\n",
    "\n",
    "# embed_img = embed.filterDate(embed_year, embed_year.advance(1, 'year')).mosaic()\n",
    "\n",
    "# data = embed_img.sampleRegions(\n",
    "#     collection=fc_date,\n",
    "#     scale=10,\n",
    "#     geometries=True\n",
    "# )\n",
    "\n",
    "# # embed_img\n",
    "# # Convert that to dataframe\n",
    "# data_df = geemap.ee_to_df(data)\n",
    "# data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old method: sample each feature collection to build training dataset -- archive\n",
    "This method uses image matching to ensure there are no cloudy pixels in the training data\n",
    "\n",
    "this is for the old training dataset\n",
    "\n",
    "Skip to 2b for faster running code if you haven't made changes to imagery / sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_dates(df):\n",
    "#     # add a 'class' column to the df by mapping the 'Type' column to numeric values\n",
    "#     df['class'] = df['Type'].map(class_mapping)\n",
    "#     # Format dates to YYYY-MM-DD\n",
    "#     df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "#     # print('pre-filter', df['Date'].unique())\n",
    "#     # print(df.shape[0])\n",
    "\n",
    "#     # Drop greenhouse, drop points from before 2018-05-09\n",
    "#     df = df[df[\"Type\"]!=\"greenhouse\"]\n",
    "#     df = df[df['Date'] > '2018-05-09']\n",
    "#     # print('post-filter', df['Date'].unique())\n",
    "#     # print(df.shape[0])\n",
    "    \n",
    "#     # get the location of the df to be sampled\n",
    "#     location = df['Location'].iloc[0]\n",
    "#     # filter the date_matching df to this location\n",
    "#     date_matching_loc = date_matching[date_matching['location'] == location]\n",
    "#     # Filter for only dates that have a matched sampling image, should be all now\n",
    "#     df = df.loc[df['Date'].isin(date_matching_loc['points_date'])]\n",
    "\n",
    "#     dates = df['Date'].unique()\n",
    "#     # print('post-filter', dates)\n",
    "\n",
    "#     all_area_data = []\n",
    "\n",
    "#     # Iterate through all of the dates in the df\n",
    "#     for date in dates:\n",
    "#         # Get df rows for that date\n",
    "#         df_date = df[df[\"Date\"]==date]\n",
    "#         fc_date = geemap.df_to_ee(df_date, latitude='Latitude', longitude='Longitude')\n",
    "#         # Get image date from the date_matching df\n",
    "#         image_date = date_matching_loc.loc[date_matching_loc['points_date'] == date, 'image_date'].iloc[0]\n",
    "#         # Filter images to only that one corresponding sampling date\n",
    "#         image_nextdate = ee.Date(image_date).advance(1, 'day').format('YYYY-MM-dd')\n",
    "#         sample_image = processedCollection.filterDate(image_date, image_nextdate).median()\n",
    "#         # Sample that image to the fc, keeping class property\n",
    "#         data = sample_image.select(bands).sampleRegions(\n",
    "#             collection=fc_date,\n",
    "#             properties=['class', 'Latitude', 'Longitude'],\n",
    "#             scale=10, #Reduces points within 10m of each other\n",
    "#             geometries=True\n",
    "#         )\n",
    "\n",
    "#         # Also sample the monthly ET (30m scale, from landsat and other weather data)\n",
    "#         et_date = ee.Date(ee.Date(image_date).format('yyyy-MM-01')) #convert to first of month for ET data\n",
    "#         et_img = et.filterDate(et_date, et_date.advance(1, 'month')).median().rename('ET')\n",
    "#         data = et_img.sampleRegions(\n",
    "#             collection=data,\n",
    "#             scale=30, #I believe 30m scale reduces points within 30m of each other\n",
    "#             geometries=True\n",
    "#         )\n",
    "#         # Add point date, sampled image date, and location as a property to each feature in the feature collection\n",
    "#         data = data.map(lambda feature: feature.set({\n",
    "#             'date': date,\n",
    "#             'image_date': image_date,\n",
    "#             'location': location\n",
    "#             }))\n",
    "\n",
    "#         # Append to list of fcs for each date\n",
    "#         all_area_data.append(data)\n",
    "\n",
    "#     # Flatten into one fc for the dataset\n",
    "#     area_fc = ee.FeatureCollection(all_area_data).flatten()\n",
    "#     return area_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = sample_dates(othertrees)\n",
    "# print(test.size().getInfo())\n",
    "# test_df = geemap.ee_to_df(test)\n",
    "# test_df\n",
    "# filtered = date_matching[date_matching['location'] == 'Oxnard']\n",
    "# print(filtered)\n",
    "# display('after sampling', test_df['class'].value_counts())\n",
    "# display('before sampling', oxnard['Type'].value_counts())\n",
    "# print(class_mapping)\n",
    "\n",
    "# make a df with manually entered points\n",
    "# but you would also have to add to date_matching for any new image dates\n",
    "# data_points = [\n",
    "#     ['Oxnard', '8/1/2023', 'mulch', -119.257339, 34.260933]\n",
    "# ]\n",
    "# other_points = pd.DataFrame(data_points, columns=['Location', 'Date', 'Type', 'Longitude', 'Latitude'])\n",
    "# test\n",
    "\n",
    "# santamaria printout says 298, excel file is 768. total rows matches. my filtering function is 768.\n",
    "# mendo is right (none)\n",
    "# watsonville printout says 65, excel file is 464. total rows matches. my filtering function is 464 (accounting for greenhouse points)\n",
    "# oxnard printout says 578, excel files (2) are 0. total rows 1474 + 59 = 1533 matches. my filtering function is 1533.\n",
    "# so all of the filtering is working as I expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # On 9.20.24, removed mendocino training data from the model \n",
    "# # mendo_training = sample_dates(mendocino)\n",
    "# watsonville_training = sample_dates(watsonville)\n",
    "# santamaria_training = sample_dates(santamaria)\n",
    "# oxnard_training = sample_dates(oxnard)\n",
    "# # On 1.30.25 added other trees dataset\n",
    "# othertrees_training = sample_dates(othertrees)\n",
    "\n",
    "# # Combine all data - list of feature collections\n",
    "# all_data = []\n",
    "# # all_data.append(mendo_training)\n",
    "# all_data.append(watsonville_training)\n",
    "# all_data.append(santamaria_training)\n",
    "# all_data.append(oxnard_training)\n",
    "# all_data.append(othertrees_training)\n",
    "\n",
    "# # generate a random number column for training/test split\n",
    "# data = ee.FeatureCollection(all_data).flatten().randomColumn(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the fc as an asset to drastically speed up later anaylsis\n",
    "# # have to delete this asset first, can't overwrite it\n",
    "\n",
    "data = training_merged_sampled\n",
    "asset_id = 'projects/ee-tnc-annietaylor/assets/agplastics/training_v_1_2_s87'\n",
    "task = ee.batch.Export.table.toAsset(\n",
    "    collection=data,\n",
    "    description='Export training data to asset',\n",
    "    assetId=asset_id\n",
    ")\n",
    "# task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b Import training data as fc instead of building it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the fc with training data prepped for each version of the model\n",
    "# asset_id = 'projects/ee-tnc-annietaylor/assets/agplastics/labeled_point_data_matched_ET_BM_o_nm_ox_all' #v0.9\n",
    "# asset_id = 'projects/ee-tnc-annietaylor/assets/agplastics/labeled_point_data_matched_ET_BM_o_nm_ox_all_trees' #v1.0\n",
    "# asset_id = 'projects/ee-tnc-annietaylor/assets/agplastics/labeled_point_data_matched_ET_BM_o_nm_ox_all_trees_pgi' #v1.1\n",
    "asset_id = 'projects/ee-tnc-annietaylor/assets/agplastics/training_v_1_2_s87' #v1.2\n",
    "# asset_id = 'projects/ee-tnc-annietaylor/assets/agplastics/training_v_1_2_embed' #v1.2 + satellite embeddings dataset\n",
    "\n",
    "data = ee.FeatureCollection(asset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c violin plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = geemap.ee_to_df(data)\n",
    "class_mapping_reversed = {v: k for k, v in class_mapping.items()}\n",
    "data_df['Type'] = data_df['class'].map(class_mapping_reversed)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the band of interest for plotting\n",
    "boi = \"NDTI\"\n",
    "# NDTI, ET, RPGI, B12, NDVI, NDMI, B11, PGI, B6, NDBI, B2, NDWI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(data_df).mark_boxplot().encode(\n",
    "    alt.X(\"Type:N\", sort=['other', 'hoop', 'mulch', 'blackmulch']),\n",
    "    alt.Y(boi).scale(zero=False),\n",
    "    alt.Color(\"Type:N\"),\n",
    "    alt.Tooltip(\"image_date:T\")\n",
    ").properties(width=300)\n",
    "\n",
    "countlabel = alt.Chart(data_df).mark_text(align='center', dy=-130, fontWeight=100, fontSize=10).encode(\n",
    "    x='Type:N',\n",
    "    text='label:N'\n",
    ").transform_joinaggregate(\n",
    "    n_count = 'count(Type)',\n",
    "    groupby=['Type']\n",
    ").transform_calculate(\n",
    "    label = \"'n = ' + datum.n_count\"\n",
    ")\n",
    "\n",
    "# chart + countlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart2 = alt.Chart(data_df, width=100).transform_density(\n",
    "    boi,\n",
    "    as_=[boi, 'density'],\n",
    "    groupby=['Type']\n",
    ").mark_area(orient='horizontal').encode(\n",
    "    alt.X('density:Q')\n",
    "        .stack('center')\n",
    "        .impute(None)\n",
    "        .title(None)\n",
    "        .axis(labels=False, values=[0], grid=False, ticks=True),\n",
    "    alt.Y(boi),\n",
    "    alt.Color('Type:N'),\n",
    "    alt.Column('Type:N', sort=['other', 'hoop', 'mulch', 'blackmulch'])\n",
    "        .spacing(0)\n",
    "        .header(titleOrient='bottom', labelOrient='bottom', labelPadding=0)\n",
    ").configure_view(\n",
    "    stroke=None\n",
    ")\n",
    "\n",
    "# chart2\n",
    "display(chart + countlabel, chart2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yskLSk4uEuey"
   },
   "source": [
    "## 3. Split training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### merge blackmulch class into mulch class\n",
    "Slightly higher overall accuracy and higher recall (aka fewer mulch points missed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine black mulch and mulch classes as this slightly improved model performance\n",
    "# data.first().get('class').getInfo()\n",
    "data = data.map(lambda feature: feature.set('class', ee.Algorithms.If(ee.Number(feature.get('class')).eq(3), 1, feature.get('class'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "8gFGTlP9gAq2",
    "outputId": "a3fed06c-23e7-4997-d0c2-78afabdd160f"
   },
   "outputs": [],
   "source": [
    "# Process data (1): Train test split\n",
    "# print(f\"Total number of labeled data:\", data.size().getInfo())\n",
    "# split = 0.7\n",
    "# training = data.filter(ee.Filter.lt('random', split))\n",
    "# test = data.filter(ee.Filter.gte('random', split))\n",
    "\n",
    "# new way -- training/test split is set based on the 'Split' field\n",
    "# training = data.filter(ee.Filter.eq('Split', 1))\n",
    "# test = data.filter(ee.Filter.eq('Split', 2))\n",
    "\n",
    "# new way TEST NEW SPLIT -- training/test split is set based on the 'Split_grouped' field\n",
    "training = data.filter(ee.Filter.eq('Split_grouped', 1))\n",
    "test = data.filter(ee.Filter.eq('Split_grouped', 2))\n",
    "\n",
    "print(f\"Number of training points: {training.size().getInfo()}\")\n",
    "print(f\"Number of test points: {test.size().getInfo()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "45FeXSRV189h",
    "outputId": "a790a801-2cec-4d2a-b3d0-76d28377e25a"
   },
   "outputs": [],
   "source": [
    "# convert to dfs for easier stats\n",
    "# training_df = geemap.ee_to_df(training)\n",
    "# test_df = geemap.ee_to_df(test)\n",
    "\n",
    "# print(f\"Number of training points: {training_df.shape[0]}\")\n",
    "# print(f\"Number of test points: {test_df.shape[0]}\")\n",
    "\n",
    "# Check number of data in each location\n",
    "# print(\"Number of training points:\", training_df['location'].value_counts())\n",
    "# print(\"Number of test points:\", test_df['location'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Address spatial autocorrelation (archive)\n",
    "- Remove points within 30 meters of each other between the training and test datasets\n",
    "- DOES NOT ADDRESS SPATIAL AUTOCORRELATION W/IN THESE DATASETS and also the fact that many of these points are from different dates :/ \n",
    "- not using this with new training dataset as of 8/7/25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### this is where kirk and I left off #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "5b10_-1RmYqm",
    "outputId": "97e17534-e6ac-4785-ddd9-833a0dec2a72"
   },
   "outputs": [],
   "source": [
    "# # Process data (2): Address autocorrelation correction\n",
    "# # TODO how are we addressing spatial autocorrelation within each dataset?\n",
    "# # right now this is simply removing any training data points within 30m of a test point\n",
    "# distFilter = ee.Filter.withinDistance(\n",
    "#     distance=30,\n",
    "#     leftField='.geo',\n",
    "#     rightField='.geo',\n",
    "#     maxError=10\n",
    "# )\n",
    "# join = ee.Join.inverted()\n",
    "# training = join.apply(training, test, distFilter)\n",
    "# # print(f\"Number of training points after removing autocorrelation: {training.size().getInfo()}\")\n",
    "\n",
    "\n",
    "# # only need to worry about this within dates, within the same week for example\n",
    "# # if val point is too close to training point, we're not actually testing our model\n",
    "# # should we validate points from new fields -- this is kirk's preference\n",
    "# # use lat long and time to cluster points, break this into n groups\n",
    "# #  wihtin this time period, \n",
    "# # geographic clusters per year and per region\n",
    "# # or i could set a max number of points per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "8RXLBSNW3N1J",
    "outputId": "d00d4cac-7bfc-4bd3-82bb-7bbf2778850b"
   },
   "outputs": [],
   "source": [
    "# convert to dfs for easier stats again\n",
    "training_df = geemap.ee_to_df(data)\n",
    "# Check number of training data in each region again\n",
    "display(\"Number of reference points:\", training_df['location'].value_counts())\n",
    "display('number of points test/training:', training_df['Split_grouped'].value_counts())\n",
    "# training_df.head(5)\n",
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Number of training points of each class:\", training_df['class'].value_counts())\n",
    "for class_name, class_id in class_mapping.items():\n",
    "   filtered_df = training_df[training_df['class'] == class_id]\n",
    "   print(f\"Number of points labeled {class_name}: {filtered_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWr5yAkAFH_r"
   },
   "source": [
    "## 4. Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Specify bands to train the model on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if  using satellite embeddings\n",
    "# bands_tr = [\n",
    "#     'B4', 'B3', 'B2', 'B6', 'B8', 'B11', 'B12',\n",
    "#     'NDVI', 'NDTI', 'PGI', 'RPGI', 'PMLI', 'NDBI', 'PGI_adj', 'NDWI', 'NDMI', 'ET',\n",
    "#     'A00', 'A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07', 'A08', 'A09', 'A10',\n",
    "#     'A11', 'A12', 'A13', 'A14', 'A15', 'A16', 'A17', 'A18', 'A19', 'A20', 'A21',\n",
    "#     'A22', 'A23', 'A24', 'A25', 'A26', 'A27', 'A28', 'A29', 'A30', 'A31', 'A32',\n",
    "#     'A33', 'A34', 'A35', 'A36', 'A37', 'A38', 'A39', 'A40', 'A41', 'A42', 'A43',\n",
    "#     'A44', 'A45', 'A46', 'A47', 'A48', 'A49', 'A50', 'A51', 'A52', 'A53', 'A54',\n",
    "#     'A55', 'A56', 'A57', 'A58', 'A59', 'A60', 'A61', 'A62', 'A63'\n",
    "# ]\n",
    "\n",
    "# bands not using satellite embeddings\n",
    "bands_tr = [\n",
    "    'B4', 'B3', 'B2', 'B6', 'B8', 'B11', 'B12',\n",
    "    'NDVI', 'NDTI', 'PGI', 'RPGI', 'PMLI', 'NDBI', 'PGI_adj', 'NDWI', 'NDMI', 'ET'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "uHDXaatAJFOG",
    "outputId": "2b83ad3f-867c-4c95-e805-e45a0cc6b2bd"
   },
   "outputs": [],
   "source": [
    "# Train and test the RF model\n",
    "trained_RF = ee.Classifier.smileRandomForest(50).train(training, 'class', bands_tr)\n",
    "trainAccuracy_RF = trained_RF.confusionMatrix()\n",
    "\n",
    "classifications_RF = test.classify(trained_RF)\n",
    "testAccuracy_RF = classifications_RF.errorMatrix('class', 'classification')\n",
    "\n",
    "# print out accuracy information\n",
    "print(f\"Training accuracy: {trainAccuracy_RF.accuracy().getInfo()*100:.2f}%\")\n",
    "print(f\"Test accuracy: {testAccuracy_RF.accuracy().getInfo()*100:.2f}%\")\n",
    "# trained_RF.explain()\n",
    "# takes about 2 min to run if you have recreated instead of importing the training data, otherwise super fast\n",
    "# classifications_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "w8LZMuC5yAUv",
    "outputId": "f1da1c9a-36fc-4d62-a19f-c2d461664d4d"
   },
   "outputs": [],
   "source": [
    "# this gives f score for each class (hoop, mulch, other) (blackmulch combined now)\n",
    "classacc = testAccuracy_RF.fscore().getInfo()\n",
    "print(f'hoop: {classacc[0]*100:.2f}%')\n",
    "print(f'mulch: {classacc[1]*100:.2f}%')\n",
    "print(f'other: {classacc[2]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "QQ9_LOAtzUuc",
    "outputId": "6a3de1a1-7930-4879-98e4-6ca32cd27149"
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "# display_labels = ['hoop', 'mulch', 'other', 'blackmulch']\n",
    "display_labels = ['hoop', 'mulch', 'other']\n",
    "\n",
    "cm_RF = np.array(testAccuracy_RF.getInfo())\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm_RF, display_labels=display_labels)\n",
    "disp.plot(cmap=plt.cm.Greys)\n",
    "disp.ax_.set_title(f\"RF Confusion Matrix ({testAccuracy_RF.accuracy().getInfo():.2%} Accuracy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = ee.Dictionary(trained_RF.explain().get('importance'))\n",
    "\n",
    "totalImportance = importance.values().reduce(ee.Reducer.sum())\n",
    "\n",
    "# helper fn to map over importance values\n",
    "def percentConvert(key, val):\n",
    "    return ee.Number(val).divide(totalImportance).multiply(100)\n",
    "relImportance = importance.map(percentConvert)\n",
    "\n",
    "importanceFC = ee.FeatureCollection([ee.Feature(None, relImportance)])\n",
    "# convert to pandas df\n",
    "varImportance = geemap.ee_to_df(importanceFC)\n",
    "\n",
    "varImportance = varImportance.transpose().rename(columns={0: 'Importance'})\n",
    "varImportance = varImportance.sort_values(by='Importance', ascending=False)\n",
    "varImportance\n",
    "\n",
    "# varImportance.plot(kind='bar')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproducible/condensed version\n",
    "Used for debugging, this is fairly old now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reproducible version to test errors\n",
    "# # import the training data - points labeled with class (0, 1, 2) and containing sampled band info\n",
    "# asset_id = 'projects/ee-annalisertaylor/assets/TNC/agplastics/labeled_point_data' # needs to be read-only/public\n",
    "# data = ee.FeatureCollection(asset_id)\n",
    "# # subset the training data by approx 70% for training (and 30% for testing)\n",
    "# training = data.filter(ee.Filter.lt('random', 0.7))\n",
    "# bands = ['B4', 'B3', 'B2', 'B6', 'B8', 'B11', 'B12', 'NDVI', 'NDTI', 'PGI', 'RPGI', 'PMLI']\n",
    "\n",
    "# trained_RF = ee.Classifier.smileRandomForest(50).train(training, 'class', bands)\n",
    "\n",
    "# importance = ee.Dictionary(trained_RF.explain().get('importance'))\n",
    "\n",
    "# totalImportance = importance.values().reduce(ee.Reducer.sum())\n",
    "\n",
    "# # helper fn to map over importance values\n",
    "# def percentConvert(key, val):\n",
    "#     return ee.Number(val).divide(totalImportance).multiply(100)\n",
    "# relImportance = importance.map(percentConvert)\n",
    "\n",
    "# importanceFC = ee.FeatureCollection([ee.Feature(None, relImportance)])\n",
    "# # convert to pandas df\n",
    "# varImportance = geemap.ee_to_df(importanceFC)\n",
    "# varImportance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different random seeds for test/train split\n",
    "trying to maximize overall testing accuracy but also mulch accuracy\n",
    "random seed 87 looks great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import data just after sampling to s2 images and reset black mulch \n",
    "# data = training_merged_sampled\n",
    "# data = data.map(lambda feature: feature.set('class', ee.Algorithms.If(ee.Number(feature.get('class')).eq(3), 1, feature.get('class'))))\n",
    "\n",
    "# # convert data fc to df\n",
    "# data_df = geemap.ee_to_df(data)\n",
    "# # add a Date_Loc field that's date + location fields\n",
    "# data_df['Date_Loc'] = data_df['date'] + data_df['location']\n",
    "\n",
    "# # display(data_df)\n",
    "\n",
    "# # function to try different seeds for splits\n",
    "# def split_data(seed):\n",
    "#     # import unique date_loc info from training data that was df (before sampling to fc)\n",
    "#     date_loc_counts = data_df['Date_Loc'].value_counts()\n",
    "#     unique_date_locs = data_df['Date_Loc'].unique()\n",
    "#     # Shuffle the unique Date_Loc values for randomness\n",
    "#     np.random.seed(seed)\n",
    "#     shuffled_date_locs = np.random.permutation(unique_date_locs)\n",
    "\n",
    "#     # Calculate the cumulative sum of rows as we add each Date_Loc\n",
    "#     cumulative_counts = date_loc_counts.loc[shuffled_date_locs].cumsum()\n",
    "#     # print(f\"Cumulative counts: {cumulative_counts}\")\n",
    "#     total_rows = date_loc_counts.sum()\n",
    "#     split_idx = np.argmax(cumulative_counts >= total_rows * 0.7) + 1  # +1 to include the index where threshold is crossed\n",
    "\n",
    "#     train_date_locs = set(shuffled_date_locs[:split_idx])\n",
    "#     test_date_locs = set(shuffled_date_locs[split_idx:])\n",
    "\n",
    "#     # Assign Split_grouped field: 1 for train, 2 for test\n",
    "#     split_col = f'Split_grouped_seed{seed}'\n",
    "#     data_df[split_col] = data_df['Date_Loc'].apply(lambda x: 1 if x in train_date_locs else 2)\n",
    "\n",
    "#     # # Check distribution across months\n",
    "#     # month_split_grouped_counts = data_df.groupby(['Month', 'Split_grouped']).size().unstack(fill_value=0)\n",
    "#     # month_split_grouped_counts.plot(kind='bar', stacked=False)\n",
    "#     # plt.xlabel('Month')\n",
    "#     # plt.ylabel('Number of Rows')\n",
    "#     # plt.title('Number of Rows per Month by Split_grouped Value')\n",
    "#     # plt.legend(title='Split_grouped')\n",
    "#     # plt.tight_layout()\n",
    "#     # plt.show()\n",
    "\n",
    "#     # # Print the percentage of points in each split\n",
    "#     # train_pct = (data_df['Split_grouped'] == 1).mean() * 100\n",
    "#     # test_pct = (data_df['Split_grouped'] == 2).mean() * 100\n",
    "#     # print(f\"Training: {train_pct:.1f}%  |  Testing: {test_pct:.1f}%\")\n",
    "\n",
    "#     # convert back to fc from df, no geometries\n",
    "#     features = []\n",
    "#     for _, row in data_df.iterrows():\n",
    "#         props = row.to_dict()\n",
    "#         features.append(ee.Feature(None, props))\n",
    "#     data = ee.FeatureCollection(features)\n",
    "\n",
    "#     # split it into test/train based on that seed\n",
    "#     training = data.filter(ee.Filter.eq(split_col, 1))\n",
    "#     test = data.filter(ee.Filter.eq(split_col, 2))\n",
    "\n",
    "#     # print(f\"Number of training points: {training.size().getInfo()}\")\n",
    "#     # print(f\"Number of test points: {test.size().getInfo()}\")\n",
    "\n",
    "#     # build and test model\n",
    "#     trained_RF = ee.Classifier.smileRandomForest(50).train(training, 'class', bands_tr)\n",
    "#     trainAccuracy_RF = trained_RF.confusionMatrix()\n",
    "\n",
    "#     classifications_RF = test.classify(trained_RF)\n",
    "#     testAccuracy_RF = classifications_RF.errorMatrix('class', 'classification')\n",
    "\n",
    "#     # save accuracy information to a df \n",
    "#     accuracy_info = {\n",
    "#         \"Seed\": seed,\n",
    "#         \"Training accuracy\": trainAccuracy_RF.accuracy().getInfo(),\n",
    "#         \"Test accuracy\": testAccuracy_RF.accuracy().getInfo(),\n",
    "#         \"Test F-score\": testAccuracy_RF.fscore().getInfo()\n",
    "#     }\n",
    "#     return accuracy_info\n",
    "\n",
    "# dfs = []\n",
    "# for seed in [10, 15, 87, 23, 54, 53, 88, 86, 31, 66]:\n",
    "#     accuracy_info = split_data(seed)\n",
    "#     dfs.append(accuracy_info)\n",
    "\n",
    "# dfs = pd.DataFrame(dfs)\n",
    "# display(dfs)\n",
    "# # ran two seeds in 3 min\n",
    "# # ran ten seeds in 7 min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbrmDkD6AJcD"
   },
   "source": [
    "## 5. Export the model\n",
    "\n",
    "per [ee docs](https://developers.google.com/earth-engine/apidocs/export-classifier-toasset#colab-python), allows you to build larger models (i.e. more trees, more training samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "8y2uvtKdJoAz",
    "outputId": "78ae4703-cd56-47a3-db23-4b565bf79457"
   },
   "outputs": [],
   "source": [
    "rf_asset_id = 'projects/ee-tnc-annietaylor/assets/agplastics/RF_classifier_v_1_2_s87'\n",
    "task = ee.batch.Export.classifier.toAsset(\n",
    "    classifier=trained_RF,\n",
    "    description='classifier_export',\n",
    "    assetId=rf_asset_id\n",
    ")\n",
    "# task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "mAoNs8iARWI4",
    "outputId": "f5af00b9-a842-4bcd-bd87-357275ab55b0"
   },
   "source": [
    "## 6. Apply standard classification model to imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring in the saved classifiers\n",
    "rf_asset_id_v1_1 = 'projects/ee-tnc-annietaylor/assets/agplastics/RF_classifier_matched_ET_BM_o_nm_ox_all_trees_pgi'\n",
    "rf_asset_id = 'projects/ee-tnc-annietaylor/assets/agplastics/RF_classifier_v_1_2_s87'\n",
    "rf_asset_id_e = 'projects/ee-tnc-annietaylor/assets/agplastics/RF_classifier_v_1_2_embed'\n",
    "savedClassifier_v1_1 = ee.Classifier.load(rf_asset_id_v1_1)\n",
    "savedClassifier = ee.Classifier.load(rf_asset_id)\n",
    "savedClassifier_e = ee.Classifier.load(rf_asset_id_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all images from one date, stitch them together, and center the map on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given date, get images from that date, stitch them together, and center the map on that\n",
    "# great image to check: '2024-10-02', earth image date 9/30/2024 in Oxnard area: \n",
    "# https://earth.google.com/web/@34.22849791,-119.2434538,6.54306045a,5617.78356609d,35y,0h,0t,0r/data=ChYqEAgBEgoyMDI0LTA5LTMwGAFCAggBMikKJwolCiExMzlZMjlnZG9FSGVSeGRrT2FuLXZqaDdRdUZpcDVPdHQgAToDCgEwQgIIAEoICJej1LcCEAE\n",
    "\n",
    "# for watsonville ground-truthing, use date = '2021-10-01' and Earth image date 9/30/2021\n",
    "# https://earth.google.com/web/@36.90600071,-121.7366486,5.73571404a,18340.9275927d,35y,0h,0t,0r/data=ChIqEAgBEgoyMDIxLTA5LTMwGAE6AwoBMA\n",
    "# for oxnard ground-truthing, use date = '2022-07-05' and Earth image date 6-30-2022\n",
    "# https://earth.google.com/web/@34.18104511,-119.12299165,16.77976172a,20704.65705214d,35y,0h,0t,0r/data=ChIqEAgBEgoyMDIyLTA2LTMwGAE6AwoBMA\n",
    "# for santa maria, use date = '2021-03-02' and Earth image date 2-28-2021\n",
    "# https://earth.google.com/web/@34.92298797,-120.36568793,104.77571464a,20361.02485151d,35y,0h,0t,0r/data=ChIqEAgBEgoyMDIxLTAyLTI4GAE6AwoBMA\n",
    "# for mendocino, use date = '2021-07-01' and Earth image date 6-30-2021 (have to also take off crop mask)\n",
    "# https://earth.google.com/web/@39.49685652,-123.43778648,742.16477863a,4440.61517426d,35y,0h,0t,0r/data=ChIqEAgBEgoyMDIxLTA2LTMwGAE6AwoBMA\n",
    "\n",
    "date = '2024-10-02'\n",
    "nextday = ee.Date(date).advance(1, 'day').format('YYYY-MM-dd')\n",
    "oneday = processedCollection.filterDate(date, nextday).median().setDefaultProjection(S2_projection)\n",
    "oneday_original = s2.filterDate(date, nextday).median()\n",
    "\n",
    "# get ET data for the month of that date\n",
    "et_date = ee.Date(ee.Date(date).format('yyyy-MM-01')) #convert to first of month for ET data\n",
    "et_img = et.filterDate(et_date, et_date.advance(1, 'month')).median().rename('ET').setDefaultProjection(ET_projection)\n",
    "\n",
    "# get satellite embeddings for the year of that date\n",
    "embed_year = ee.Date(ee.Date(date).format('yyyy-01-01'))\n",
    "embed_img = embed.filterDate(embed_year, embed_year.advance(1, 'year')).mosaic()\n",
    "\n",
    "# # old workflow: add ET band to sentinel-2 image\n",
    "# oneday = oneday.addBands(et_img).addBands(embed_img)\n",
    "\n",
    "# new workflow: get them projected onto the same grid\n",
    "stacked = oneday.addBands(et_img) #.addBands(embed_img) if you need to test the satellite embeddings dataset\n",
    "# scale = 30, projection = ca teale albers (these are set above)\n",
    "stackedResampled = stacked.reduceResolution(\n",
    "  reducer = ee.Reducer.mean(),\n",
    "  maxPixels = 1024\n",
    ").reproject(\n",
    "    crs = gridProjection\n",
    ")\n",
    "\n",
    "m = geemap.Map()\n",
    "m.addLayer(stackedResampled, {'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 3000}, 'stacked resampled')\n",
    "m.centerObject(ventura, 10)\n",
    "# m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New crop mask\n",
    "this is already resampled to the 30m grid to match s2 and ET data, using 75% pixel coverage of the 10m crop mask\n",
    "this just converts it to a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops = cropsResampled.where(cropsResampled, 1)\n",
    "# this is the old crop mask\n",
    "# crops = crops.where(crops, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify the image and add to map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the image\n",
    "classified = stackedResampled.classify(savedClassifier).updateMask(crops)\n",
    "# classified_v1_1 = stackedResampled.classify(savedClassifier_v1_1).updateMask(crops)\n",
    "# classified_e = stackedResampled.classify(savedClassifier_e).updateMask(crops)\n",
    "\n",
    "# Remap values for exporting and visualizing in arc pro\n",
    "remapped = classified.remap(\n",
    "  [0, 1, 2], # from values (hoop, mulch, other)\n",
    "  [1, 2, 3] # to values\n",
    ")\n",
    "# export the true color sentinel image oneday and the classified plastic image for figures\n",
    "# export_s2 = ee.batch.Export.image.toDrive(\n",
    "#     image=oneday.select('B4', 'B3', 'B2'),\n",
    "#     description='S2_10022024',\n",
    "#     fileNamePrefix='S2_10022024',\n",
    "#     folder='EarthEngine',\n",
    "#     scale=10,\n",
    "#     region=socal.geometry(),\n",
    "#     maxPixels=1e13\n",
    "# )\n",
    "# export_s2.start()\n",
    "\n",
    "# export_plastic = ee.batch.Export.image.toDrive(\n",
    "#     image=remapped,\n",
    "#     description='plastic',\n",
    "#     fileNamePrefix='classified_plastic_v1_2_10022024_remap',\n",
    "#     folder='EarthEngine',\n",
    "#     scale=10,\n",
    "#     region=socal.geometry(),\n",
    "#     maxPixels=1e13\n",
    "# )\n",
    "# export_plastic.start()\n",
    "\n",
    "# add the classified plastic and composite image to map\n",
    "m = geemap.Map()\n",
    "m.add_basemap('HYBRID')\n",
    "# m.addLayer(oneday_original, rgbVis, 'Full Sentinel: ' + date)\n",
    "# m.addLayer(oneday.select('RPGI'), RPGI_viz, 'RPGI', False)\n",
    "# m.addLayer(oneday.select('NDTI'), NDTI_viz, 'NDTI', False)\n",
    "m.addLayer(oneday, rgbVis, 'Cloudless Sentinel: ' + date)\n",
    "m.addLayer(classified, plastic_viz, 'Classified Plastic v1.2')\n",
    "# m.addLayer(classified_v1_1, plastic_viz, 'Classified Plastic v1.1', False) # old classification for comparison\n",
    "# m.addLayer(classified_e, plastic_viz, 'Classified Plastic Embedded', False) # embedded classifier for comparison\n",
    "m.addLayer(stackedResampled, rgbVis, 'Resampled Cloudless Sentinel + ET', False)\n",
    "m.centerObject(ca, 6)\n",
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print out the map legend\n",
    "# # colors = ['#FF5733', '#F4F31D', '#2596be', '#333333']\n",
    "# # labels = ['hoop', 'mulch', 'other', 'blackmulch']\n",
    "# colors = ['#FF5733', '#F4F31D', '#2596be']\n",
    "# labels = ['hoop house', 'plastic mulch', 'other']\n",
    "\n",
    "# # Create a legend with the colors and labels\n",
    "# patches = [plt.Rectangle((0, 0), 1, 1, color=color) for color in colors]\n",
    "# plt.legend(patches, labels)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-probability classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_prob_RF = ee.Classifier.smileRandomForest(50).setOutputMode('MULTIPROBABILITY').train(training, 'class', bands_tr)\n",
    "\n",
    "classified_multi = stackedResampled.classify(trained_prob_RF).updateMask(crops)\n",
    "\n",
    "# Extract the probabilities for each cover type\n",
    "probabilities = classified_multi.arrayFlatten([['hoop', 'mulch', 'other']])\n",
    "\n",
    "m = geemap.Map()\n",
    "# m.add_basemap('HYBRID')\n",
    "m.addLayer(oneday_original, rgbVis, 'Full Sentinel: ' + date)\n",
    "m.addLayer(oneday, rgbVis, 'Cloudless Sentinel: ' + date, False)\n",
    "m.addLayer(stackedResampled, rgbVis, 'Resampled Cloudless Sentinel + ET', False)\n",
    "m.addLayer(classified_multi, {}, 'Raw output', False)\n",
    "m.addLayer(probabilities, {}, 'Classified Probabilities')\n",
    "m.centerObject(ca, 6)\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time series of plastic probability "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a start date and the number of images you'd like to sample\n",
    "# images are captured every 5 days in a given place\n",
    "# if you start at beginning of WY 2022 (10.1.21), you'll be in the Watsonville area\n",
    "# if you start at beginning of WY 2022 (10.3.21), you'll get Santa Maria and Oxnard areas\n",
    "first_date = '2021-10-03'\n",
    "first_date = datetime.strptime(first_date, '%Y-%m-%d')\n",
    "days_bt = 5 # multiples of 5 ensure the images will land in the same place in CA\n",
    "num_dates = 73 # 365/5\n",
    "\n",
    "dates = []\n",
    "for i in range(num_dates):\n",
    "    new_date = first_date + timedelta(days=i*days_bt)\n",
    "    dates.append(new_date.strftime('%Y-%m-%d'))\n",
    "\n",
    "if '2022-08-19' in dates: dates.remove('2022-08-19') #causes an error with lt 92% cloud cover\n",
    "if '2021-12-22' in dates: dates.remove('2021-12-22') #causes an error with lt 30% cloud cover\n",
    "dates\n",
    "\n",
    "# six days spread out through the year, I think they will all cover the watsonville area\n",
    "# best to have these separated by 5 days if you'd like to stay in one area\n",
    "# dates = ['2021-01-04', '2021-03-15', '2021-05-24', '2021-08-02', '2021-10-11', '2021-12-20']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create time series for those dates with sentinel and ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries = ee.ImageCollection([])\n",
    "# didn't add the reprojecting here because it causes a memory error\n",
    "\n",
    "for date in dates:\n",
    "    nextday = ee.Date(date).advance(1, 'day').format('YYYY-MM-dd')\n",
    "    oneday = processedCollection.filterDate(date, nextday).median()#.setDefaultProjection(S2_projection)\n",
    "    oneday_original = s2.filterDate(date, nextday).median()\n",
    "\n",
    "    # get ET data for the month of that date\n",
    "    et_date = ee.Date(ee.Date(date).format('yyyy-MM-01')) #convert to first of month for ET data\n",
    "    et_img = et.filterDate(et_date, et_date.advance(1, 'month')).median().rename('ET')#.setDefaultProjection(ET_projection)\n",
    "\n",
    "    # add ET band to sentinel-2 image\n",
    "    oneday = oneday.addBands(et_img).set('date', date)\n",
    "\n",
    "    # add each image to the collection\n",
    "    timeseries = timeseries.merge(ee.ImageCollection([oneday]))\n",
    "\n",
    "# if you get an error while running the classifier on the whole TS, print the collection and check \n",
    "#   for images that don't have S2 bands. Then remove them from the dates list above\n",
    "# display(timeseries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify probability for each class across each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify each image in the timeseries collection\n",
    "def classify_image(image):\n",
    "    date = image.get('date')\n",
    "    classified_multi = image.classify(trained_prob_RF).updateMask(crops)\n",
    "    probabilities = classified_multi.arrayFlatten([['hoop', 'mulch', 'other']])\n",
    "    return probabilities.set('date', date)\n",
    "\n",
    "classified_timeseries = timeseries.map(classify_image)\n",
    "\n",
    "# display(classified_timeseries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the result and draw a geometry for the time series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Map = geemap.Map()\n",
    "Map.add_basemap('HYBRID')\n",
    "Map.addLayer(oneday, rgbVis, 'Cloudless Sentinel: ' + date) #adds last image from the for loop\n",
    "# hoop = red, mulch = green, blue = other\n",
    "Map.addLayer(classified_timeseries.median(), {'min': 0, 'max': 1.0}, 'Classified Probabilities')\n",
    "Map.centerObject(ca, 10)\n",
    "Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you need, export the region to your assets\n",
    "# region = ee.Feature(Map.draw_last_feature)\n",
    "# task = ee.batch.Export.table.toAsset(\n",
    "#     collection=ee.FeatureCollection(region),\n",
    "#     description='region_export',\n",
    "#     assetId='projects/ee-annalisertaylor/assets/TNC/agplastics/sample_hoop_region'\n",
    "# )\n",
    "# task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create df with each class probability across an image collection for a region\n",
    "this runs slowly so I'm commenting it + next block out for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert the last drawn feature from map above to an ee.Feature\n",
    "# region = ee.Feature(Map.draw_last_feature)\n",
    "\n",
    "# can also import a good example feature from assets\n",
    "# this one only works when you're using a date seq that intersects santa maria region\n",
    "region = ee.FeatureCollection('projects/ee-tnc-annietaylor/assets/agplastics/SM_test_field').first()\n",
    "\n",
    "def RR_all_features(image):\n",
    "    # this is a dictionary\n",
    "    means_dict = image.reduceRegion(ee.Reducer.mean(), region.geometry(), 1000)\n",
    "    # add the image date to the ee.Dictionary\n",
    "    means_dict = means_dict.set('date', image.get('date'))\n",
    "    feat = ee.Feature(None, means_dict)\n",
    "    return feat\n",
    "\n",
    "result = ee.FeatureCollection(classified_timeseries.map(RR_all_features))\n",
    "\n",
    "df = geemap.ee_to_df(result)\n",
    "df\n",
    "# print out number of null values in each column\n",
    "df.isnull().sum() # 15 nulls for cloudiness lte 30%. 7 nulls for cloudiness lt 92%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chart the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to datetime values\n",
    "df['dateT'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# line chart layered with points for clarity\n",
    "wy_title = first_date.year + 1\n",
    "# plt.title(f\"Plastic Probability over WY {wy_title}, starting on {first_date.date()}\")\n",
    "plt.title(f\"Class Probabilities over WY {wy_title}\")\n",
    "plt.plot(df['dateT'], df['hoop'], '-o', color='#D55E00', label='Hoop')\n",
    "plt.plot(df['dateT'], df['mulch'], '-o', color='#F0E442', label='Mulch')\n",
    "\n",
    "plt.plot(df['dateT'], df['other'], '-o', color='#94D9FF', label='Other')\n",
    "# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# Custom legend with dark background and light text\n",
    "legend = plt.legend(loc='best', frameon=True, facecolor='#333333', edgecolor='white')\n",
    "for text in legend.get_texts():\n",
    "    text.set_color('white')\n",
    "plt.grid(axis='x', color='darkgray', linestyle='-', linewidth=0.5)\n",
    "# plt.axhline(y=0.5, color='yellow', linestyle='--', linewidth=1) # lines to show the threshold for plastic\n",
    "# plt.axhline(y=0.6, color='red', linestyle='--', linewidth=1)\n",
    "\n",
    "# Set x-axis to show only month-year labels\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator())\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%b-%Y'))\n",
    "\n",
    "plt.xticks(rotation=40, ha='right', fontsize=8)\n",
    "plt.gcf().set_facecolor(\"#424242\")\n",
    "plt.gca().set_facecolor('#424242')\n",
    "plt.grid(axis='y', color='darkgray', linestyle='-', linewidth=0.5)\n",
    "plt.tick_params(colors='white')\n",
    "plt.xlabel('Date', color='white')\n",
    "plt.ylabel('Probability', color='white')\n",
    "plt.title(f\"Class Probabilities, WY {wy_title}\", color='white')\n",
    "plt.rcParams['font.family'] = 'Calibri'\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intra-annual charts\n",
    "I want three charts showing typical temporal patterns:\n",
    "- mulch: mulch is applied and then crops grow over it through spring\n",
    "- hoop house: high prob of hoop house the whole WY\n",
    "- other: mostly other, but a few noisy observations of mulch/hoop that get filtered out by our algo\n",
    "- OR double application of mulch? Need to check for fields that do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # just a line chart\n",
    "# df.plot(x='date', y=['hoop', 'mulch', 'other'], kind='line', color=['#FF5733', '#F4F31D', '#2596be'])\n",
    "\n",
    "# # line chart layered with points for clarity\n",
    "# plt.plot(df['date'], df['other'], '-o', color='#2596be', label='other')\n",
    "# plt.plot(df['date'], df['hoop'], '-o', color='#FF5733', label='hoop')\n",
    "# plt.plot(df['date'], df['mulch'], '-o', color='#F4F31D', label='mulch')\n",
    "# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# plt.xticks(df['date'][::2], rotation=75, ha='right', fontsize=8)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### can test if the region updated appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = geemap.Map()\n",
    "# m.add_basemap('HYBRID')\n",
    "# m.addLayer(classified_timeseries.first(), {}, 'Classified Probabilities')\n",
    "# m.addLayer(region, {}, 'Region')\n",
    "# m.centerObject(region, 13)\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Classify all images in a year across CA/one county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all sentinel images in a time period (cloud filtered)\n",
    "# set the water year and county of interest\n",
    "wy = 2021\n",
    "county = ventura # ventura, mendo, cruz, santabarbara, sanbenito, humboldt\n",
    "# get text name of county\n",
    "county_name = county.first().get('NAME').getInfo().replace(\" \", \"\")\n",
    "\n",
    "# # for exporting across the county line in watsonville\n",
    "# wats_region = ee.FeatureCollection('projects/ee-tnc-annietaylor/assets/agplastics/watsonville_region').first()\n",
    "# county = wats_region.geometry() \n",
    "# county_name = 'Watsonville'  \n",
    "\n",
    "# create dates\n",
    "stdate = f'{wy-1}-10-01'\n",
    "endate = f'{wy}-10-01' # end date is exclusive\n",
    "\n",
    "# ~24 images per day across CA, ~169 images to cover CA over 5 days\n",
    "# ~256 images per year in santa cruz county\n",
    "sentinel_ts = processedCollection.filterDate(stdate, endate).filterBounds(county) #if statewide, comment out filterBounds\n",
    "print(f'Plastic classification for {county_name} County in water year {wy}')\n",
    "# if statewide, use this line instead\n",
    "# print(f'Plastic classification for the state of California in water year {wy}')\n",
    "print('Number of images: ', sentinel_ts.size().getInfo())\n",
    "\n",
    "# then add corresponding ET data for each month (map over collection)\n",
    "def add_ET(image):\n",
    "    image_sts = image.get('system:time_start')\n",
    "    date = image.date()\n",
    "    et_date = ee.Date(ee.Date(date).format('yyyy-MM-01')) #convert to first of month for ET data\n",
    "    et_img = et.filterDate(et_date, et_date.advance(1, 'month')).median().rename('ET').setDefaultProjection(ET_projection)\n",
    "    image.setDefaultProjection(S2_projection) #have to define the other band's projections too when stacking\n",
    "    return image.addBands(et_img).set('system:time_start', image_sts)\n",
    "\n",
    "sentinel_ts_ET = sentinel_ts.map(add_ET)\n",
    "\n",
    "# RESAMPLE/REPROJECT \n",
    "# Choose the grid size and projection\n",
    "gridScale = 30\n",
    "# CA teale albers = applicable to all CA\n",
    "gridProjection = ee.Projection('EPSG:3310').atScale(gridScale)\n",
    "# UTM zone 11 = socal\n",
    "# gridProjection = ee.Projection('EPSG:32611').atScale(gridScale)\n",
    "\n",
    "# then classify each image (map over collection)\n",
    "# this function also reprojects the image to the grid\n",
    "def classify_image_2(image):\n",
    "    image_sts = image.get('system:time_start') # enables filterDate functionality\n",
    "    date = image.get('date')\n",
    "    # not included here, could set resampling method to bilinear\n",
    "    image.reduceResolution(reducer = ee.Reducer.mean()).reproject(crs = gridProjection)\n",
    "    classified_multi = image.classify(trained_prob_RF).updateMask(crops)\n",
    "    probabilities = classified_multi.arrayFlatten([['hoop', 'mulch', 'other']])\n",
    "    return probabilities.set('date', date).set('system:time_start', image_sts)\n",
    "\n",
    "classified_timeseries = sentinel_ts_ET.map(classify_image_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get top percentile value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mosaic for one day and add to map for testing\n",
    "\n",
    "# either by testing a date\n",
    "# date = '2022-01-20'\n",
    "# nextday = ee.Date(date).advance(1, 'day').format('YYYY-MM-dd')\n",
    "# classified = classified_timeseries.filterDate(date, nextday).median()\n",
    "\n",
    "# or taking the entire collection\n",
    "count = sentinel_ts.count()\n",
    "classified = classified_timeseries.reduce(ee.Reducer.percentile([95])).select(['hoop_p95', 'mulch_p95'])\n",
    "# classifiedmask = classified.gt(0.75)\n",
    "sentinel_image = sentinel_ts.median()\n",
    "\n",
    "# Get the max and min pixel value of the count image, to get a sense of the range of how many images cover each pixel\n",
    "count_min = count.select('B1').reduceRegion(ee.Reducer.min(), county.geometry(), 800).getInfo()\n",
    "count_max = count.select('B1').reduceRegion(ee.Reducer.max(), county.geometry(), 800).getInfo()\n",
    "# print('Min number of images at a pixel:', count_min)\n",
    "# print('Max number of images at a pixel:', count_max)\n",
    "\n",
    "# # add to map\n",
    "# ca_outline = ee.FeatureCollection(ca).style(fillColor='00000000')\n",
    "# m = geemap.Map()\n",
    "# m.add_basemap('HYBRID')\n",
    "# m.addLayer(sentinel_image, rgbVis, 'Cloudless Sentinel', False)\n",
    "# m.addLayer(classified, {}, 'Classified Probabilities', False)\n",
    "# m.addLayer(count.select('B1'), {'min': 0, 'max': 400}, 'Number of Images')\n",
    "# m.addLayer(ca_outline, {}, 'California')\n",
    "# m.centerObject(ca, 6)\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the two bands to get integrated raster\n",
    "i.e. if a pixel is classified as mulch and hoop, use difference from threshold to determine which class it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_classes(image):\n",
    "    # Define my thresholds for each band\n",
    "    bh_th = 0.5\n",
    "    bm_th = 0.5\n",
    "\n",
    "    # Define the two bands you want to compare\n",
    "    # input image is classified, has a hoop_p95 and mulch_p95 band\n",
    "    bh = image.select('hoop_p95')\n",
    "    bm = image.select('mulch_p95')\n",
    "\n",
    "    # difference from threshold for each\n",
    "    bh_diff = bh.subtract(bh_th).rename('bh_diff')\n",
    "    bm_diff = bm.subtract(bm_th).rename('bm_diff')\n",
    "\n",
    "    class_band = image.expression(\n",
    "        # \"bh_gt >= 0 ? 1 : (bm_gt >= 0 ? 2 : 0)\", # super simple version\n",
    "        # if bh if gte threshold, class = 1 (hoop)\n",
    "        # if bm if gte threshold, class = 2 (mulch)\n",
    "        # if bh and bm are both under threshold, class = 0 (other)\n",
    "        # this is the more complex version that takes into account the difference from the threshold\n",
    "        # if bh_diff = bm_diff, classify it as hoop (1)\n",
    "        \"(bh_diff >= 0 && (bm_diff < 0 || bh_diff >= bm_diff)) ? 1 : ((bm_diff >= 0 && (bh_diff < 0 || bm_diff > bh_diff)) ? 2 : 0)\",\n",
    "        {\n",
    "            # 'bh_gt': bh_gt, # these are 0 or 1\n",
    "            # 'bm_gt': bm_gt, # 0 or 1\n",
    "            'bh_diff': bh_diff,\n",
    "            'bm_diff': bm_diff\n",
    "        }\n",
    "    ).rename('class')\n",
    "    return image.addBands(class_band)\n",
    "\n",
    "classified_integrated = integrate_classes(classified)\n",
    "# if statewide, comment out clip below\n",
    "classes = classified_integrated.select('class').updateMask(crops).clip(county)\n",
    "\n",
    "# Add the results to the map\n",
    "m = geemap.Map()\n",
    "m.addLayer(classified, {}, 'Classified Probabilities', False)\n",
    "m.addLayer(classes, {'min': 0, 'max': 2, 'palette': ['blue', 'red', 'yellow']}, 'Final Classes')\n",
    "m.centerObject(ventura, 6)\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate num/area of pixels classified as both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def integrate_classes(image):\n",
    "#     # Define my thresholds for each band\n",
    "#     bh_th = 0.6\n",
    "#     bm_th = 0.5\n",
    "\n",
    "#     # Define the two bands you want to compare\n",
    "#     # input image is classified, has a hoop_p95 and mulch_p95 band\n",
    "#     bh = image.select('hoop_p95')\n",
    "#     bm = image.select('mulch_p95')\n",
    "\n",
    "#     # difference from threshold for each\n",
    "#     bh_diff = bh.subtract(bh_th).rename('bh_diff')\n",
    "#     bm_diff = bm.subtract(bm_th).rename('bm_diff')\n",
    "\n",
    "#     class_band = image.expression(\n",
    "#         # if both are below threshold, class = 0 (other)\n",
    "#         # if it's bh_diff >= 0 and bm_diff < 0, it's hoop = 1\n",
    "#         # if it's bm_diff >= 0 and bh_diff < 0, it's mulch = 2\n",
    "#         # if it's bh_diff >= 0 and bm_diff >= 0, then compare the two\n",
    "#         #   if bh_diff > bm_diff, class = 3 (confident hoop)\n",
    "#         #   if bm_diff > bh_diff, class = 4 (confident mulch)\n",
    "#         \"(bh_diff < 0 && bm_diff < 0) ? 0\"  # both below threshold\n",
    "#         \" : (bh_diff >= 0 && bm_diff < 0) ? 1\"  # hoop only\n",
    "#         \" : (bm_diff >= 0 && bh_diff < 0) ? 2\"  # mulch only\n",
    "#         \" : (bh_diff > bm_diff) ? 3\"            # both above, hoop more confident\n",
    "#         \" : 4\",                                # both above, mulch more confident\n",
    "#         {\n",
    "\n",
    "#             'bh_diff': bh_diff,\n",
    "#             'bm_diff': bm_diff\n",
    "#         }\n",
    "#     ).rename('class_confusion')\n",
    "#     return image.addBands(class_band)\n",
    "\n",
    "# classified_count = integrate_classes(classified)\n",
    "# # if statewide, comment out clip below\n",
    "# class_confusion = classified_count.select('class_confusion').updateMask(crops).clip(county)\n",
    "\n",
    "# # Add the results to the map\n",
    "# m = geemap.Map()\n",
    "# m.addLayer(classified, {}, 'Classified Probabilities', False)\n",
    "# m.addLayer(class_confusion, {'min': 0, 'max': 4, 'palette': ['blue', 'red', 'yellow', 'pink', 'orange']}, 'Classes with Confusion')\n",
    "# m.centerObject(ventura, 6)\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export the masked, unmasked probability image OR integrated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export integrated raster to asset\n",
    "# export_task = ee.batch.Export.image.toAsset(\n",
    "#     image=classes,\n",
    "#     description=f'classes_image_{county_name}_{wy}',\n",
    "#     assetId=f'projects/ee-tnc-annietaylor/assets/agplastics/PlasticClass_{county_name}Co_WY{wy}_v1_2',\n",
    "#     region=county.geometry(),\n",
    "#     scale=10,\n",
    "#     maxPixels=1e13\n",
    "# )\n",
    "# export_task.start()\n",
    "\n",
    "# export integrated raster to drive\n",
    "export_task = ee.batch.Export.image.toDrive(\n",
    "    # image=classified, # this is all exported probabilities\n",
    "    image=classes, # this is the integrated class\n",
    "    description=f'classes_image_{county_name}_{wy}',\n",
    "    fileNamePrefix=f'PlasticClass_{county_name}Co_WY{wy}_v1_3',\n",
    "    folder='EarthEngine',\n",
    "    region=county.geometry(),\n",
    "    scale=30,\n",
    "    maxPixels=1e13,\n",
    "    crs = 'EPSG:3310' # CA teale albers\n",
    ")\n",
    "# export_task.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Export all waters years for one county "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# county of interest\n",
    "county = santabarbara # ventura, mendo, cruz, santabarbara, sanbenito, humboldt\n",
    "# get text name of county\n",
    "county_name = county.first().get('NAME').getInfo().replace(\" \", \"\")\n",
    "\n",
    "# wys = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "wys = [2024]\n",
    "\n",
    "for wy in wys:\n",
    "    stdate = f'{wy-1}-10-01'\n",
    "    endate = f'{wy}-10-01' # end date is exclusive\n",
    "    sentinel_ts = processedCollection.filterDate(stdate, endate).filterBounds(county)\n",
    "    print(f'Plastic classification for {county_name} County in water year {wy}. Images = ', sentinel_ts.size().getInfo())\n",
    "    # add corresponding ET data for each month (map over collection)\n",
    "    sentinel_ts_ET = sentinel_ts.map(add_ET)\n",
    "    # then classify each image (map over collection)\n",
    "    classified_timeseries = sentinel_ts_ET.map(classify_image_2) #incorporates reprojection to grid\n",
    "    # get the 95 percentile values of that time series\n",
    "    classified = classified_timeseries.reduce(ee.Reducer.percentile([95])).select(['hoop_p95', 'mulch_p95'])\n",
    "    # combine hoop and mulch classes based on distance above respective thresholds\n",
    "    classified_integrated = integrate_classes(classified)\n",
    "    # mask to crops and clip to county\n",
    "    classes = classified_integrated.select('class').updateMask(crops).clip(county)\n",
    "    # export integrated raster to drive or asset\n",
    "    export_tif = ee.batch.Export.image.toDrive(\n",
    "        image=classes,\n",
    "        description=f'classes_image_{county_name}_{wy}',\n",
    "        fileNamePrefix=f'PlasticClass_{county_name}Co_WY{wy}_v1_3',\n",
    "        folder='EarthEngine',\n",
    "        region=county.geometry(),\n",
    "        scale=30,\n",
    "        maxPixels=1e13,\n",
    "        crs = 'EPSG:3310' # CA teale albers\n",
    "    )\n",
    "    export_asset = ee.batch.Export.image.toAsset(\n",
    "        image=classes,\n",
    "        description=f'AssetExport_PlasticClass_{county_name}_{wy}',\n",
    "        assetId=f'projects/ee-tnc-annietaylor/assets/agplastics/PlasticClass_{county_name}Co_WY{wy}_v1_3',\n",
    "        region=county.geometry(),\n",
    "        scale=30,\n",
    "        maxPixels=1e13,\n",
    "        crs = 'EPSG:3310' # CA teale albers\n",
    "    )\n",
    "    # export_tif.start()\n",
    "    # export_asset.start()\n",
    "# print('Exporting images to drive or assets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Export month combinations to make training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many days centered around each training data point to include?\n",
    "period = 366 # total days before + after the training data point to include in classification\n",
    "\n",
    "# any period under 55 days will work on the fly\n",
    "# or you can import assets for these periods: 60, 120, 240, 366\n",
    "\n",
    "# Create a set of (location, date) pairs for training data I'm using\n",
    "location_date_pairs = {\n",
    "    (\"Watsonville\", \"2023-04-12\"),\n",
    "    (\"Oxnard\", \"2023-05-06\"),\n",
    "    (\"Santa Maria\", \"2023-05-05\"),\n",
    "    (\"Oxnard\", \"2022-10-01\"),\n",
    "    (\"Oxnard\", \"2022-11-01\"),\n",
    "    (\"Watsonville\", \"2022-09-11\"),\n",
    "}\n",
    "\n",
    "# Function to filter the df based on location and date\n",
    "def filter_by_location_date(location, date):\n",
    "    df = all_data_test\n",
    "    year_month = date[0:7]\n",
    "    return df[(df['Location'] == location) & (df['Date'].str.startswith(year_month))]\n",
    "\n",
    "def classify_plastic_at_location_date(location, date, period):\n",
    "    # get the training points for that location and date\n",
    "    filtered_data = filter_by_location_date(location, date)\n",
    "    # print(f\"{filtered_data.shape[0]} total training points in {location} on {date}\")\n",
    "    # get the county for that location\n",
    "    if location == \"Watsonville\":\n",
    "        county, county_name = cruz, 'Santa Cruz'\n",
    "    elif location == \"Oxnard\":\n",
    "        county, county_name = ventura, 'Ventura'\n",
    "    elif location == \"Santa Maria\":\n",
    "        county, county_name = santabarbara, 'Santa Barbara'\n",
    "    else:\n",
    "        print(f\"Location {location} not recognized, need a county to filter by.\")\n",
    "    \n",
    "    # get the start and end dates for the imagery \n",
    "    start_date = pd.to_datetime(date) - pd.Timedelta(days=period // 2)\n",
    "    end_date = pd.to_datetime(date) + pd.Timedelta(days=(period // 2) +1) # adding 1 bc end date is exclusive\n",
    "\n",
    "    # filter the timeseries collection for that location and date\n",
    "    sentinel_ts = processedCollection.filterDate(start_date, end_date).filterBounds(county)\n",
    "    # print(f'Classifying plastic in {county_name} County from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}, period of {period} days')\n",
    "    # print(f'Number of images = ', sentinel_ts.size().getInfo())\n",
    "    # add corresponding ET data for each month\n",
    "    sentinel_ts_ET = sentinel_ts.map(add_ET)\n",
    "    # then classify each image (map over collection)\n",
    "    classified_timeseries = sentinel_ts_ET.map(classify_image_2)\n",
    "    # get the 95 percentile values of that time series\n",
    "    classified = classified_timeseries.reduce(ee.Reducer.percentile([95])).select(['hoop_p95', 'mulch_p95'])\n",
    "    # combine hoop and mulch classes based on distance above respective thresholds\n",
    "    classified_integrated = integrate_classes(classified)\n",
    "    # mask to crops and clip to county\n",
    "    classes = classified_integrated.select('class').updateMask(crops).clip(county)\n",
    "    # convert filtered_data DataFrame to an ee.FeatureCollection\n",
    "    features = [\n",
    "        ee.Feature(\n",
    "            ee.Geometry.Point(row['Longitude'], row['Latitude']),\n",
    "            {\n",
    "                'Type': row['Type'],\n",
    "                'Date': row['Date'],\n",
    "                'Location': row['Location']\n",
    "            }\n",
    "        )\n",
    "        for _, row in filtered_data.iterrows()\n",
    "    ]\n",
    "    points_fc = ee.FeatureCollection(features)\n",
    "    # sample the classified image at those points\n",
    "    sampled_points = classes.sampleRegions(\n",
    "        collection=points_fc,\n",
    "        scale=10\n",
    "    )\n",
    "    # return sampled_points\n",
    "    # convert results to pandas df\n",
    "    sampled_df = geemap.ee_to_df(sampled_points)\n",
    "    # print(f\"{sampled_df.shape[0]} training points sampled in {location} on {date} \\n\")\n",
    "    # Map class values: 0 = other, 1 = hoop, 2 = mulch\n",
    "    class_map = {0: 'other', 1: 'hoop', 2: 'mulch'}\n",
    "    sampled_df['class'] = sampled_df['class'].map(class_map)\n",
    "    return sampled_df\n",
    "    # export the sampled points for each location/date to separate csv files\n",
    "    # file_name = f\"output/{location}_{date.replace('-', '')}_{period}day_period.csv\"\n",
    "    # sampled_df.to_csv(sampled_df, file_name, index=False)\n",
    "    # print(f\"Exported sampled points to {file_name}\")\n",
    "\n",
    "def extract_plastic_at_location_date(location, date, period):\n",
    "    # useful for periods longer than 55 days, the function above times out\n",
    "    # get the training points for that location and date\n",
    "    filtered_data = filter_by_location_date(location, date)\n",
    "    # print(f\"{filtered_data.shape[0]} total training points in {location} on {date}\")\n",
    "    # get the county for that location\n",
    "    if location == \"Watsonville\":\n",
    "        county, county_name = cruz, 'SantaCruz'\n",
    "    elif location == \"Oxnard\":\n",
    "        county, county_name = ventura, 'Ventura'\n",
    "    elif location == \"Santa Maria\":\n",
    "        county, county_name = santabarbara, 'SantaBarbara'\n",
    "    else:\n",
    "        print(f\"Location {location} not recognized, need a county to filter by.\")\n",
    "    # import relevant asset \n",
    "    classes = ee.Image(f'projects/ee-tnc-annietaylor/assets/agplastics/PlasticClass_{county_name}Co_{date.replace(\"-\", \"\")}_{period}day_period_v1_1')\n",
    "    # convert filtered_data DataFrame to an ee.FeatureCollection\n",
    "    features = [\n",
    "        ee.Feature(\n",
    "            ee.Geometry.Point(row['Longitude'], row['Latitude']),\n",
    "            {\n",
    "                'Type': row['Type'],\n",
    "                'Date': row['Date'],\n",
    "                'Location': row['Location']\n",
    "            }\n",
    "        )\n",
    "        for _, row in filtered_data.iterrows()\n",
    "    ]\n",
    "    points_fc = ee.FeatureCollection(features)\n",
    "    # sample the classified image at those points\n",
    "    sampled_points = classes.sampleRegions(\n",
    "        collection=points_fc,\n",
    "        scale=10\n",
    "    )\n",
    "    # return sampled_points\n",
    "    # convert results to pandas df\n",
    "    sampled_df = geemap.ee_to_df(sampled_points)\n",
    "    # print(f\"{sampled_df.shape[0]} training points sampled in {location} on {date} \\n\")\n",
    "    # Map class values: 0 = other, 1 = hoop, 2 = mulch\n",
    "    class_map = {0: 'other', 1: 'hoop', 2: 'mulch'}\n",
    "    sampled_df['class'] = sampled_df['class'].map(class_map)\n",
    "    return sampled_df\n",
    "\n",
    "def combine_sampled_data(location_date_pairs, period):\n",
    "    # Combine sampled data for multiple location-date pairs into a single df\n",
    "    all_dfs = [] \n",
    "    for location, date in location_date_pairs:\n",
    "        # df = classify_plastic_at_location_date(location, date, period) # on the fly for shorter periods\n",
    "        df = extract_plastic_at_location_date(location, date, period) # pulls from asset for longer periods\n",
    "        all_dfs.append(df)\n",
    "    return pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "# def combined_sampled_fc(location_date_pairs, period):\n",
    "#     # Combine sampled data for multiple location-date pairs into a single FeatureCollection\n",
    "#     all_features = ee.FeatureCollection()\n",
    "#     for location, date in location_date_pairs:\n",
    "#         sampled_points = classify_plastic_at_location_date(location, date, period)\n",
    "#         all_features.merge(sampled_points)\n",
    "#     return ee.FeatureCollection(all_features)\n",
    "    \n",
    "# all_sampled_data = combined_sampled_fc(location_date_pairs, period)\n",
    "\n",
    "\n",
    "# this part actually runs something, not just fn set up\n",
    "# all_sampled_data = combine_sampled_data(location_date_pairs, period)\n",
    "\n",
    "# # export the combined sampled data to a csv file\n",
    "# output_file = f\"output/test_data_{period}day_period.csv\"\n",
    "# all_sampled_data.to_csv(output_file, index=False)\n",
    "# print(f\"Exported all sampled data to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run this workflow on a list of period values, and compare total and class accuracy values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_sampled_data = combine_sampled_data(location_date_pairs, period)\n",
    "period = 60\n",
    "all_sampled_data = pd.read_csv(f'output/validation_data_{period}day_period.csv')\n",
    "all_sampled_data\n",
    "\n",
    "def accuracy_assessment(all_sampled_data):\n",
    "    # Calculate the accuracy of the classification\n",
    "    cm = confusion_matrix(all_sampled_data['Type'], all_sampled_data['class'], labels=['hoop', 'mulch', 'other'])\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "    # Save overall accuracy and class accuracies to a new df\n",
    "    class_accuracies = [cm[i, i] / np.sum(cm[i, :]) for i in range(len(['hoop', 'mulch', 'other']))]\n",
    "    acc_df = pd.DataFrame({\n",
    "        'class': ['overall', 'hoop', 'mulch', 'other'],\n",
    "        'accuracy': [accuracy] + class_accuracies\n",
    "    })\n",
    "    return acc_df\n",
    "\n",
    "def accuracy_assessment_szn(all_sampled_data):\n",
    "    # Calculate the accuracy of the classification for each season\n",
    "    seasons = {\n",
    "        'Spring': ['03', '04', '05'],\n",
    "        'Summer': ['06', '07', '08'],\n",
    "        'Fall': ['09', '10', '11'],\n",
    "        'Winter': ['12', '01', '02']\n",
    "    }\n",
    "    results = []\n",
    "    for season, months in seasons.items():\n",
    "        # Compare months to the [5:7] index of the date string to get the season\n",
    "        season_data = all_sampled_data[all_sampled_data['Date'].str[5:7].isin(months)]\n",
    "        if not season_data.empty:\n",
    "            cm = confusion_matrix(season_data['Type'], season_data['class'], labels=['hoop', 'mulch', 'other'])\n",
    "            accuracy = np.trace(cm) / np.sum(cm)\n",
    "            class_accuracies = [cm[i, i] / np.sum(cm[i, :]) for i in range(len(['hoop', 'mulch', 'other']))]\n",
    "            results.append({\n",
    "                'season': season,\n",
    "                'overall_accuracy': accuracy,\n",
    "                'hoop_accuracy': class_accuracies[0],\n",
    "                'mulch_accuracy': class_accuracies[1],\n",
    "                'other_accuracy': class_accuracies[2]\n",
    "            })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "print(accuracy_assessment_szn(all_sampled_data))\n",
    "\n",
    "# # confusion matrix compare actual (type) to predicted (class_april23)\n",
    "# cm = confusion_matrix(all_sampled_data['Type'], all_sampled_data['class'], labels=['hoop', 'mulch', 'other'])\n",
    "# # Plot the confusion matrix using seaborn heatmap\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['hoop', 'mulch', 'other'], yticklabels=['hoop', 'mulch', 'other'])\n",
    "# plt.title('Confusion Matrix')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.show()\n",
    "# accuracy = np.trace(cm) / np.sum(cm)\n",
    "\n",
    "# print(f'{all_sampled_data.shape[0]} training points')\n",
    "# print(f'Overall accuracy: {accuracy:.2f}')\n",
    "# # print the accuracy for each class\n",
    "# for i, label in enumerate(['hoop', 'mulch', 'other']):\n",
    "#     class_accuracy = cm[i, i] / np.sum(cm[i, :])\n",
    "#     print(f'{label} accuracy: {class_accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot accuracy vs period by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "\n",
    "for x in [16, 30, 50, 60, 120, 240, 366]: #have to add to this as I export more data\n",
    "    file = f\"output/validation_data_{x}day_period.csv\"\n",
    "    df = pd.read_csv(file)\n",
    "    acc_df = accuracy_assessment_szn(df)\n",
    "    acc_df['period'] = x\n",
    "    all_dfs.append(acc_df)\n",
    "\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "display(df)\n",
    "\n",
    "metrics = ['overall_accuracy', 'hoop_accuracy', 'mulch_accuracy', 'other_accuracy']\n",
    "seasons = df['season'].unique()\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(14, 10), sharex=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axs[idx]\n",
    "    for season in seasons:\n",
    "        season_df = df[df['season'] == season]\n",
    "        ax.plot(season_df['period'], season_df[metric], marker='o', label=season)\n",
    "    ax.set_title(metric.replace('_', ' ').capitalize())\n",
    "    ax.set_xlabel('Period (days)')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot accuracy vs period\n",
    "Using existing exported csvs, see code block right under 8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "\n",
    "for x in [16, 30, 50, 60, 120, 240, 366]: #have to add to this as I export more data\n",
    "    file = f\"output/validation_data_{x}day_period.csv\"\n",
    "    df = pd.read_csv(file)\n",
    "    acc_df = accuracy_assessment(df)\n",
    "    # transpose the df\n",
    "    acc_df = acc_df.set_index('class').T\n",
    "    acc_df['period'] = x\n",
    "    all_dfs.append(acc_df)\n",
    "\n",
    "df = pd.concat(all_dfs, ignore_index=True)\n",
    "display(df)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "fields = ['overall', 'hoop', 'mulch', 'other']\n",
    "for field in fields:\n",
    "    plt.plot(df['period'], df[field], marker='o', label=field)\n",
    "plt.xlabel('Period (days)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy by Period for Each Class')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export images to asset for validation data\n",
    "This is needed for the periods greater than ~55 days. Once exported, you can run code above to get csvs exported, then imported into the chart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = 366\n",
    "\n",
    "location_date_pairs = {\n",
    "    (\"Watsonville\", \"2023-04-12\"),\n",
    "    (\"Oxnard\", \"2023-05-06\"),\n",
    "    (\"Santa Maria\", \"2023-05-05\"),\n",
    "    (\"Oxnard\", \"2022-10-01\"),\n",
    "    (\"Oxnard\", \"2022-11-01\"),\n",
    "    (\"Watsonville\", \"2022-09-11\"),\n",
    "}\n",
    "\n",
    "def export_plastic_at_location_date(location, date, period):\n",
    "    # get the county for that location\n",
    "    if location == \"Watsonville\":\n",
    "        county, county_name = cruz, 'Santa Cruz'\n",
    "    elif location == \"Oxnard\":\n",
    "        county, county_name = ventura, 'Ventura'\n",
    "    elif location == \"Santa Maria\":\n",
    "        county, county_name = santabarbara, 'Santa Barbara'\n",
    "    else:\n",
    "        print(f\"Location {location} not recognized, need a county to filter by.\")\n",
    "    \n",
    "    # get the start and end dates for the imagery \n",
    "    start_date = pd.to_datetime(date) - pd.Timedelta(days=period // 2)\n",
    "    end_date = pd.to_datetime(date) + pd.Timedelta(days=(period // 2) +1) # adding 1 bc end date is exclusive\n",
    "\n",
    "    # filter the timeseries collection for that location and date\n",
    "    sentinel_ts = processedCollection.filterDate(start_date, end_date).filterBounds(county)\n",
    "    # print(f'Classifying plastic in {county_name} County from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}, period of {period} days')\n",
    "    # print(f'Number of images = ', sentinel_ts.size().getInfo())\n",
    "    # add corresponding ET data for each month\n",
    "    sentinel_ts_ET = sentinel_ts.map(add_ET)\n",
    "    # then classify each image (map over collection)\n",
    "    classified_timeseries = sentinel_ts_ET.map(classify_image_2)\n",
    "    # get the 95 percentile values of that time series\n",
    "    classified = classified_timeseries.reduce(ee.Reducer.percentile([95])).select(['hoop_p95', 'mulch_p95'])\n",
    "    # combine hoop and mulch classes based on distance above respective thresholds\n",
    "    classified_integrated = integrate_classes(classified)\n",
    "    # mask to crops and clip to county\n",
    "    classes = classified_integrated.select('class').updateMask(crops).clip(county)\n",
    "    county_name = county_name.replace(\" \", \"\")\n",
    "    filename = f'PlasticClass_{county_name}Co_{date.replace(\"-\", \"\")}_{period}day_period_v1_1'\n",
    "    # export to asset\n",
    "    export_task = ee.batch.Export.image.toAsset(\n",
    "        image=classes,\n",
    "        description=filename,\n",
    "        assetId=f'projects/ee-tnc-annietaylor/assets/agplastics/{filename}',\n",
    "        region=county.geometry(),\n",
    "        scale=10,\n",
    "        maxPixels=1e13\n",
    "    )\n",
    "    export_task.start()\n",
    "    print(f'Exporting {filename} to asset')\n",
    "\n",
    "# Exports five assets per period selected\n",
    "# for location, date in location_date_pairs:\n",
    "#     export_plastic_at_location_date(location, date, period)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 Export in batches to cover CA\n",
    "Have to update code above to be ca instead of county, and uses that water year\n",
    "\n",
    "Pulling from https://spatialthoughts.com/2024/10/23/large-image-exports-gee/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ca_geom = ca.geometry()\n",
    "\n",
    "# # Export CRS\n",
    "# crs = 'EPSG:3310'  # California Albers m\n",
    "\n",
    "# # Pixel size for export (meters)\n",
    "# pixelSize = 10\n",
    "\n",
    "# # Export tile size (pixels) - ideal to keep under 10,000 x 10,000 pixel limit\n",
    "# # tileSize = 10000\n",
    "# tileSize = 20000\n",
    "\n",
    "# # Calculate the grid size (meters)\n",
    "# gridSize = tileSize * pixelSize\n",
    "\n",
    "# # Create the grid covering the geometry bounds\n",
    "# bounds = ca_geom.bounds(**{\n",
    "#   'proj': crs, 'maxError': 1\n",
    "# })\n",
    "\n",
    "# grid = bounds.coveringGrid(**{\n",
    "#   'proj':crs, 'scale': gridSize\n",
    "# })\n",
    "# m = geemap.Map(width=800)\n",
    "# # m.addLayer(classes, {}, 'Classification Image')\n",
    "# m.centerObject(ca, 5)\n",
    "# m.addLayer(grid, {'color': 'blue'}, 'Grid')\n",
    "# # with tileSize 10000, this is 120 images, which isn't a great solution\n",
    "# # with tileSize 20000, this is 35 images -- not sure if this will export though\n",
    "\n",
    "# # this filtered grid is for the tileSize = 15000\n",
    "# # grid_filt = grid.filter(\n",
    "# #     ee.Filter.inList('system:index', tile_ids[0:11] + tile_ids[14:16] + tile_ids[-7:])\n",
    "# # )\n",
    "\n",
    "# # get grid tile IDs\n",
    "# tile_ids = grid.aggregate_array('system:index').getInfo()\n",
    "# # filter first to intersecting with the ca geom\n",
    "# grid_filt = grid.filterBounds(ca_geom)\n",
    "# # then filter out tiles that barely intersect with the ca geom\n",
    "# grid_filt = grid_filt.filter( # these indices are specific to the tileSize = 20000\n",
    "#     ee.Filter.inList('system:index', [tile_ids[3], tile_ids[6], tile_ids[27], tile_ids[32]]).Not()\n",
    "# )\n",
    "\n",
    "# m.addLayer(grid_filt, {'color': 'pink'}, 'Grid Filtered')\n",
    "# m.addLayer(ca_geom, {'color': 'red'}, 'California Geometry')\n",
    "# # m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate the CRS transform for export\n",
    "disregard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the coordinates of the top-left corner of the grid\n",
    "# bounds = grid.geometry().bounds(**{\n",
    "#   'proj': crs, 'maxError': 1\n",
    "# })\n",
    "\n",
    "# # Extract the coordinates of the grid\n",
    "# coordList = ee.Array.cat(bounds.coordinates(), 1)\n",
    "# xCoords = coordList.slice(1, 0, 1)\n",
    "# yCoords = coordList.slice(1, 1, 2)\n",
    "\n",
    "# # Get the coordinates of the top-left pixel\n",
    "# xMin = xCoords.reduce('min', [0]).get([0,0])\n",
    "# yMax = yCoords.reduce('max', [0]).get([0,0])\n",
    "\n",
    "# # Create the CRS Transform\n",
    "# # The transform consists of 6 parameters:\n",
    "# # [xScale, xShearing, xTranslation, yShearing, yScale, yTranslation]\n",
    "# transform = ee.List([pixelSize, 0, xMin, 0, -pixelSize, yMax]).getInfo()\n",
    "# print(transform)\n",
    "\n",
    "# # Assign a no-data value\n",
    "# noDataValue = 0\n",
    "# classes_unmasked = classes.unmask(**{\n",
    "#     'value':noDataValue,\n",
    "#     'sameFootprint': False\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export tiles for CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # commenting this out for now so that nothing runs\n",
    "# # lets try exporting one of the tiles\n",
    "# tile_ids_filt = grid_filt.aggregate_array('system:index').getInfo()\n",
    "# print('Total tiles', len(tile_ids_filt))\n",
    "\n",
    "# tile_ids_filt = tile_ids_filt[0:1] # just for testing, export one tile\n",
    "\n",
    "# # Exporting all tiles from list to google drive\n",
    "# for i, tile_id in enumerate(tile_ids_filt):\n",
    "#     feature = ee.Feature(grid_filt.toList(3, i).get(0))\n",
    "#     # display(grid_filt.toList(3, i).get(0))\n",
    "#     geometry = feature.geometry()\n",
    "#     task_name = f'Plastic_CA_WY{wy}_tile_' + tile_id.replace(',', '_') + 'v1_1'\n",
    "#     task = ee.batch.Export.image.toDrive(**{\n",
    "#         'image': classes_unmasked,\n",
    "#         'description': f'Export_{task_name}',\n",
    "#         'fileNamePrefix': task_name,\n",
    "#         'folder':'EarthEngine',\n",
    "#         'crs': crs,\n",
    "#         'crsTransform': transform,\n",
    "#         'region': geometry,\n",
    "#         'maxPixels': 1e13\n",
    "#     })\n",
    "#     task.start()\n",
    "#     print(f'Started Task to export tile {tile_id}, export #{i+1}')\n",
    "# # grid_filt\n",
    "# # tile_ids_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Thresholding - extract WY values to validation points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split into EE water years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set which water years you need\n",
    "# wys = [2019, 2020, 2021, 2022, 2023]\n",
    "\n",
    "# # initialize an empty image\n",
    "# wys_all = ee.Image()\n",
    "\n",
    "# for wy in wys:\n",
    "#     stdate = f'{wy-1}-10-01'\n",
    "#     endate = f'{wy}-09-30'\n",
    "#     mulch_label = f'mulch_wy{wy}'\n",
    "#     hoop_label = f'hoop_wy{wy}'\n",
    "#     wy_images = classified_timeseries.filterDate(stdate, endate).select(['hoop', 'mulch', 'other'])\n",
    "#     wy_bands = wy_images.reduce(ee.Reducer.percentile([95])).select(['hoop_p95', 'mulch_p95']).rename([hoop_label, mulch_label])\n",
    "#     wys_all = wys_all.addBands(wy_bands)\n",
    "# wys_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# # gonna try exporting it to an asset\n",
    "# export_task = ee.batch.Export.image.toAsset(\n",
    "#     image=wys_all,\n",
    "#     description='wats_wys',\n",
    "#     assetId='projects/ee-tnc-annietaylor/assets/agplastics/SantaCruzCo_WYs_95p',\n",
    "#     region=cruz.geometry(), #CHANGE THIS\n",
    "#     scale=10,\n",
    "#     maxPixels=1e13\n",
    "# )\n",
    "# export_task.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export band values to training points using EE fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample hoop/mulch probability for each WY to all validation points\n",
    "# wats_fcpts = geemap.df_to_ee(watsonville, latitude='Latitude', longitude='Longitude')\n",
    "# wys_all = ee.Image('projects/ee-tnc-annietaylor/assets/agplastics/SantaCruzCo_WYs_95p')\n",
    "\n",
    "# sampled_points = wys_all.sampleRegions(\n",
    "#     collection=wats_fcpts,\n",
    "#     scale=200 # ideally 10m\n",
    "# )\n",
    "\n",
    "# # Convert the sampled points to df\n",
    "# sampled_points_df = geemap.ee_to_df(sampled_points)\n",
    "\n",
    "# # Display the DataFrame\n",
    "# print(sampled_points_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export band values to training points in geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop through all rasters and sample to the points for that region\n",
    "# # watsonville:\n",
    "# # df = watsonville\n",
    "# # wys = [2020, 2021, 2022, 2023]\n",
    "# # rasters = [\n",
    "# #     'Plastic_SantaCruzCo_95p_WY2020.tif',\n",
    "# #     'Plastic_SantaCruzCo_95p_WY2021.tif',\n",
    "# #     'PlasticClass_SantaCruzCo_95p_WY2022.tif',\n",
    "# #     'Plastic_SantaCruzCo_95p_WY2023.tif'\n",
    "# # ]\n",
    "\n",
    "# # df = oxnard\n",
    "# # wys = [2019, 2020, 2021, 2022, 2023]\n",
    "# # rasters = [\n",
    "# #     'Plastic_VenturaCo_95p_WY2019.tif',\n",
    "# #     'Plastic_VenturaCo_95p_WY2020.tif',\n",
    "# #     'Plastic_VenturaCo_95p_WY2021.tif',\n",
    "# #     'PlasticClass_venturaCo_95p_WY2022.tif',\n",
    "# #     'Plastic_VenturaCo_95p_WY2023.tif'\n",
    "# # ]\n",
    "\n",
    "# df = santamaria\n",
    "# wys = [2021, 2022, 2023]\n",
    "# rasters = [\n",
    "#     'Plastic_SBCo_95p_WY2021.tif',\n",
    "#     'PlasticClass_santabarbaraCo_95p_WY2022.tif',\n",
    "#     'Plastic_SBCo_95p_WY2023.tif'\n",
    "# ]\n",
    "\n",
    "# for wy, raster in zip(wys, rasters):\n",
    "#     # src = rasterio.open(f'N:\\OCEANS_Program\\Plastics\\Agricultural_Plastics\\AgPlastics_Pro\\EE_uploads_exports\\Watsonville\\\\{raster}')\n",
    "#     # src = rasterio.open(f'N:\\OCEANS_Program\\Plastics\\Agricultural_Plastics\\AgPlastics_Pro\\EE_uploads_exports\\Oxnard\\\\{raster}')\n",
    "#     src = rasterio.open(f'N:\\OCEANS_Program\\Plastics\\Agricultural_Plastics\\AgPlastics_Pro\\EE_uploads_exports\\SantaMaria\\\\{raster}')\n",
    "#     coord_list = [(x, y) for x, y in zip(df[\"Longitude\"], df[\"Latitude\"])]\n",
    "#     df[[f'hoop_wy{wy}', f'mulch_wy{wy}']] = [a for a in src.sample(coord_list)]\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign hoop & mulch fields to WY value based on training data date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First add a col with water year to the df\n",
    "# # make a new col that is in datetime format\n",
    "# df['pDate'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# def add_WY(row):\n",
    "#     if row['pDate'].month >= 10:\n",
    "#         return row['pDate'].year + 1\n",
    "#     else:\n",
    "#         return row['pDate'].year\n",
    "\n",
    "# df['WY'] = df.apply(add_WY, axis=1)\n",
    "\n",
    "# # print(df['WY'].unique())\n",
    "\n",
    "# # exclude 2018 since we don't have images for it\n",
    "# # this will be different for each region\n",
    "# # df = df[df['WY'] > 2018] # need for watsonville\n",
    "# # df = df[df['WY'] < 2024] # need for oxnard\n",
    "# df = df[df['WY'] > 2020] # need for santamaria\n",
    "\n",
    "# # set the hoop field to be the corresponding hoop_wy{wy} value based on the WY field\n",
    "# df['hoop'] = df.apply(lambda x: x[f'hoop_wy{x[\"WY\"]}'], axis=1)\n",
    "# df['mulch'] = df.apply(lambda x: x[f'mulch_wy{x[\"WY\"]}'], axis=1)\n",
    "\n",
    "# df.head()\n",
    "# # df[df['WY'] == 2021].head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to csv for easy import into threshold_determination script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_name = train_path + 'Watsonville_val_BM_P95.csv'\n",
    "# csv_name = train_path + 'Oxnard_all_P95.csv'\n",
    "# csv_name = train_path + 'SantaMaria_val_o_P95.csv'\n",
    "# df.to_csv(csv_name)\n",
    "# when importing, drop 0 and null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summarize by Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce regions for one county/wy at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DWR field polygons from 2023 filtered to our three working counties\n",
    "fields = ee.FeatureCollection('projects/ee-tnc-annietaylor/assets/agplastics/DWR_2023_filtered_wgs')\n",
    "# still 34K polygons\n",
    "\n",
    "# filter this to county I'm working in and a few columns to save memory\n",
    "fields_county = fields.filter(ee.Filter.eq('COUNTY', 'Santa Cruz')).select(['COUNTY', 'MAIN_CROP', 'SYMB_CLASS', 'ACRES'])\n",
    "\n",
    "# pull up an asset from santa cruz\n",
    "wy23cruz = ee.Image('projects/ee-tnc-annietaylor/assets/agplastics/PlasticClass_SantaCruzCo_WY2023_v1_2')\n",
    "\n",
    "fields_county_majority = wy23cruz.reduceRegions(\n",
    "    collection=fields_county,\n",
    "    reducer=ee.Reducer.mode(),\n",
    "    scale=30,\n",
    "    crs='EPSG:3310' # CA Albers\n",
    ")\n",
    "\n",
    "map = geemap.Map()\n",
    "map.centerObject(fields_county_majority, 8)\n",
    "map.addLayer(wy23cruz, {'min': 0, 'max': 2, 'palette': ['blue', 'red', 'yellow']}, 'WY2023 Santa Cruz Plastic Classes')\n",
    "\n",
    "# Paint both the fill and the edges.\n",
    "empty = ee.Image().byte()\n",
    "fields_fill = empty.paint(fields_county_majority, 'mode')#.paint(fields_county_majority, 0, 2)\n",
    "map.add_layer(\n",
    "    fields_fill,\n",
    "    {'palette': ['blue', 'red', 'yellow'], 'min': 0, 'max': 2},\n",
    "    'Fields Majority',\n",
    ")\n",
    "\n",
    "# map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce regions for all wys in a county, or all counties in a wy, or all in all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select water years and county of interest\n",
    "# wys = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "wys = [2024]\n",
    "county = cruz # ventura, cruz, santabarbara, sanbenito, humboldt, mendo\n",
    "\n",
    "# extract text name of county\n",
    "county_name = county.first().get('NAME').getInfo() # 'Santa Cruz'\n",
    "county_name_strp = county_name.replace(\" \", \"\") # 'SantaCruz'\n",
    "\n",
    "for wy in wys:\n",
    "    # get the plastic classification asset for that county and year\n",
    "    plastic_class = ee.Image(f'projects/ee-tnc-annietaylor/assets/agplastics/PlasticClass_{county_name_strp}Co_WY{wy}_v1_3')\n",
    "    # get the fields dataset from that year\n",
    "    # crop_wy = 2023 if wy == 2024 else wy # we now have 2024 dataset\n",
    "    fields_wy = ee.FeatureCollection(f'projects/ee-tnc-annietaylor/assets/agplastics/DWR_{wy}_filtered_wgs')\n",
    "    # filter this to county I'm working in and a few columns to save memory\n",
    "    fields_wy_county = fields_wy.filter(ee.Filter.eq('COUNTY', county_name)).select(['COUNTY', 'MAIN_CROP', 'SYMB_CLASS', 'ACRES', 'UniqueID'])\n",
    "\n",
    "    fields_county_majority = plastic_class.reduceRegions(\n",
    "        collection=fields_wy_county,\n",
    "        reducer=ee.Reducer.mode(),\n",
    "        scale=30,\n",
    "        crs='EPSG:3310' # CA Albers\n",
    "    )\n",
    "    # export the results to new feature collection asset\n",
    "    export_fields = ee.batch.Export.table.toAsset(\n",
    "        collection=fields_county_majority,\n",
    "        description=f'FieldMajority_{county_name_strp}_WY{wy}_v1_3',\n",
    "        assetId = f'projects/ee-tnc-annietaylor/assets/agplastics/FieldMajority_{county_name_strp}_WY{wy}_v1_3',\n",
    "    )\n",
    "    # export_fields.start()\n",
    "\n",
    "    # # export fields to csv on drive, have to join with fields data locally using UniqueID \n",
    "    # export_fields_csv = ee.batch.Export.table.toDrive(\n",
    "    #     collection=fields_county_majority,\n",
    "    #     description=f'FieldMajority_{county_name_strp}_WY{wy}_v1_3_csv',\n",
    "    #     folder='EarthEngine',\n",
    "    #     fileNamePrefix=f'FieldMajority_{county_name_strp}_WY{wy}_v1_3',\n",
    "    #     fileFormat='CSV'\n",
    "    # )\n",
    "    # # export_fields_csv.start()\n",
    "    \n",
    "# fields_county_majority\n",
    "\n",
    "# can easily loop through all WY and county combos once I have those classification assets exported\n",
    "# counties = [ventura, cruz, santabarbara]\n",
    "# for county in counties: \n",
    "#     county_name = county.first().get('NAME').getInfo() # 'Santa Cruz'\n",
    "#     county_name_strp = county_name.replace(\" \", \"\") # 'SantaCruz'\n",
    "#     for wy in wys:\n",
    "#         # get the fields dataset from that year\n",
    "#         fields_wy = ee.FeatureCollection(f'projects/ee-tnc-annietaylor/assets/agplastics/DWR_{wy}_filtered_wgs')\n",
    "#         # filter this to county I'm working in and a few columns to save memory\n",
    "#         fields_wy_county = fields_wy.filter(ee.Filter.eq('COUNTY', county_name)).select(['COUNTY', 'MAIN_CROP', 'SYMB_CLASS', 'ACRES', 'UniqueID'])\n",
    "#         # get the plastic classification asset for that county and year\n",
    "#         plastic_class = ee.Image(f'projects/ee-tnc-annietaylor/assets/agplastics/PlasticClass_{county_name_strp}Co_WY{wy}_v1_3')\n",
    "#         fields_county_majority = plastic_class.reduceRegions(\n",
    "#             collection=fields_wy_county,\n",
    "#             reducer=ee.Reducer.mode(),\n",
    "#             scale=30,\n",
    "#             crs='EPSG:3310' # CA Albers\n",
    "#         )\n",
    "#         # export the results to a csv\n",
    "#         export_fields = ee.batch.Export.table.toDrive(\n",
    "#             collection=fields_county_majority,\n",
    "#             description=f'FieldMajority_{county_name_strp}_WY{wy}_v1_3',\n",
    "#             folder='EarthEngine',\n",
    "#             fileNamePrefix=f'FieldMajority_{county_name_strp}_WY{wy}_v1_3',\n",
    "#             fileFormat='CSV'\n",
    "#         )\n",
    "#         export_fields.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Area Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Get area of mulch and hoop from feature collections\n",
    "\n",
    "landID data already has a ACRES field, calculated in CA teale albers m or similar\n",
    "\n",
    "that's likely to be more accurate than calculating from geometry area in wgs84/EE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select water years and county of interest\n",
    "wys = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "counties = [ventura, cruz, santabarbara]\n",
    "\n",
    "# initialize empty list to hold dataframes\n",
    "all_dfs = []\n",
    "\n",
    "\n",
    "for county in counties:\n",
    "    # extract text name of county\n",
    "    county_name = county.first().get('NAME').getInfo() # 'Santa Cruz'\n",
    "    county_name_strp = county_name.replace(\" \", \"\") # 'SantaCruz'\n",
    "\n",
    "    for wy in wys:\n",
    "        # import fields data from asset\n",
    "        assetId = f'projects/ee-tnc-annietaylor/assets/agplastics/FieldMajority_{county_name_strp}_WY{wy}_v1_3'\n",
    "        fields_county_majority = ee.FeatureCollection(assetId)\n",
    "\n",
    "        # Filter out features where 'mode' (plastic class) is None, then sum ACRES by class\n",
    "        filtered_fields = fields_county_majority.filter(ee.Filter.notNull(['mode']))\n",
    "        sums = filtered_fields.reduceColumns(\n",
    "            selectors=['ACRES', 'mode'],\n",
    "            reducer=ee.Reducer.sum().group(groupField=1, groupName='mode'),\n",
    "        )\n",
    "\n",
    "        # get total acres of all fields within the county\n",
    "        total_acres = filtered_fields.aggregate_sum('ACRES').getInfo()\n",
    "\n",
    "        # make a row of a df with county, wy, total acres, hoop acres, mulch acres, other acres, hoop %, mulch %, other %\n",
    "        sums_list = sums.get('groups').getInfo()\n",
    "        acres_dict = {item['mode']: item['sum'] for item in sums_list}\n",
    "        hoop_acres = acres_dict.get(1, 0)\n",
    "        mulch_acres = acres_dict.get(2, 0)\n",
    "        other_acres = acres_dict.get(0, 0)\n",
    "\n",
    "        hoop_pct = (hoop_acres / total_acres) * 100 if total_acres > 0 else 0\n",
    "        mulch_pct = (mulch_acres / total_acres) * 100 if total_acres > 0 else 0\n",
    "        other_pct = (other_acres / total_acres) * 100 if total_acres > 0 else 0\n",
    "\n",
    "        summary_df = pd.DataFrame([{\n",
    "            'County': county_name,\n",
    "            'Water Year': wy,\n",
    "            'Total Acres': total_acres,\n",
    "            'Hoop Acres': hoop_acres,\n",
    "            'Mulch Acres': mulch_acres,\n",
    "            'Other Acres': other_acres,\n",
    "            'Hoop %': hoop_pct,\n",
    "            'Mulch %': mulch_pct,\n",
    "            'Other %': other_pct,\n",
    "            'Hoop km2': hoop_acres * 0.00404686,\n",
    "            'Mulch km2': mulch_acres * 0.00404686\n",
    "        }])\n",
    "        all_dfs.append(summary_df)\n",
    "\n",
    "# concatenate all dataframes into one\n",
    "final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "display(final_df)\n",
    "# save to csv\n",
    "# final_df.to_csv('output/FieldMajority_Area_AllCounties_WYs_v1_3_2024update.csv', index=False)\n",
    "# runs all three counties in 10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked area chart for Hoop House and Mulch Plastic Acres over time for each county\n",
    "# counties = final_df['County'].unique()\n",
    "counties = ['Ventura', 'Santa Barbara', 'Santa Cruz']\n",
    "\n",
    "\n",
    "water_years = sorted(final_df['Water Year'].unique())\n",
    "\n",
    "# Define custom colors for the three counties\n",
    "county_colors = ['lightseagreen', 'orange', 'skyblue']\n",
    "\n",
    "plt.rcParams['font.family'] = 'Calibri'\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6), sharex=True)\n",
    "\n",
    "title_fontsize = 20\n",
    "label_fontsize = 16\n",
    "tick_fontsize = 14\n",
    "legend_fontsize = 14\n",
    "\n",
    "for ax, class_col, title in zip(axs,\n",
    "    ['Hoop km2', 'Mulch km2'],\n",
    "    ['Hoop House Area by County', 'Mulch Plastic Area by County']\n",
    "    ):\n",
    "    data = []\n",
    "    for county in counties:\n",
    "        county_data = final_df[final_df['County'] == county].set_index('Water Year').reindex(water_years)\n",
    "        data.append(county_data[class_col].values)\n",
    "    data = np.array(data)\n",
    "    ax.stackplot(water_years, data, labels=counties, colors=county_colors)\n",
    "    ax.set_title(title, fontsize=title_fontsize)\n",
    "    ax.set_xlabel('Water Year', fontsize=label_fontsize)\n",
    "    ax.set_ylabel('Km', fontsize=label_fontsize)\n",
    "    ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, p: f'{int(x):,}'))\n",
    "    ax.grid(axis='x', linestyle='-', color='white')\n",
    "    ax.tick_params(axis='both', labelsize=tick_fontsize)\n",
    "\n",
    "axs[0].set_ylim(0, 140) # same y axis for comparison\n",
    "\n",
    "# reordering the legend labels\n",
    "handles, labels = axs[0].get_legend_handles_labels()\n",
    "labels = ['Ventura County', 'Santa Barbara County', 'Santa Cruz County'] # add county to label\n",
    "order = [2, 1, 0]\n",
    "\n",
    "# pass handle & labels lists along with order as below\n",
    "axs[0].legend([handles[i] for i in order], [labels[i] for i in order],\n",
    "    loc='upper left',\n",
    "    handleheight=1.2,\n",
    "    handlelength=1.75,\n",
    "    fontsize=legend_fontsize\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked area percent cover by county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# counties = final_df['County'].unique() not N to S\n",
    "counties = ['Santa Cruz', 'Santa Barbara', 'Ventura']\n",
    "\n",
    "title_fontsize = 20\n",
    "label_fontsize = 16\n",
    "tick_fontsize = 14\n",
    "legend_fontsize = 14\n",
    "\n",
    "for ax, county in zip(axes, counties):\n",
    "    df_county = final_df[final_df['County'] == county]\n",
    "    ax.stackplot(\n",
    "        df_county['Water Year'],\n",
    "        df_county['Mulch %'],\n",
    "        df_county['Hoop %'],\n",
    "        labels=['Mulch', 'Hoop House'],\n",
    "        colors=['#F0E442','#D55E00'],\n",
    "        alpha=0.95\n",
    "    )\n",
    "    ax.set_title(f'{county} County', fontsize=title_fontsize)\n",
    "    ax.set_xlabel('Water Year', fontsize=label_fontsize)\n",
    "    ax.set_xticks(df_county['Water Year'])\n",
    "    ax.grid(axis='x', linestyle='-', color='white')\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter())\n",
    "    ax.set_ylabel('Percent of Ag Area', fontsize=label_fontsize)\n",
    "    ax.set_ylim(0, 50)\n",
    "    ax.tick_params(axis='both', labelsize=tick_fontsize)\n",
    "axes[2].legend(['Plastic Mulch', 'Hoop House'],\n",
    "        loc='upper left',\n",
    "        handleheight=1.2,\n",
    "        handlelength=1.75,\n",
    "        fontsize=legend_fontsize\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Frequency of plastic application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing this just for watsonville right now\n",
    "county = ventura\n",
    "wys = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "# wys = [2024]\n",
    "\n",
    "county_name = county.first().get('NAME').getInfo() # 'Santa Cruz'\n",
    "county_name_strp = county_name.replace(\" \", \"\") # 'SantaCruz'\n",
    "\n",
    "# initialize an empty image\n",
    "all_plastic = ee.Image()\n",
    "all_hoop = ee.Image()\n",
    "all_mulch = ee.Image()\n",
    "\n",
    "for wy in wys:\n",
    "    # import fields data from asset\n",
    "    assetId = f'projects/ee-tnc-annietaylor/assets/agplastics/FieldMajority_{county_name_strp}_WY{wy}_v1_3'\n",
    "    fields_county_majority = ee.FeatureCollection(assetId)\n",
    "\n",
    "    # Filter out features where 'mode' (plastic class) is None, then convert to raster\n",
    "    fields_img = fields_county_majority.filter(ee.Filter.notNull(['mode'])).reduceToImage(\n",
    "        properties=['mode'], reducer=ee.Reducer.first()\n",
    "    ) # resolution seems very high\n",
    "\n",
    "    # create a new image that is 1 whenever the fields image is 1 or 2\n",
    "    plastic = fields_img.gt(0).rename(f'plastic_{wy}')\n",
    "    all_plastic = all_plastic.addBands(plastic)\n",
    "\n",
    "    # make an image from only hoop pixels and set to 1\n",
    "    hoop = fields_img.eq(1).rename(f'hoop_{wy}')\n",
    "    all_hoop = all_hoop.addBands(hoop)\n",
    "\n",
    "    # make an image from only mulch pixels and set to 1\n",
    "    mulch = fields_img.eq(2).rename(f'mulch_{wy}')\n",
    "    all_mulch = all_mulch.addBands(mulch)\n",
    "\n",
    "# Select only bands that start with 'plastic' to remove 'constant' band\n",
    "# don't need to do this, the constant band is set to None \n",
    "# plastic_bands = [band for band in all_plastic.bandNames().getInfo() if band.startswith('plastic')]\n",
    "# all_plastic = all_plastic.select(plastic_bands)\n",
    "\n",
    "# sum all bands together\n",
    "all_plastic = all_plastic.reduce(ee.Reducer.sum()).rename('plastic_sum')\n",
    "all_hoop = all_hoop.reduce(ee.Reducer.sum()).rename('hoop_sum')\n",
    "all_mulch = all_mulch.reduce(ee.Reducer.sum()).rename('mulch_sum')\n",
    "\n",
    "# combine these images into one with three bands for exporting\n",
    "plastic_sum = all_plastic.addBands(all_hoop).addBands(all_mulch).toInt8()\n",
    "# display(plastic_sum)\n",
    "\n",
    "map = geemap.Map()\n",
    "map.centerObject(county, 8)\n",
    "map.add_basemap('SATELLITE')\n",
    "map.addLayer(all_plastic, {'min': 0, 'max': 6, 'palette': ['white', 'purple']}, 'Num Plastic Years')\n",
    "map.addLayer(all_hoop, {'min': 0, 'max': 6, 'palette': ['white', 'red']}, 'Num Hoop Years')\n",
    "map.addLayer(all_mulch, {'min': 0, 'max': 6, 'palette': ['white', 'orange']}, 'Num Mulch Years')\n",
    "# display(map)\n",
    "\n",
    "# export to tif\n",
    "export_task = ee.batch.Export.image.toDrive(\n",
    "    image=plastic_sum,\n",
    "    description=f'plastic_freq_{county_name_strp}',\n",
    "    fileNamePrefix=f'PlasticFrequency_{county_name_strp}Co_v1_3',\n",
    "    folder='EarthEngine',\n",
    "    region=county.geometry(),\n",
    "    scale=10,\n",
    "    maxPixels=1e13,\n",
    "    crs = 'EPSG:3310' # CA teale albers\n",
    ")\n",
    "# export_task.start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### old method - Get area of mulch and hoop coverage from final WY rasters for each county, save to csv\n",
    "takes about a minute to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# county = santabarbara\n",
    "# wys = [2019, 2020, 2021, 2022, 2023, 2024]\n",
    "\n",
    "# county_name = county.first().get('NAME').getInfo().replace(\" \", \"\")\n",
    "\n",
    "# # Create a pixel area image. Pixel values are square meters based on a given CRS and scale (or CRS transform).\n",
    "# pixel_area = ee.Image.pixelArea()\n",
    "# # The default projection is WGS84 with 1-degree scale.\n",
    "# # display('Pixel area default projection', pixel_area.projection())\n",
    "\n",
    "# df_county_ts = []\n",
    "\n",
    "# for wy in wys:\n",
    "#     # import asset\n",
    "#     ras = ee.Image(f'projects/ee-tnc-annietaylor/assets/agplastics/PlasticClass_{county_name}Co_WY{wy}_v1_2')\n",
    "\n",
    "#     # reproject the raster (doesn't appear necessary, results identical) \n",
    "#     ras = ras.reproject(crs='EPSG:3310', scale=10)\n",
    "\n",
    "#     hoop = ras.select('class').eq(1)\n",
    "#     mulch = ras.select('class').eq(2) # TODO print these to map to double check\n",
    "\n",
    "#     hoop_area = pixel_area.updateMask(hoop)\n",
    "#     mulch_area = pixel_area.updateMask(mulch)\n",
    "\n",
    "#     hoop_area_calc = hoop_area.reduceRegion(\n",
    "#         reducer=ee.Reducer.sum(),\n",
    "#         geometry=county.geometry(),\n",
    "#         crs=ee.Projection('EPSG:3310'),\n",
    "#         scale=10,\n",
    "#         maxPixels=1e8,\n",
    "#     )\n",
    "\n",
    "#     mulch_area_calc = mulch_area.reduceRegion(\n",
    "#         reducer=ee.Reducer.sum(),\n",
    "#         geometry=county.geometry(),\n",
    "#         crs=ee.Projection('EPSG:3310'),\n",
    "#         scale=10,\n",
    "#         maxPixels=1e8,\n",
    "#     )\n",
    "\n",
    "#     hoop_m2 = hoop_area_calc.getNumber('area').getInfo()\n",
    "#     mulch_m2 = mulch_area_calc.getNumber('area').getInfo()\n",
    "\n",
    "#     # save these numbers to a dataframe\n",
    "#     data = {\n",
    "#         'Year': [wy],\n",
    "#         'County': [county_name],\n",
    "#         'Hoop_m2': [hoop_m2],\n",
    "#         'Hoop_acres': [(hoop_m2/4046.8564224)], # to convert m2 to acres\n",
    "#         'Mulch_m2': [mulch_m2],\n",
    "#         'Mulch_acres': [(mulch_m2/4046.8564224)]\n",
    "#     }\n",
    "#     df_county_ts.append(pd.DataFrame(data))\n",
    "\n",
    "# df_area = pd.concat(df_county_ts)\n",
    "# display(df_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate acreage as percent of ag lands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ag = ee.Image('projects/ee-tnc-annietaylor/assets/agplastics/dwr_allyears_23')\n",
    "\n",
    "# # clip to county\n",
    "# county_ag = all_ag.clip(county)\n",
    "\n",
    "# # reproject the raster\n",
    "# county_ag = county_ag.reproject(crs='EPSG:3310', scale=10)\n",
    "\n",
    "# # mask pixel area image to ag lands\n",
    "# ag_area = pixel_area.updateMask(county_ag)\n",
    "\n",
    "# ag_area_calc = ag_area.reduceRegion(\n",
    "#     reducer=ee.Reducer.sum(),\n",
    "#     geometry=county.geometry(),\n",
    "#     crs=ee.Projection('EPSG:3310'),\n",
    "#     scale=10,\n",
    "#     maxPixels=1e8,\n",
    "# )\n",
    "\n",
    "# # extract area and add to a df\n",
    "# ag_area_m2 = ag_area_calc.getNumber('area').getInfo()\n",
    "\n",
    "# results = {\n",
    "#     'County': county_name,\n",
    "#     'Ag_m2': ag_area_m2,\n",
    "#     'Ag_acres': (ag_area_m2/4046.8564224)\n",
    "# }\n",
    "\n",
    "# display(results)\n",
    "\n",
    "# # add a column to df_area standardized by ag land area in that county\n",
    "# df_area['Hoop_acres_percent'] = df_area['Hoop_acres'] / results['Ag_acres']\n",
    "# df_area['Mulch_acres_percent'] = df_area['Mulch_acres'] / results['Ag_acres']\n",
    "# df_area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save to a csv for import into a stacked chart\n",
    "# # csv_name = f'N:/OCEANS_Program/Plastics/Agricultural_Plastics/Area_by_County/plasticAreaOverTime_{county_name}County_v1_2.csv'\n",
    "# csv_name = f'C:/Users/annie.taylor/Documents/Plastics/ag-plastics/data/Area_by_County/plasticAreaOverTime_{county_name}County_v1_2.csv'\n",
    "# df_area.to_csv(csv_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot plastic acreage over time - one county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# county_name = county_name.replace(\"Santa\", \"Santa \")\n",
    "# # Plot the change in mulch_acres and hoop_acres over Year side by side\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# # Plot for Mulch Acres\n",
    "# ax1.plot(df_area['Year'], df_area['Mulch_acres'], marker='o', linestyle='-', color='orange')\n",
    "# ax1.set_ylabel('Acres of Plastic Mulch')\n",
    "# ax1.set_title(f'Plastic Mulch Acreage in {county_name} County')\n",
    "# ax1.set_xticks(df_area['Year'])  # Set x-axis ticks to be the integer values of the year\n",
    "\n",
    "# # Plot for Hoop Acres\n",
    "# ax2.plot(df_area['Year'], df_area['Hoop_acres'], marker='o', linestyle='-', color='darkred')\n",
    "# ax2.set_ylabel('Acres of Hoop Houses')\n",
    "# ax2.set_title(f'Hoop House Acreage in {county_name} County')\n",
    "# ax2.set_xticks(df_area['Year'])  # Set x-axis ticks to be the integer values of the year\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot plastic percent of acreage over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the change in mulch_acres_percent and hoop_acres_percent over year side by side\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# # Plot for Mulch Acres\n",
    "# ax1.plot(df_area['Year'], df_area['Mulch_acres_percent'], marker='o', linestyle='-', color='orange')\n",
    "# ax1.set_ylabel('Plastic Mulch Coverage of Ag Land (%)')\n",
    "# ax1.set_title(f'Plastic Mulch % Coverage in {county_name} County')\n",
    "# ax1.set_xticks(df_area['Year'])  # Set x-axis ticks to be the integer values of the year\n",
    "\n",
    "# # Plot for Hoop Acres\n",
    "# ax2.plot(df_area['Year'], df_area['Hoop_acres_percent'], marker='o', linestyle='-', color='darkred')\n",
    "# ax2.set_ylabel('Hoop House Coverage of Ag Land (%)')\n",
    "# ax2.set_title(f'Hoop House % Coverage in {county_name} County')\n",
    "# ax2.set_xticks(df_area['Year'])  \n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD Area charts - import csvs with area calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all three csvs and combine into one df\n",
    "# df_sb = pd.read_csv(r'N:/OCEANS_Program/Plastics/Agricultural_Plastics/Area_by_County/plasticAreaOverTime_SantaBarbaraCounty_v1_2.csv')\n",
    "# df_vt = pd.read_csv(r'N:/OCEANS_Program/Plastics/Agricultural_Plastics/Area_by_County/plasticAreaOverTime_VenturaCounty_v1_2.csv')\n",
    "# df_sc = pd.read_csv(r'N:/OCEANS_Program/Plastics/Agricultural_Plastics/Area_by_County/plasticAreaOverTime_SantaCruzCounty_v1_2.csv')\n",
    "\n",
    "df_sb = pd.read_csv(r'C:/Users/annie.taylor/Documents/Plastics/ag-plastics/data/Area_by_County/plasticAreaOverTime_SantaBarbaraCounty_v1_2.csv')\n",
    "df_vt = pd.read_csv(r'C:/Users/annie.taylor/Documents/Plastics/ag-plastics/data/Area_by_County/plasticAreaOverTime_VenturaCounty_v1_2.csv')\n",
    "df_sc = pd.read_csv(r'C:/Users/annie.taylor/Documents/Plastics/ag-plastics/data/Area_by_County/plasticAreaOverTime_SantaCruzCounty_v1_2.csv')\n",
    "\n",
    "df_all = pd.concat([df_sb, df_vt, df_sc])\n",
    "df_all['Hoop_acres_percent'] *= 100 #adjusting this for plotting\n",
    "df_all['Mulch_acres_percent'] *= 100\n",
    "df_all['County_Name'] = df_all['County'].str.replace('Santa', 'Santa ')\n",
    "# df_all.to_csv('N:/OCEANS_Program/Plastics/Agricultural_Plastics/Area_by_County/plasticAreaOverTime_allCounties_v1_2.csv', index=False)\n",
    "\n",
    "# filter for just wy2024 rows and display\n",
    "df_wy2024 = df_all[df_all['Year'] == 2024][['Year', 'County_Name', 'Hoop_acres', 'Mulch_acres']]\n",
    "display(df_wy2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked area charts - counties side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the three counties' dataframes to plot them\n",
    "df_list = [df_sc, df_sb, df_vt] # in order N to S\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "for ax, df in zip(axes, df_list):\n",
    "    location = df['County'].unique()[0].replace('Santa', 'Santa ')\n",
    "    ax.stackplot(df['Year'], df['Hoop_acres'], df['Mulch_acres'], labels=['Hoop House', 'Mulch'], colors=['coral', 'lightseagreen'], alpha=0.7)\n",
    "    ax.set_title(f'{location} County')\n",
    "    ax.set_xlabel('Water Year')\n",
    "    ax.set_xticks(df['Year'])\n",
    "    ax.grid(axis='x', linestyle='-', color='white')\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "    ax.set_ylabel('Acres')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "for ax, df in zip(axes, df_list):\n",
    "    df['Hoop_acres_percent'] *= 100 #TODO just convert this at the creation stage, not here\n",
    "    df['Mulch_acres_percent'] *= 100\n",
    "    location = df['County'].unique()[0].replace('Santa', 'Santa ')\n",
    "    ax.stackplot(df['Year'], df['Hoop_acres_percent'], df['Mulch_acres_percent'], labels=['Hoop House', 'Mulch'], colors=['coral', 'lightseagreen'], alpha=0.7)\n",
    "    ax.set_title(f'{location} County')\n",
    "    ax.set_xlabel('Water Year')\n",
    "    ax.set_xticks(df['Year'])\n",
    "    ax.grid(axis='x', linestyle='-', color='white')\n",
    "    ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter())\n",
    "    ax.set_ylabel('Percent of Ag Area')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked area charts - mulch and hoop split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a stacked area chart with all three counties together using df_all\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "# Add space after 'Santa' in the County column for display\n",
    "\n",
    "ax1, ax2 = axes\n",
    "# Create the stacked area chart for Hoop Acres\n",
    "ax1.stackplot(df_all['Year'].unique(), df_all.pivot(index='Year', columns='County_Name', values='Mulch_acres').fillna(0).T, labels=df_all['County_Name'].unique(), alpha=0.5)\n",
    "ax1.set_title('Mulch Acres by County')\n",
    "ax1.set_xlabel('Water Year')\n",
    "ax1.set_ylabel('Acres')\n",
    "ax1.set_xticks(df['Year'])\n",
    "ax1.grid(axis='x', linestyle='-', color='white')\n",
    "ax1.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "# ax1.legend()\n",
    "\n",
    "# Create the stacked area chart for Mulch Acres\n",
    "ax2.stackplot(df_all['Year'].unique(), df_all.pivot(index='Year', columns='County_Name', values='Hoop_acres').fillna(0).T, labels=df_all['County_Name'].unique(), alpha=0.5)\n",
    "ax2.set_title('Hoop Acres by County')\n",
    "ax2.set_xlabel('Water Year')\n",
    "ax2.set_ylabel('Acres')\n",
    "ax2.set_xticks(df['Year'])\n",
    "ax2.grid(axis='x', linestyle='-', color='white')\n",
    "ax2.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked area chart - all acres combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stacked area chart with all three counties together using df_all\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "# Pivot the dataframe to get the data in the right format for stacking\n",
    "pivot_df = df_all.pivot(index='Year', columns='County_Name', values=['Hoop_acres', 'Mulch_acres'])\n",
    "\n",
    "color_map = [\"coral\", \"sandybrown\", \"peachpuff\", \"lightseagreen\", \"mediumaquamarine\", \"lightgreen\"]\n",
    "\n",
    "\n",
    "# Plot the stacked area chart for Hoop and Mulch Acres\n",
    "ax.stackplot(pivot_df.index, \n",
    "             pivot_df['Hoop_acres'].T, \n",
    "             pivot_df['Mulch_acres'].T, \n",
    "             labels=[f'{county} Hoop' for county in pivot_df['Hoop_acres'].columns] + [f'{county} Mulch' for county in pivot_df['Mulch_acres'].columns], \n",
    "             alpha=0.8,\n",
    "             colors=color_map)\n",
    "\n",
    "# Set the title and labels\n",
    "ax.set_title('Hoop and Mulch Acres by County')\n",
    "ax.set_xlabel('Water Year')\n",
    "ax.set_ylabel('Acres')\n",
    "ax.set_xticks(df_all['Year'].unique())\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "ax.grid(axis='x', linestyle='-', color='white')\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "\n",
    "# export the plot as an svg for sara\n",
    "# plt.savefig('N:/OCEANS_Program/Plastics/Agricultural_Plastics/Maps_Figures/Hoop_Mulch_Acres_by_County.svg', format='svg', bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area of plastic -- side by side hoop vs mulch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate percent change of total plastic mulch acres over each year\n",
    "# Sum mulch acres across all three counties for each year\n",
    "years = df_sb['Year']\n",
    "total_mulch_acres = df_sb['Mulch_acres'].values + df_vt['Mulch_acres'].values + df_sc['Mulch_acres'].values\n",
    "total_hoop_acres = df_sb['Hoop_acres'].values + df_vt['Hoop_acres'].values + df_sc['Hoop_acres'].values\n",
    "# percent_change = 100 * (total_mulch_acres[1:] - total_mulch_acres[:-1]) / total_mulch_acres[:-1]\n",
    "\n",
    "change_df = pd.DataFrame({\n",
    "    'Year': years,\n",
    "    'Total_Mulch_Acres': total_mulch_acres,\n",
    "    'Total_Hoop_Acres': total_hoop_acres\n",
    "})\n",
    "\n",
    "change_df['Percent_Change_Mulch'] = change_df['Total_Mulch_Acres'].pct_change() * 100\n",
    "change_df['Percent_Change_Hoop'] = change_df['Total_Hoop_Acres'].pct_change() * 100\n",
    "\n",
    "# Format percent change columns to add '+' for positive values\n",
    "change_df['Percent_Change_Mulch_fmt'] = change_df['Percent_Change_Mulch'].apply(lambda x: f\"+{x:.1f}%\" if pd.notnull(x) and x > 0 else f\"{x:.1f}%\" if pd.notnull(x) else \"\")\n",
    "change_df['Percent_Change_Hoop_fmt'] = change_df['Percent_Change_Hoop'].apply(lambda x: f\"+{x:.1f}%\" if pd.notnull(x) and x > 0 else f\"{x:.1f}%\" if pd.notnull(x) else \"\")\n",
    "\n",
    "display(change_df)\n",
    "\n",
    "# chart it\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "# Left: Hoop Acres stacked area chart\n",
    "axes[0].stackplot(\n",
    "    df_sb['Year'], df_vt['Hoop_acres'], df_sb['Hoop_acres'], df_sc['Hoop_acres'],\n",
    "    labels=['Ventura', 'Santa Barbara', 'Santa Cruz'],\n",
    "    colors=[\"#0072B2\", \"#009E73\", \"#E69F00\"], alpha=0.7  # blue, green, orange (colorblind safe)\n",
    ")\n",
    "axes[0].set_title('Hoop house acres by county')\n",
    "axes[0].set_xlabel('Water Year')\n",
    "axes[0].set_ylabel('Acres')\n",
    "axes[0].set_xticks(df_sb['Year'])\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='x', linestyle='-', color='white')\n",
    "\n",
    "# Right: Mulch Acres stacked area chart\n",
    "axes[1].stackplot(\n",
    "    df_sb['Year'], df_vt['Mulch_acres'], df_sb['Mulch_acres'], df_sc['Mulch_acres'],\n",
    "    labels=['Ventura', 'Santa Barbara', 'Santa Cruz'],\n",
    "    colors=[\"#0072B2\", \"#009E73\", \"#E69F00\"], alpha=0.7\n",
    ")\n",
    "axes[1].set_title('Plastic mulch acres by county')\n",
    "axes[1].set_xlabel('Water Year')\n",
    "axes[1].set_xticks(df_sb['Year'])\n",
    "# axes[1].legend()\n",
    "axes[1].grid(axis='x', linestyle='-', color='white')\n",
    "axes[1].yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis='both', labelsize=10)\n",
    "    ax.set_title(ax.get_title(), fontname='Calibri', fontsize=14)\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontname='Calibri', fontsize=12)\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontname='Calibri', fontsize=12)\n",
    "    for text in ax.texts:\n",
    "        text.set_fontname('Calibri')\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontname('Calibri')\n",
    "\n",
    "\n",
    "# Annotate percent change values on the mulch and hoop charts\n",
    "for i, year in enumerate(years[1:], start=1):\n",
    "    # Mulch percent change annotation (right plot)\n",
    "    pct = change_df.loc[i, 'Percent_Change_Mulch_fmt']\n",
    "    axes[1].annotate(pct, (year, total_mulch_acres[i]),\n",
    "                     textcoords=\"offset points\", xytext=(0, 6), ha='center', fontsize=9, color='black')\n",
    "    # Hoop percent change annotation (left plot)\n",
    "    pct_hoop = change_df.loc[i, 'Percent_Change_Hoop_fmt']\n",
    "    axes[0].annotate(pct_hoop, (year, total_hoop_acres[i]),\n",
    "                     textcoords=\"offset points\", xytext=(0, 5), ha='center', fontsize=9, color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# display(change_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get total percent coverage across all counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the three counties\n",
    "counties = [santabarbara, ventura, cruz]\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results_list = []\n",
    "\n",
    "# Loop through each county and calculate the agricultural area\n",
    "for county in counties:\n",
    "    # Get the county name\n",
    "    county_name = county.first().get('NAME').getInfo()\n",
    "    \n",
    "    # Clip the all_ag image to the county\n",
    "    county_ag = all_ag.clip(county)\n",
    "    \n",
    "    # Reproject the raster\n",
    "    county_ag = county_ag.reproject(crs='EPSG:3310', scale=10)\n",
    "    \n",
    "    # Mask pixel area image to ag lands\n",
    "    ag_area = pixel_area.updateMask(county_ag)\n",
    "    \n",
    "    # Calculate the area\n",
    "    ag_area_calc = ag_area.reduceRegion(\n",
    "        reducer=ee.Reducer.sum(),\n",
    "        geometry=county.geometry(),\n",
    "        crs=ee.Projection('EPSG:3310'),\n",
    "        scale=10,\n",
    "        maxPixels=1e8,\n",
    "    )\n",
    "    \n",
    "    # Extract area in square meters and convert to acres\n",
    "    ag_area_m2 = ag_area_calc.getNumber('area').getInfo()\n",
    "    ag_area_acres = ag_area_m2 / 4046.8564224\n",
    "    \n",
    "    # Add the result to the list\n",
    "    results_list.append({\n",
    "        'County': county_name,\n",
    "        'Ag_acres': ag_area_acres\n",
    "    })\n",
    "\n",
    "# Convert the list to a dataframe\n",
    "df_ag_acres = pd.DataFrame(results_list)\n",
    "\n",
    "# Display the dataframe\n",
    "display(df_ag_acres)\n",
    "total_ag_acres = df_ag_acres['Ag_acres'].sum()\n",
    "display('Total ag acres across counties: ', total_ag_acres)\n",
    "\n",
    "# Add a column to df_all standardized by total ag land area across all counties\n",
    "df_all['Hoop_acres_percent_all'] = df_all['Hoop_acres'] / total_ag_acres * 100\n",
    "df_all['Mulch_acres_percent_all'] = df_all['Mulch_acres'] / total_ag_acres * 100\n",
    "# df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacked area chart - percent of total ag area combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stacked area chart with all three counties together using df_all\n",
    "fig, ax = plt.subplots(figsize=(9, 6))\n",
    "\n",
    "# Pivot the dataframe to get the data in the right format for stacking\n",
    "pivot_df = df_all.pivot(index='Year', columns='County_Name', values=['Hoop_acres_percent_all', 'Mulch_acres_percent_all'])\n",
    "\n",
    "color_map = [\"coral\", \"sandybrown\", \"peachpuff\", \"lightseagreen\", \"mediumaquamarine\", \"lightgreen\"]\n",
    "\n",
    "\n",
    "# Plot the stacked area chart for Hoop and Mulch Acres\n",
    "ax.stackplot(pivot_df.index, \n",
    "             pivot_df['Hoop_acres_percent_all'].T, \n",
    "             pivot_df['Mulch_acres_percent_all'].T, \n",
    "             labels=[f'{county} Hoop' for county in pivot_df['Hoop_acres_percent_all'].columns] + [f'{county} Mulch' for county in pivot_df['Mulch_acres_percent_all'].columns], \n",
    "             alpha=0.8,\n",
    "             colors=color_map)\n",
    "\n",
    "# Set the title and labels\n",
    "ax.set_title('Hoop and Mulch Percent of Total Ag Area')\n",
    "ax.set_xlabel('Water Year')\n",
    "ax.set_ylabel('Percent of Total Ag Area')\n",
    "ax.set_xticks(df_all['Year'].unique())\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "ax.grid(axis='x', linestyle='-', color='white')\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter())\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get area calcs for field-classification rasters\n",
    "only have wy24 right now, test difference from existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pixel area image. Pixel values are square meters based on a given CRS and scale (or CRS transform).\n",
    "pixel_area = ee.Image.pixelArea()\n",
    "# The default projection is WGS84 with 1-degree scale.\n",
    "# display('Pixel area default projection', pixel_area.projection())\n",
    "\n",
    "counties = [cruz, ventura, santabarbara]\n",
    "wy = 2024\n",
    "\n",
    "df_county_ts = []\n",
    "\n",
    "for county in counties:\n",
    "    county_name = county.first().get('NAME').getInfo().replace(\" \", \"\")\n",
    "\n",
    "    # import asset\n",
    "    ras = ee.Image(f'projects/ee-tnc-annietaylor/assets/PlasticClass_{county_name}Co_WY{wy}_v1_2_prj_majority_wgs')\n",
    "\n",
    "    # reproject the raster (doesn't appear necessary, results identical) \n",
    "    ras = ras.reproject(crs='EPSG:3310', scale=10)\n",
    "\n",
    "    hoop = ras.select('b1').eq(1)\n",
    "    mulch = ras.select('b1').eq(2) # TODO print these to map to double check\n",
    "\n",
    "    hoop_area = pixel_area.updateMask(hoop)\n",
    "    mulch_area = pixel_area.updateMask(mulch)\n",
    "\n",
    "    hoop_area_calc = hoop_area.reduceRegion(\n",
    "        reducer=ee.Reducer.sum(),\n",
    "        geometry=county.geometry(),\n",
    "        crs=ee.Projection('EPSG:3310'),\n",
    "        scale=10,\n",
    "        maxPixels=1e8,\n",
    "    )\n",
    "\n",
    "    mulch_area_calc = mulch_area.reduceRegion(\n",
    "        reducer=ee.Reducer.sum(),\n",
    "        geometry=county.geometry(),\n",
    "        crs=ee.Projection('EPSG:3310'),\n",
    "        scale=10,\n",
    "        maxPixels=1e8,\n",
    "    )\n",
    "\n",
    "    hoop_m2 = hoop_area_calc.getNumber('area').getInfo()\n",
    "    mulch_m2 = mulch_area_calc.getNumber('area').getInfo()\n",
    "\n",
    "    # save these numbers to a dataframe\n",
    "    data = {\n",
    "        'Year': [wy],\n",
    "        'County': [county_name],\n",
    "        'Hoop_m2': [hoop_m2],\n",
    "        'Hoop_acres': [(hoop_m2/4046.8564224)], # to convert m2 to acres\n",
    "        'Mulch_m2': [mulch_m2],\n",
    "        'Mulch_acres': [(mulch_m2/4046.8564224)]\n",
    "    }\n",
    "    df_county_ts.append(pd.DataFrame(data))\n",
    "\n",
    "df_area = pd.concat(df_county_ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display('pixel:', df_wy2024)\n",
    "county_order = ['SantaCruz', 'Ventura', 'SantaBarbara']\n",
    "df_area_sorted = df_area.set_index('County').loc[county_order].reset_index()\n",
    "display('field:', df_area_sorted[['Year', 'County', 'Hoop_acres', 'Mulch_acres']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add final classifications to map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wy = 2023\n",
    "# county_name = 'SantaBarbara'\n",
    "\n",
    "# ras = ee.Image(f'projects/ee-tnc-annietaylor/assets/agplastics/PlasticClass_{county_name}Co_WY{wy}_int_v2')\n",
    "\n",
    "# # test my masking as well TODO from above\n",
    "\n",
    "\n",
    "# # add to map\n",
    "# m = geemap.Map()\n",
    "# m.add_basemap('SATELLITE')\n",
    "# m.addLayer(ras.select('class'), {'min': 0, 'max': 2, 'palette': ['blue', 'red', 'yellow']}, 'Final Classes')\n",
    "# m.centerObject(ca, 6)\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert watsonville df to a geopandas df\n",
    "# gdf = gpd.GeoDataFrame(watsonville, geometry=gpd.points_from_xy(watsonville.Longitude, watsonville.Latitude))\n",
    "# gdf = gdf.set_crs('EPSG:4326')\n",
    "\n",
    "# plot the points\n",
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "# gdf.plot(ax=ax, color='red', markersize=10)\n",
    "# plt.show()\n",
    "\n",
    "# src = rasterio.open('N:\\OCEANS_Program\\Plastics\\Agricultural_Plastics\\AgPlastics_Pro\\EE_uploads_exports\\Watsonville\\Plastic_SantaCruzCo_95p_WY2023.tif')\n",
    "\n",
    "# # plot points with raster\n",
    "# fig, ax = plt.subplots()\n",
    "# # transform rasterio plot to real world coords\n",
    "# extent = [src.bounds[0], src.bounds[2], src.bounds[1], src.bounds[3]]\n",
    "# extent = [-121.9, src.bounds[2], src.bounds[1], 37.0]\n",
    "# ax = rasterio.plot.show(mulch, extent=extent, ax=ax, cmap=\"viridis\")\n",
    "# gdf.plot(ax=ax, color='red', markersize=0.5)\n",
    "# # print(extent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old acreage calculation\n",
    "# plastic_type = 'mulch_p95'\n",
    "# county = ventura\n",
    "\n",
    "# # I've shared these from my personal EE account\n",
    "# plastic_2019_95p = ee.Image('projects/ee-annalisertaylor/assets/TNC/agplastics/PlasticClass_VenturaCo_95p_2019_mask75')\n",
    "# plastic_2023_95p = ee.Image('projects/ee-annalisertaylor/assets/TNC/agplastics/PlasticClass_VenturaCo_95p_2023_mask75')\n",
    "\n",
    "# # Create a pixel area image. Pixel values are square meters based on\n",
    "# # a given CRS and scale (or CRS transform).\n",
    "# pixel_area = ee.Image.pixelArea()\n",
    "\n",
    "# # The default projection is WGS84 with 1-degree scale.\n",
    "# # display('Pixel area default projection', pixel_area.projection())\n",
    "\n",
    "# plastic_2019 = plastic_2019_95p.select(plastic_type).gt(0)\n",
    "# plastic_2023 = plastic_2023_95p.select(plastic_type).gt(0)\n",
    "\n",
    "# # Apply the hoop or mulch classified area mask to the pixel area image.\n",
    "# plastic_area_2019 = pixel_area.updateMask(plastic_2019)\n",
    "# plastic_area_2023 = pixel_area.updateMask(plastic_2023)\n",
    "\n",
    "# area_2019 = plastic_area_2019.reduceRegion(\n",
    "#     reducer=ee.Reducer.sum(),\n",
    "#     geometry=county.geometry(),\n",
    "#     crs=ee.Projection('EPSG:3310'),\n",
    "#     # crs=hoop.projection(),\n",
    "#     scale=10,\n",
    "#     maxPixels=1e8,\n",
    "# )\n",
    "\n",
    "# area_2023 = plastic_area_2023.reduceRegion(\n",
    "#     reducer=ee.Reducer.sum(),\n",
    "#     geometry=county.geometry(),\n",
    "#     crs=ee.Projection('EPSG:3310'),\n",
    "#     # crs=hoop.projection(),\n",
    "#     scale=10,\n",
    "#     maxPixels=1e8,\n",
    "# )\n",
    "\n",
    "# square_meters_19 = area_2019.getNumber('area')\n",
    "# acres_19 = square_meters_19.divide(4046.8564224) # to convert m2 to acres\n",
    "\n",
    "# square_meters_23 = area_2023.getNumber('area')\n",
    "# acres_23 = square_meters_23.divide(4046.8564224) # to convert m2 to acres\n",
    "\n",
    "# display(f'Area of {plastic_type} in 2019  m2: {square_meters_19.getInfo()}, acres: {acres_19.getInfo()}')\n",
    "# display(f'Area of {plastic_type} in 2023  m2: {square_meters_23.getInfo()}, acres: {acres_23.getInfo()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All counties combined - line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot hoop acres and mulch acres for each county\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# # Set color cycle\n",
    "# colors = ['mediumaquamarine', 'mediumaquamarine', 'tomato', 'tomato', 'slateblue', 'slateblue']\n",
    "# ax.set_prop_cycle(color=colors)\n",
    "\n",
    "# for county in df_all['County'].unique():\n",
    "#     county_data = df_all[df_all['County'] == county]\n",
    "#     county = county.replace('Santa', 'Santa ')\n",
    "#     ax.plot(county_data['Year'], county_data['Hoop_acres_percent'], label=f'{county} Hoop', linestyle='-', marker='o')\n",
    "#     ax.plot(county_data['Year'], county_data['Mulch_acres_percent'], label=f'{county} Mulch', linestyle='--', marker='o')\n",
    "\n",
    "# legend_elements = [\n",
    "#     Line2D([0], [0], marker='o', color='mediumaquamarine', markerfacecolor='mediumaquamarine', lw=2, label='Santa Barbara County', linestyle='-'),\n",
    "#     Line2D([0], [0], marker='o', color='tomato', markerfacecolor='tomato', lw=2, label='Ventura County', linestyle='-'),\n",
    "#     Line2D([0], [0], marker='o', color='slateblue', markerfacecolor='slateblue', lw=2, label='Santa Cruz County', linestyle='-'),\n",
    "#     Line2D([0], [0], color='black', lw=2, label='Hoop', linestyle='-'),\n",
    "#     Line2D([0], [0], color='black', lw=2, label='Mulch', linestyle='--')\n",
    "# ]\n",
    "\n",
    "# ax.set_xlabel('Water Year')\n",
    "# ax.set_ylabel('Percent of Ag Area in County')\n",
    "# ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter())\n",
    "# ax.legend(handles=legend_elements)#, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# ax.set_xticks(df_all['Year'])  # Set x-axis ticks to be the integer values of the year\n",
    "# ax.set_ylim(bottom=0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot hoop acres and mulch acres for each county\n",
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# # Set color cycle\n",
    "# colors = ['mediumaquamarine', 'mediumaquamarine', 'tomato', 'tomato', 'slateblue', 'slateblue']\n",
    "# ax.set_prop_cycle(color=colors)\n",
    "\n",
    "# for county in df_all['County'].unique():\n",
    "#     county_data = df_all[df_all['County'] == county]\n",
    "#     county = county.replace('Santa', 'Santa ')\n",
    "#     ax.plot(county_data['Year'], county_data['Hoop_acres'], label=f'{county} Hoop', linestyle='-', marker='o')\n",
    "#     ax.plot(county_data['Year'], county_data['Mulch_acres'], label=f'{county} Mulch', linestyle='--', marker='o')\n",
    "\n",
    "# # Separate the legend so that county is distinguished by color and hoop/mulch is distinguished by solid/dashed lines\n",
    "# legend_elements = [\n",
    "#     Line2D([0], [0], marker='o', color='mediumaquamarine', markerfacecolor='mediumaquamarine', lw=2, label='Santa Barbara County', linestyle='-'),\n",
    "#     Line2D([0], [0], marker='o', color='tomato', markerfacecolor='tomato', lw=2, label='Ventura County', linestyle='-'),\n",
    "#     Line2D([0], [0], marker='o', color='slateblue', markerfacecolor='slateblue', lw=2, label='Santa Cruz County', linestyle='-'),\n",
    "#     Line2D([0], [0], color='black', lw=2, label='Hoop', linestyle='-'),\n",
    "#     Line2D([0], [0], color='black', lw=2, label='Mulch', linestyle='--')\n",
    "# ]\n",
    "\n",
    "# ax.set_xlabel('Water Year')\n",
    "# ax.set_ylabel('Acres')\n",
    "# ax.legend(handles=legend_elements)#, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "# ax.yaxis.set_major_formatter(mpl.ticker.StrMethodFormatter('{x:,.0f}'))\n",
    "# ax.set_xticks(df_all['Year'])  # Set x-axis ticks to be the integer values of the year\n",
    "# ax.set_ylim(bottom=0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area calculation with rioxarray to double check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set county\n",
    "county_name = 'Santa Cruz'\n",
    "\n",
    "# Define the folder containing the tif files and the shapefile path\n",
    "tif_folder = 'N:/OCEANS_Program/Plastics/Agricultural_Plastics/AgPlastics_Pro/EE_uploads_exports'\n",
    "shapefile_path = 'N:/California/data/administrative/California_Counties_2024/California_Counties.shp'\n",
    "\n",
    "# Read the shapefile and grab the county of interest\n",
    "shapefile = gpd.read_file(shapefile_path)\n",
    "county_shp = shapefile[shapefile['NAME'] == (county_name + ' County')]\n",
    "# project county_shp to EPSG:3310\n",
    "county_shp = county_shp.to_crs('EPSG:3310')\n",
    "# display(county_shp)#, county_shp.crs)\n",
    "\n",
    "# # test / visualize one raster at a time\n",
    "# wy = 2019\n",
    "# county_var = county_name.replace(\" \", \"\")\n",
    "# tif_path = os.path.join(tif_folder, f'PlasticClass_{county_var}Co_WY{wy}_int_v2.tif')\n",
    "\n",
    "# # import the raster as an xarray object\n",
    "# plastic_ras = rioxarray.open_rasterio(tif_path, masked=True).squeeze()\n",
    "# # Reproject the raster (I believe this is using nearest neighbor)\n",
    "# plastic_ras_prj = plastic_ras.rio.reproject(county_shp.crs)\n",
    "# # display(plastic_ras_prj.rio.crs)\n",
    "\n",
    "# # print out the map\n",
    "# f, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "# plastic_ras_prj.plot.imshow(ax=ax)#, cmap='Greys')\n",
    "# county_shp.plot(color='None',\n",
    "#                     edgecolor='white',\n",
    "#                     linewidth=1,\n",
    "#                     ax=ax)\n",
    "# ax.set(title=f\"Plastic Classification for {county_name} County in {wy}\")\n",
    "# ax.set_axis_off()\n",
    "# plt.show()\n",
    "\n",
    "# histogram to check for weird na values\n",
    "# plastic_ras_prj.plot.hist(color=\"purple\")\n",
    "\n",
    "wys = [2019, 2020, 2021, 2022, 2023]\n",
    "county_var = county_name.replace(\" \", \"\")\n",
    "df_county_ts = []\n",
    "\n",
    "for wy in wys: \n",
    "    tif_path = os.path.join(tif_folder, f'PlasticClass_{county_var}Co_WY{wy}_int_v2.tif')\n",
    "    # import the raster as an xarray object\n",
    "    plastic_ras = rioxarray.open_rasterio(tif_path, masked=True).squeeze()\n",
    "    # Reproject the raster (I believe this is using nearest neighbor) and clip to county\n",
    "    plastic_ras_prj = plastic_ras.rio.reproject(county_shp.crs)\n",
    "    plastic_ras_clipped = plastic_ras_prj.rio.clip(county_shp.geometry)\n",
    "    pixel_area_m2 = abs(plastic_ras_clipped.rio.resolution()[0] * plastic_ras_clipped.rio.resolution()[1])\n",
    "    pixel_area_acres = pixel_area_m2 / 4046.8564224\n",
    "\n",
    "    # Calculate the number of pixels for each band value (1 and 2) within the clipped raster\n",
    "    unique, counts = np.unique(plastic_ras_clipped, return_counts=True)\n",
    "    pixel_counts = dict(zip(unique, counts))\n",
    "\n",
    "    # Multiply the pixel counts by the area of each pixel to get the area of each band value\n",
    "    area_counts = {value: count * pixel_area_acres for value, count in pixel_counts.items()}\n",
    "\n",
    "\n",
    "    # save these numbers to a dataframe\n",
    "    data = {\n",
    "        'Year': [wy],\n",
    "        'County': [county_name],\n",
    "        'Hoop_acres': [area_counts.get(1)],\n",
    "        'Mulch_acres': [area_counts.get(2)]\n",
    "    }\n",
    "    df_county_ts.append(pd.DataFrame(data))\n",
    "\n",
    "df_area = pd.concat(df_county_ts)\n",
    "display(df_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### percent of ag area - santa cruz county only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is only for santa cruz, had to reproject, clip, and reclass it in pro due to size limits when running in rioxarray\n",
    "all_ag = 'N:/OCEANS_Program/Plastics/Agricultural_Plastics/AgPlastics_Pro/EE_uploads_exports/dwr_allyears_cruz_reclass.tif'\n",
    "\n",
    "# calculate ag area in county\n",
    "ag_ras = rioxarray.open_rasterio(all_ag, masked=True).squeeze()\n",
    "# ag_ras_prj = ag_ras.rio.reproject(county_shp.crs) # this raster is already project\n",
    "ag_ras_clipped = ag_ras.rio.clip(county_shp.geometry) #and should already be clipped but this is safest\n",
    "\n",
    "# ag_ras_clipped.plot.hist(color=\"purple\")\n",
    "pixel_area_m2 = abs(ag_ras_clipped.rio.resolution()[0] * ag_ras_clipped.rio.resolution()[1])\n",
    "pixel_area_acres = pixel_area_m2 / 4046.8564224\n",
    "\n",
    "# Filter the raster to only values greater than 0 and not NAN\n",
    "# ag_ras_filtered = ag_ras_clipped.where((ag_ras_clipped > 0) & (~np.isnan(ag_ras_clipped)))\n",
    "\n",
    "# set all values of the raster greater than 0 to a value of 1\n",
    "# ag_ras_reset = ag_ras_clipped.where(ag_ras_clipped > 0, 0)\n",
    "\n",
    "# Calculate the number of pixels for each band value (1 and 2) within the clipped raster\n",
    "unique, counts = np.unique(ag_ras_clipped, return_counts=True)\n",
    "pixel_counts = dict(zip(unique, counts))\n",
    "\n",
    "# Multiply the pixel counts by the area of each pixel to get the area of each band value\n",
    "area_counts = {value: count * pixel_area_acres for value, count in pixel_counts.items()}\n",
    "\n",
    "# Sum up all the values in the dictionary\n",
    "total_area_acres = area_counts.get(1)\n",
    "display(total_area_acres)\n",
    "\n",
    "# santa cruz is 18378.100519 ag acres with EE, 18374.072926411085 acres with rioxarray\n",
    "\n",
    "# add a column to df_area standardized by ag land area in that county\n",
    "df_area['Hoop_acres_percent'] = df_area['Hoop_acres'] / total_area_acres\n",
    "df_area['Mulch_acres_percent'] = df_area['Mulch_acres'] / total_area_acres\n",
    "df_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the change in mulch_acres and hoop_acres over Year side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Plot for Mulch Acres\n",
    "ax1.plot(df_area['Year'], df_area['Mulch_acres'], marker='o', linestyle='-', color='orange')\n",
    "ax1.set_ylabel('Acres of Plastic Mulch')\n",
    "ax1.set_title(f'Plastic Mulch Acreage in {county_name} County')\n",
    "ax1.set_xticks(df_area['Year'])  # Set x-axis ticks to be the integer values of the year\n",
    "\n",
    "# Plot for Hoop Acres\n",
    "ax2.plot(df_area['Year'], df_area['Hoop_acres'], marker='o', linestyle='-', color='darkred')\n",
    "ax2.set_ylabel('Acres of Hoop Houses')\n",
    "ax2.set_title(f'Hoop House Acreage in {county_name} County')\n",
    "ax2.set_xticks(df_area['Year'])  # Set x-axis ticks to be the integer values of the year\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3IAp87d7CAo9"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "plastic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
